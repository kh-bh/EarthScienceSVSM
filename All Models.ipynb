{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "---Run time is 0.0004385999999954038 seconds ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 70 #display 70 dpi in Jupyter Notebook, may consider100 dpi \n",
    "plt.rcParams['savefig.dpi'] = 300 #define 300 dpi for saving figures\n",
    "\n",
    "import seaborn as sns\n",
    "## here are some settings \n",
    "sns.set_style('whitegrid')\n",
    "sns.set(rc={\"figure.dpi\":70, 'savefig.dpi':300}) #defining dpi setting\n",
    "sns.set_context('notebook')\n",
    "sns.set_style(\"ticks\")\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "# Tells matplotlib to display images inline instead of a new window\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "random.seed(1000)\n",
    "\n",
    "from time import time\n",
    "import timeit #imports timeit module\n",
    "start_time = timeit.default_timer() #defines start time so computational time can be calculated\n",
    "print(\"Hello World\")\n",
    "elapsed = timeit.default_timer() - start_time #gives total computation time\n",
    "print(\"---Run time is %s seconds ---\" % elapsed) #prints computation time\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import neighbors\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-3ca219f8874c>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dffull2021[\"Veg_class\"] = \"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OID__x</th>\n",
       "      <th>Id</th>\n",
       "      <th>gridcode</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>CH_mean</th>\n",
       "      <th>ARVI_mean</th>\n",
       "      <th>ARVI_max</th>\n",
       "      <th>ARVI_med</th>\n",
       "      <th>EVI_mean</th>\n",
       "      <th>EVI_max</th>\n",
       "      <th>EVI_med</th>\n",
       "      <th>NDVI_mean</th>\n",
       "      <th>NDVI_max</th>\n",
       "      <th>NDVI_med</th>\n",
       "      <th>SAVI_mean</th>\n",
       "      <th>SAVI_max</th>\n",
       "      <th>SAVI_med</th>\n",
       "      <th>Veg_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.2</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.053054</td>\n",
       "      <td>0.252378</td>\n",
       "      <td>-0.007871</td>\n",
       "      <td>0.145961</td>\n",
       "      <td>0.234673</td>\n",
       "      <td>0.126273</td>\n",
       "      <td>0.221155</td>\n",
       "      <td>0.400501</td>\n",
       "      <td>0.170450</td>\n",
       "      <td>0.145235</td>\n",
       "      <td>0.234261</td>\n",
       "      <td>0.125838</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>5.73</td>\n",
       "      <td>0.016248</td>\n",
       "      <td>0.040314</td>\n",
       "      <td>0.252378</td>\n",
       "      <td>0.043301</td>\n",
       "      <td>0.141544</td>\n",
       "      <td>0.234673</td>\n",
       "      <td>0.135954</td>\n",
       "      <td>0.206992</td>\n",
       "      <td>0.400501</td>\n",
       "      <td>0.207240</td>\n",
       "      <td>0.139659</td>\n",
       "      <td>0.234261</td>\n",
       "      <td>0.134476</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.014410</td>\n",
       "      <td>0.111847</td>\n",
       "      <td>0.281741</td>\n",
       "      <td>0.085010</td>\n",
       "      <td>0.167371</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.152276</td>\n",
       "      <td>0.274226</td>\n",
       "      <td>0.432882</td>\n",
       "      <td>0.249348</td>\n",
       "      <td>0.167777</td>\n",
       "      <td>0.255698</td>\n",
       "      <td>0.152786</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.012281</td>\n",
       "      <td>0.204984</td>\n",
       "      <td>0.310943</td>\n",
       "      <td>0.281741</td>\n",
       "      <td>0.211967</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.361578</td>\n",
       "      <td>0.438509</td>\n",
       "      <td>0.432882</td>\n",
       "      <td>0.213450</td>\n",
       "      <td>0.255698</td>\n",
       "      <td>0.255698</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>18.2</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.260015</td>\n",
       "      <td>0.365155</td>\n",
       "      <td>0.256816</td>\n",
       "      <td>0.203891</td>\n",
       "      <td>0.232604</td>\n",
       "      <td>0.209083</td>\n",
       "      <td>0.403924</td>\n",
       "      <td>0.488291</td>\n",
       "      <td>0.410121</td>\n",
       "      <td>0.207537</td>\n",
       "      <td>0.236474</td>\n",
       "      <td>0.212997</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657503</th>\n",
       "      <td>1657605</td>\n",
       "      <td>1657605</td>\n",
       "      <td>1657605</td>\n",
       "      <td>9.4</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.014932</td>\n",
       "      <td>0.509040</td>\n",
       "      <td>0.536940</td>\n",
       "      <td>0.506748</td>\n",
       "      <td>0.382288</td>\n",
       "      <td>0.407742</td>\n",
       "      <td>0.407359</td>\n",
       "      <td>0.584744</td>\n",
       "      <td>0.606661</td>\n",
       "      <td>0.589247</td>\n",
       "      <td>0.353369</td>\n",
       "      <td>0.377064</td>\n",
       "      <td>0.373630</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657504</th>\n",
       "      <td>1657606</td>\n",
       "      <td>1657606</td>\n",
       "      <td>1657606</td>\n",
       "      <td>7.2</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.194732</td>\n",
       "      <td>0.493816</td>\n",
       "      <td>0.545701</td>\n",
       "      <td>0.531150</td>\n",
       "      <td>0.355797</td>\n",
       "      <td>0.390464</td>\n",
       "      <td>0.351225</td>\n",
       "      <td>0.571676</td>\n",
       "      <td>0.602668</td>\n",
       "      <td>0.582336</td>\n",
       "      <td>0.332901</td>\n",
       "      <td>0.363158</td>\n",
       "      <td>0.328131</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657505</th>\n",
       "      <td>1657607</td>\n",
       "      <td>1657607</td>\n",
       "      <td>1657607</td>\n",
       "      <td>10.2</td>\n",
       "      <td>2.59</td>\n",
       "      <td>0.035261</td>\n",
       "      <td>0.525481</td>\n",
       "      <td>0.641269</td>\n",
       "      <td>0.543615</td>\n",
       "      <td>0.453275</td>\n",
       "      <td>0.507084</td>\n",
       "      <td>0.464525</td>\n",
       "      <td>0.607242</td>\n",
       "      <td>0.687645</td>\n",
       "      <td>0.619722</td>\n",
       "      <td>0.410453</td>\n",
       "      <td>0.451535</td>\n",
       "      <td>0.418655</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657506</th>\n",
       "      <td>1657608</td>\n",
       "      <td>1657608</td>\n",
       "      <td>1657608</td>\n",
       "      <td>11.8</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.847143</td>\n",
       "      <td>0.529820</td>\n",
       "      <td>0.610390</td>\n",
       "      <td>0.529278</td>\n",
       "      <td>0.417614</td>\n",
       "      <td>0.479161</td>\n",
       "      <td>0.392868</td>\n",
       "      <td>0.604037</td>\n",
       "      <td>0.665525</td>\n",
       "      <td>0.598336</td>\n",
       "      <td>0.381633</td>\n",
       "      <td>0.430885</td>\n",
       "      <td>0.361104</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657507</th>\n",
       "      <td>1657609</td>\n",
       "      <td>1657609</td>\n",
       "      <td>1657609</td>\n",
       "      <td>12.8</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.013516</td>\n",
       "      <td>0.440997</td>\n",
       "      <td>0.530145</td>\n",
       "      <td>0.452717</td>\n",
       "      <td>0.368150</td>\n",
       "      <td>0.433651</td>\n",
       "      <td>0.362634</td>\n",
       "      <td>0.529237</td>\n",
       "      <td>0.605752</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.336447</td>\n",
       "      <td>0.393824</td>\n",
       "      <td>0.332581</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1657508 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          OID__x       Id  gridcode  Shape_Length  Shape_Area   CH_mean  \\\n",
       "0              1        1         1          14.2        5.04  0.010000   \n",
       "1              2        2         2          26.2        5.73  0.016248   \n",
       "2              3        3         3          10.6        2.29  0.014410   \n",
       "3              4        4         4          11.0        2.85  0.012281   \n",
       "4              5        5         5          18.2        3.78  0.012011   \n",
       "...          ...      ...       ...           ...         ...       ...   \n",
       "1657503  1657605  1657605   1657605           9.4        2.21  0.014932   \n",
       "1657504  1657606  1657606   1657606           7.2        2.07  1.194732   \n",
       "1657505  1657607  1657607   1657607          10.2        2.59  0.035261   \n",
       "1657506  1657608  1657608   1657608          11.8        2.90  0.847143   \n",
       "1657507  1657609  1657609   1657609          12.8        2.19  0.013516   \n",
       "\n",
       "         ARVI_mean  ARVI_max  ARVI_med  EVI_mean   EVI_max   EVI_med  \\\n",
       "0         0.053054  0.252378 -0.007871  0.145961  0.234673  0.126273   \n",
       "1         0.040314  0.252378  0.043301  0.141544  0.234673  0.135954   \n",
       "2         0.111847  0.281741  0.085010  0.167371  0.254378  0.152276   \n",
       "3         0.204984  0.310943  0.281741  0.211967  0.254378  0.254378   \n",
       "4         0.260015  0.365155  0.256816  0.203891  0.232604  0.209083   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "1657503   0.509040  0.536940  0.506748  0.382288  0.407742  0.407359   \n",
       "1657504   0.493816  0.545701  0.531150  0.355797  0.390464  0.351225   \n",
       "1657505   0.525481  0.641269  0.543615  0.453275  0.507084  0.464525   \n",
       "1657506   0.529820  0.610390  0.529278  0.417614  0.479161  0.392868   \n",
       "1657507   0.440997  0.530145  0.452717  0.368150  0.433651  0.362634   \n",
       "\n",
       "         NDVI_mean  NDVI_max  NDVI_med  SAVI_mean  SAVI_max  SAVI_med  \\\n",
       "0         0.221155  0.400501  0.170450   0.145235  0.234261  0.125838   \n",
       "1         0.206992  0.400501  0.207240   0.139659  0.234261  0.134476   \n",
       "2         0.274226  0.432882  0.249348   0.167777  0.255698  0.152786   \n",
       "3         0.361578  0.438509  0.432882   0.213450  0.255698  0.255698   \n",
       "4         0.403924  0.488291  0.410121   0.207537  0.236474  0.212997   \n",
       "...            ...       ...       ...        ...       ...       ...   \n",
       "1657503   0.584744  0.606661  0.589247   0.353369  0.377064  0.373630   \n",
       "1657504   0.571676  0.602668  0.582336   0.332901  0.363158  0.328131   \n",
       "1657505   0.607242  0.687645  0.619722   0.410453  0.451535  0.418655   \n",
       "1657506   0.604037  0.665525  0.598336   0.381633  0.430885  0.361104   \n",
       "1657507   0.529237  0.605752  0.528302   0.336447  0.393824  0.332581   \n",
       "\n",
       "        Veg_class  \n",
       "0                  \n",
       "1                  \n",
       "2                  \n",
       "3                  \n",
       "4                  \n",
       "...           ...  \n",
       "1657503            \n",
       "1657504            \n",
       "1657505            \n",
       "1657506            \n",
       "1657507            \n",
       "\n",
       "[1657508 rows x 19 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2017 = pd.read_csv('SRER_2017_training_bi.csv', na_values = '?').dropna()\n",
    "df2021 = pd.read_csv(\"SRER21_dataset_v1.csv\", na_values = '?').dropna()\n",
    "dfJornada2017 = pd.read_csv(\"JORN17_dataset_v1.csv\", na_values = '?').dropna()\n",
    "dfJornada2021 = pd.read_csv(\"JORN21_dataset_v1.csv\", na_values = '?').dropna()\n",
    "dfJornada2017[\"Year\"] = '2017'\n",
    "dfJornada2021[\"Year\"] = '2021'\n",
    "dfJornada2017 = dfJornada2017.reindex(columns=['OID_','Id', 'gridcode','Shape_Length','Shape_Area', 'CH_mean', 'ARVI_max', 'ARVI_mean', 'ARVI_med', 'EVI_max', 'EVI_mean', 'EVI_med', 'NDVI_max', 'NDVI_mean', 'NDVI_med', 'SAVI_max', 'SAVI_mean', 'SAVI_med', 'Year', 'Veg_class'])\n",
    "dfJornada2021 = dfJornada2021.reindex(columns=['OID_','Id', 'gridcode','Shape_Length','Shape_Area', 'CH_mean', 'ARVI_max', 'ARVI_mean', 'ARVI_med', 'EVI_max', 'EVI_mean', 'EVI_med', 'NDVI_max', 'NDVI_mean', 'NDVI_med', 'SAVI_max', 'SAVI_mean', 'SAVI_med', 'Year', 'Veg_class'])\n",
    "dffull2017_ = pd.read_csv(\"SRER17_pred.csv\", na_values = '?')\n",
    "del dffull2017_[\"Veg_class\"]\n",
    "dffull2017 = dffull2017_.dropna()\n",
    "dffull2017[\"Veg_class\"] = \"\"\n",
    "\n",
    "dffull2021_ = pd.read_csv(\"SRER21_pred.csv\", na_values = '?')\n",
    "del dffull2021_[\"Veg_class\"]\n",
    "dffull2021 = dffull2021_.dropna()\n",
    "dffull2021[\"Veg_class\"] = \"\"\n",
    "\n",
    "\n",
    "#dfCombined = dfCombined.reindex(columns=['OID_','Id', 'gridcode','Shape_Length','Shape_Area', 'CH_mean', 'ARVI_max', 'ARVI_mean', 'ARVI_med', 'EVI_max', 'EVI_mean', 'EVI_med', 'NDVI_max', 'NDVI_mean', 'NDVI_med', 'SAVI_max', 'SAVI_mean', 'SAVI_med', 'Year', 'Veg_class'])\n",
    "\n",
    "\n",
    "frames = [dfJornada2017,dfJornada2021]\n",
    "\n",
    "#dfCombined = pd.concat(frames)\n",
    "#dfCombined = dfCombined.reindex(columns=['OID_','Id', 'gridcode','Shape_Length','Shape_Area', 'CH_mean', 'ARVI_max', 'ARVI_mean', 'ARVI_med', 'EVI_max', 'EVI_mean', 'EVI_med', 'NDVI_max', 'NDVI_mean', 'NDVI_med', 'SAVI_max', 'SAVI_mean', 'SAVI_med', 'Year', 'Veg_class'])\n",
    "\n",
    "dffull2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OID_</th>\n",
       "      <th>Id</th>\n",
       "      <th>gridcode</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>CH_mean</th>\n",
       "      <th>ARVI_max</th>\n",
       "      <th>ARVI_mean</th>\n",
       "      <th>ARVI_med</th>\n",
       "      <th>EVI_max</th>\n",
       "      <th>EVI_mean</th>\n",
       "      <th>EVI_med</th>\n",
       "      <th>NDVI_max</th>\n",
       "      <th>NDVI_mean</th>\n",
       "      <th>NDVI_med</th>\n",
       "      <th>SAVI_max</th>\n",
       "      <th>SAVI_mean</th>\n",
       "      <th>SAVI_med</th>\n",
       "      <th>Veg_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>41.6</td>\n",
       "      <td>17.41</td>\n",
       "      <td>0.190714</td>\n",
       "      <td>0.609099</td>\n",
       "      <td>0.344183</td>\n",
       "      <td>0.379428</td>\n",
       "      <td>0.443455</td>\n",
       "      <td>0.292047</td>\n",
       "      <td>0.290985</td>\n",
       "      <td>0.665698</td>\n",
       "      <td>0.476709</td>\n",
       "      <td>0.507514</td>\n",
       "      <td>0.406991</td>\n",
       "      <td>0.286203</td>\n",
       "      <td>0.288351</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>31.4</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.245301</td>\n",
       "      <td>0.274936</td>\n",
       "      <td>0.332059</td>\n",
       "      <td>0.252919</td>\n",
       "      <td>0.244477</td>\n",
       "      <td>0.526375</td>\n",
       "      <td>0.406745</td>\n",
       "      <td>0.433791</td>\n",
       "      <td>0.328585</td>\n",
       "      <td>0.254814</td>\n",
       "      <td>0.248126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>33.4</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>0.026805</td>\n",
       "      <td>0.150550</td>\n",
       "      <td>0.131985</td>\n",
       "      <td>0.134891</td>\n",
       "      <td>0.208300</td>\n",
       "      <td>0.181575</td>\n",
       "      <td>0.205117</td>\n",
       "      <td>0.150184</td>\n",
       "      <td>0.131642</td>\n",
       "      <td>0.135063</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>32.2</td>\n",
       "      <td>14.20</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.189842</td>\n",
       "      <td>0.033418</td>\n",
       "      <td>0.018961</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>0.141579</td>\n",
       "      <td>0.138338</td>\n",
       "      <td>0.341480</td>\n",
       "      <td>0.207390</td>\n",
       "      <td>0.197327</td>\n",
       "      <td>0.197330</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>0.138725</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>28.8</td>\n",
       "      <td>10.73</td>\n",
       "      <td>0.013750</td>\n",
       "      <td>0.322694</td>\n",
       "      <td>0.080315</td>\n",
       "      <td>0.061453</td>\n",
       "      <td>0.248442</td>\n",
       "      <td>0.158710</td>\n",
       "      <td>0.142326</td>\n",
       "      <td>0.457103</td>\n",
       "      <td>0.247391</td>\n",
       "      <td>0.228112</td>\n",
       "      <td>0.248598</td>\n",
       "      <td>0.158618</td>\n",
       "      <td>0.142817</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4334</th>\n",
       "      <td>4335</td>\n",
       "      <td>4487</td>\n",
       "      <td>4487</td>\n",
       "      <td>8.8</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.290182</td>\n",
       "      <td>0.290182</td>\n",
       "      <td>0.290182</td>\n",
       "      <td>0.283959</td>\n",
       "      <td>0.283959</td>\n",
       "      <td>0.283959</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.269486</td>\n",
       "      <td>0.269486</td>\n",
       "      <td>0.269486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4335</th>\n",
       "      <td>4336</td>\n",
       "      <td>4488</td>\n",
       "      <td>4488</td>\n",
       "      <td>10.8</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.408320</td>\n",
       "      <td>0.408320</td>\n",
       "      <td>0.408320</td>\n",
       "      <td>0.372648</td>\n",
       "      <td>0.372648</td>\n",
       "      <td>0.372648</td>\n",
       "      <td>0.510469</td>\n",
       "      <td>0.510469</td>\n",
       "      <td>0.510469</td>\n",
       "      <td>0.340879</td>\n",
       "      <td>0.340879</td>\n",
       "      <td>0.340879</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4336</th>\n",
       "      <td>4337</td>\n",
       "      <td>4489</td>\n",
       "      <td>4489</td>\n",
       "      <td>11.8</td>\n",
       "      <td>2.62</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.437195</td>\n",
       "      <td>0.413297</td>\n",
       "      <td>0.413297</td>\n",
       "      <td>0.235606</td>\n",
       "      <td>0.234926</td>\n",
       "      <td>0.234926</td>\n",
       "      <td>0.499074</td>\n",
       "      <td>0.485601</td>\n",
       "      <td>0.485601</td>\n",
       "      <td>0.227606</td>\n",
       "      <td>0.226722</td>\n",
       "      <td>0.226722</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4337</th>\n",
       "      <td>4338</td>\n",
       "      <td>4490</td>\n",
       "      <td>4490</td>\n",
       "      <td>11.2</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.570094</td>\n",
       "      <td>0.550652</td>\n",
       "      <td>0.550652</td>\n",
       "      <td>0.416298</td>\n",
       "      <td>0.388085</td>\n",
       "      <td>0.388085</td>\n",
       "      <td>0.640722</td>\n",
       "      <td>0.621694</td>\n",
       "      <td>0.621694</td>\n",
       "      <td>0.387576</td>\n",
       "      <td>0.363288</td>\n",
       "      <td>0.363288</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338</th>\n",
       "      <td>4339</td>\n",
       "      <td>4491</td>\n",
       "      <td>4491</td>\n",
       "      <td>14.8</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.487398</td>\n",
       "      <td>0.484593</td>\n",
       "      <td>0.484593</td>\n",
       "      <td>0.286897</td>\n",
       "      <td>0.277861</td>\n",
       "      <td>0.277861</td>\n",
       "      <td>0.543662</td>\n",
       "      <td>0.540401</td>\n",
       "      <td>0.540401</td>\n",
       "      <td>0.270742</td>\n",
       "      <td>0.263390</td>\n",
       "      <td>0.263390</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4339 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      OID_    Id  gridcode  Shape_Length  Shape_Area   CH_mean  ARVI_max  \\\n",
       "0        1     2         2          41.6       17.41  0.190714  0.609099   \n",
       "1        2     3         3          31.4        5.05  0.045000  0.387755   \n",
       "2        3     5         5          33.4        4.84  0.010000  0.038844   \n",
       "3        4     6         6          32.2       14.20  0.011667  0.189842   \n",
       "4        5     7         7          28.8       10.73  0.013750  0.322694   \n",
       "...    ...   ...       ...           ...         ...       ...       ...   \n",
       "4334  4335  4487      4487           8.8        2.30  0.020000  0.290182   \n",
       "4335  4336  4488      4488          10.8        2.03  0.020000  0.408320   \n",
       "4336  4337  4489      4489          11.8        2.62  0.015000  0.437195   \n",
       "4337  4338  4490      4490          11.2        3.09  0.015000  0.570094   \n",
       "4338  4339  4491      4491          14.8        3.33  0.865000  0.487398   \n",
       "\n",
       "      ARVI_mean  ARVI_med   EVI_max  EVI_mean   EVI_med  NDVI_max  NDVI_mean  \\\n",
       "0      0.344183  0.379428  0.443455  0.292047  0.290985  0.665698   0.476709   \n",
       "1      0.245301  0.274936  0.332059  0.252919  0.244477  0.526375   0.406745   \n",
       "2      0.004040  0.026805  0.150550  0.131985  0.134891  0.208300   0.181575   \n",
       "3      0.033418  0.018961  0.198972  0.141579  0.138338  0.341480   0.207390   \n",
       "4      0.080315  0.061453  0.248442  0.158710  0.142326  0.457103   0.247391   \n",
       "...         ...       ...       ...       ...       ...       ...        ...   \n",
       "4334   0.290182  0.290182  0.283959  0.283959  0.283959  0.419200   0.419200   \n",
       "4335   0.408320  0.408320  0.372648  0.372648  0.372648  0.510469   0.510469   \n",
       "4336   0.413297  0.413297  0.235606  0.234926  0.234926  0.499074   0.485601   \n",
       "4337   0.550652  0.550652  0.416298  0.388085  0.388085  0.640722   0.621694   \n",
       "4338   0.484593  0.484593  0.286897  0.277861  0.277861  0.543662   0.540401   \n",
       "\n",
       "      NDVI_med  SAVI_max  SAVI_mean  SAVI_med  Veg_class  \n",
       "0     0.507514  0.406991   0.286203  0.288351          1  \n",
       "1     0.433791  0.328585   0.254814  0.248126          0  \n",
       "2     0.205117  0.150184   0.131642  0.135063          0  \n",
       "3     0.197327  0.197330   0.141500  0.138725          0  \n",
       "4     0.228112  0.248598   0.158618  0.142817          0  \n",
       "...        ...       ...        ...       ...        ...  \n",
       "4334  0.419200  0.269486   0.269486  0.269486          1  \n",
       "4335  0.510469  0.340879   0.340879  0.340879          1  \n",
       "4336  0.485601  0.227606   0.226722  0.226722          1  \n",
       "4337  0.621694  0.387576   0.363288  0.363288          1  \n",
       "4338  0.540401  0.270742   0.263390  0.263390          1  \n",
       "\n",
       "[4339 rows x 19 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2017.Veg_class = df2017.Veg_class.map({'non-woody':0, 'woody':1})\n",
    "df2021.Veg_class = df2021.Veg_class.map({'non-woody':0, 'woody':1})\n",
    "dfJornada2017.Veg_class = dfJornada2017.Veg_class.map({'non-woody':0, 'woody':1})\n",
    "dfJornada2021.Veg_class = dfJornada2021.Veg_class.map({'non-woody':0, 'woody':1})\n",
    "#dfCombined.Veg_class = dfCombined.Veg_class.map({'non-woody':0, 'woody':1})\n",
    "df2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2649, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfJornada2017.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2017 Jorn (Training) to 2021 Jorn (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 5.833470599999998 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.703962703962704, 0.7289719626168224, 0.7009345794392523, 0.6915887850467289, 0.705607476635514]\n",
      "Avg accuracy: 0.7062131015402043\n",
      "Std of accuracy : \n",
      "0.012370369895507384\n",
      "\n",
      "[[ 162  536]\n",
      " [  93 1350]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.23      0.34       698\n",
      "           1       0.72      0.94      0.81      1443\n",
      "\n",
      "    accuracy                           0.71      2141\n",
      "   macro avg       0.68      0.58      0.58      2141\n",
      "weighted avg       0.69      0.71      0.66      2141\n",
      "\n",
      "Sensitivity: 0.23209169054441262\n",
      "Specificity: 0.9355509355509356\n",
      "Precision: 0.6352941176470588\n",
      "F1_Score: 0.3399790136411333\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 16.087129399999995 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7435897435897436, 0.6915887850467289, 0.735981308411215, 0.7126168224299065, 0.7476635514018691]\n",
      "Avg accuracy: 0.7262880421758926\n",
      "Std of accuracy : \n",
      "0.02117149674185346\n",
      "\n",
      "[[ 176  522]\n",
      " [  64 1379]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.25      0.38       698\n",
      "           1       0.73      0.96      0.82      1443\n",
      "\n",
      "    accuracy                           0.73      2141\n",
      "   macro avg       0.73      0.60      0.60      2141\n",
      "weighted avg       0.73      0.73      0.68      2141\n",
      "\n",
      "Sensitivity: 0.2521489971346705\n",
      "Specificity: 0.9556479556479557\n",
      "Precision: 0.7333333333333333\n",
      "F1_Score: 0.37526652452025583\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 36.08463090000001 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7435897435897436, 0.6752336448598131, 0.7219626168224299, 0.719626168224299, 0.6892523364485982]\n",
      "Avg accuracy: 0.7099329019889768\n",
      "Std of accuracy : \n",
      "0.024507879027805986\n",
      "\n",
      "[[  90  608]\n",
      " [  13 1430]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.13      0.22       698\n",
      "           1       0.70      0.99      0.82      1443\n",
      "\n",
      "    accuracy                           0.71      2141\n",
      "   macro avg       0.79      0.56      0.52      2141\n",
      "weighted avg       0.76      0.71      0.63      2141\n",
      "\n",
      "Sensitivity: 0.12893982808022922\n",
      "Specificity: 0.990990990990991\n",
      "Precision: 0.8737864077669902\n",
      "F1_Score: 0.22471910112359547\n"
     ]
    }
   ],
   "source": [
    "#Ada boost\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 79.1485632 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7365967365967366, 0.7219626168224299, 0.735981308411215, 0.7242990654205608, 0.7266355140186916]\n",
      "Avg accuracy: 0.7290950482539268\n",
      "Std of accuracy : \n",
      "0.006060004043954826\n",
      "\n",
      "[[ 185  513]\n",
      " [  67 1376]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.27      0.39       698\n",
      "           1       0.73      0.95      0.83      1443\n",
      "\n",
      "    accuracy                           0.73      2141\n",
      "   macro avg       0.73      0.61      0.61      2141\n",
      "weighted avg       0.73      0.73      0.68      2141\n",
      "\n",
      "Sensitivity: 0.26504297994269344\n",
      "Specificity: 0.9535689535689535\n",
      "Precision: 0.7341269841269841\n",
      "F1_Score: 0.3894736842105263\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2021 Jorn (Training) to 2017 Jorn (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 79.3651336 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6526806526806527, 0.735981308411215, 0.7126168224299065, 0.7266355140186916, 0.6612149532710281]\n",
      "Avg accuracy: 0.6978258501622988\n",
      "Std of accuracy : \n",
      "0.034301764424485105\n",
      "\n",
      "[[ 324  139]\n",
      " [ 508 1170]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.70      0.50       463\n",
      "           1       0.89      0.70      0.78      1678\n",
      "\n",
      "    accuracy                           0.70      2141\n",
      "   macro avg       0.64      0.70      0.64      2141\n",
      "weighted avg       0.78      0.70      0.72      2141\n",
      "\n",
      "Sensitivity: 0.6997840172786177\n",
      "Specificity: 0.6972586412395709\n",
      "Precision: 0.3894230769230769\n",
      "F1_Score: 0.5003861003861003\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 92.09981810000001 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.668997668997669, 0.7079439252336449, 0.7266355140186916, 0.7009345794392523, 0.7032710280373832]\n",
      "Avg accuracy: 0.7015565431453281\n",
      "Std of accuracy : \n",
      "0.018618055822609583\n",
      "\n",
      "[[ 319  144]\n",
      " [ 495 1183]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.69      0.50       463\n",
      "           1       0.89      0.71      0.79      1678\n",
      "\n",
      "    accuracy                           0.70      2141\n",
      "   macro avg       0.64      0.70      0.64      2141\n",
      "weighted avg       0.78      0.70      0.73      2141\n",
      "\n",
      "Sensitivity: 0.6889848812095032\n",
      "Specificity: 0.7050059594755661\n",
      "Precision: 0.3918918918918919\n",
      "F1_Score: 0.4996084573218481\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 121.83818459999999 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6643356643356644, 0.6682242990654206, 0.7219626168224299, 0.7313084112149533, 0.6939252336448598]\n",
      "Avg accuracy: 0.6959512450166656\n",
      "Std of accuracy : \n",
      "0.02719983036877213\n",
      "\n",
      "[[ 310  153]\n",
      " [ 498 1180]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.67      0.49       463\n",
      "           1       0.89      0.70      0.78      1678\n",
      "\n",
      "    accuracy                           0.70      2141\n",
      "   macro avg       0.63      0.69      0.64      2141\n",
      "weighted avg       0.78      0.70      0.72      2141\n",
      "\n",
      "Sensitivity: 0.6695464362850972\n",
      "Specificity: 0.7032181168057211\n",
      "Precision: 0.38366336633663367\n",
      "F1_Score: 0.48780487804878053\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 175.722528 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7365967365967366, 0.7219626168224299, 0.735981308411215, 0.7242990654205608, 0.7266355140186916]\n",
      "Avg accuracy: 0.7290950482539268\n",
      "Std of accuracy : \n",
      "0.006060004043954826\n",
      "\n",
      "[[ 185  513]\n",
      " [  67 1376]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.27      0.39       698\n",
      "           1       0.73      0.95      0.83      1443\n",
      "\n",
      "    accuracy                           0.73      2141\n",
      "   macro avg       0.73      0.61      0.61      2141\n",
      "weighted avg       0.73      0.73      0.68      2141\n",
      "\n",
      "Sensitivity: 0.26504297994269344\n",
      "Specificity: 0.9535689535689535\n",
      "Precision: 0.7341269841269841\n",
      "F1_Score: 0.3894736842105263\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2017 SRER (Training) to 2021 Jorn (Testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 175.9474684 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7249417249417249, 0.7336448598130841, 0.7780373831775701, 0.6448598130841121, 0.7289719626168224]\n",
      "Avg accuracy: 0.7220911487266626\n",
      "Std of accuracy : \n",
      "0.04308972030260952\n",
      "\n",
      "[[ 280  418]\n",
      " [ 177 1266]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.40      0.48       698\n",
      "           1       0.75      0.88      0.81      1443\n",
      "\n",
      "    accuracy                           0.72      2141\n",
      "   macro avg       0.68      0.64      0.65      2141\n",
      "weighted avg       0.71      0.72      0.70      2141\n",
      "\n",
      "Sensitivity: 0.40114613180515757\n",
      "Specificity: 0.8773388773388774\n",
      "Precision: 0.612691466083151\n",
      "F1_Score: 0.4848484848484848\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6) #criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 4012.5537627000003 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.662004662004662, 0.6425233644859814, 0.6892523364485982, 0.6565420560747663, 0.633177570093458]\n",
      "Avg accuracy: 0.6566999978214931\n",
      "Std of accuracy : \n",
      "0.019194273608216853\n",
      "\n",
      "[[ 404  294]\n",
      " [ 441 1002]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.58      0.52       698\n",
      "           1       0.77      0.69      0.73      1443\n",
      "\n",
      "    accuracy                           0.66      2141\n",
      "   macro avg       0.63      0.64      0.63      2141\n",
      "weighted avg       0.68      0.66      0.66      2141\n",
      "\n",
      "Sensitivity: 0.5787965616045845\n",
      "Specificity: 0.6943866943866944\n",
      "Precision: 0.47810650887573963\n",
      "F1_Score: 0.5236552171095268\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 4048.4801246 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6899766899766899, 0.6985981308411215, 0.6915887850467289, 0.647196261682243, 0.6495327102803738]\n",
      "Avg accuracy: 0.6753785155654315\n",
      "Std of accuracy : \n",
      "0.02225881915609393\n",
      "\n",
      "[[ 132  566]\n",
      " [ 129 1314]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.19      0.28       698\n",
      "           1       0.70      0.91      0.79      1443\n",
      "\n",
      "    accuracy                           0.68      2141\n",
      "   macro avg       0.60      0.55      0.53      2141\n",
      "weighted avg       0.64      0.68      0.62      2141\n",
      "\n",
      "Sensitivity: 0.18911174785100288\n",
      "Specificity: 0.9106029106029107\n",
      "Precision: 0.5057471264367817\n",
      "F1_Score: 0.27528675703858185\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 4102.1882469 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.627039627039627, 0.633177570093458, 0.6098130841121495, 0.6588785046728972, 0.6682242990654206]\n",
      "Avg accuracy: 0.6394266169967106\n",
      "Std of accuracy : \n",
      "0.02134072099999624\n",
      "\n",
      "[[472 226]\n",
      " [546 897]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.68      0.55       698\n",
      "           1       0.80      0.62      0.70      1443\n",
      "\n",
      "    accuracy                           0.64      2141\n",
      "   macro avg       0.63      0.65      0.62      2141\n",
      "weighted avg       0.69      0.64      0.65      2141\n",
      "\n",
      "Sensitivity: 0.6762177650429799\n",
      "Specificity: 0.6216216216216216\n",
      "Precision: 0.4636542239685658\n",
      "F1_Score: 0.55011655011655\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2021 SRER (Training) to 2021 Jorn (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 5150.1953007 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.4382284382284382, 0.5630841121495327, 0.5537383177570093, 0.5093457943925234, 0.5280373831775701]\n",
      "Avg accuracy: 0.5184868091410147\n",
      "Std of accuracy : \n",
      "0.04437964851581787\n",
      "\n",
      "[[635  63]\n",
      " [968 475]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.91      0.55       698\n",
      "           1       0.88      0.33      0.48      1443\n",
      "\n",
      "    accuracy                           0.52      2141\n",
      "   macro avg       0.64      0.62      0.52      2141\n",
      "weighted avg       0.72      0.52      0.50      2141\n",
      "\n",
      "Sensitivity: 0.9097421203438395\n",
      "Specificity: 0.32917532917532916\n",
      "Precision: 0.39613225202744856\n",
      "F1_Score: 0.5519339417644503\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6) #criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 5158.297985900001 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.5431235431235432, 0.5210280373831776, 0.544392523364486, 0.544392523364486, 0.5163551401869159]\n",
      "Avg accuracy: 0.5338583534845217\n",
      "Std of accuracy : \n",
      "0.012480069680645366\n",
      "\n",
      "[[520 178]\n",
      " [820 623]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.74      0.51       698\n",
      "           1       0.78      0.43      0.56      1443\n",
      "\n",
      "    accuracy                           0.53      2141\n",
      "   macro avg       0.58      0.59      0.53      2141\n",
      "weighted avg       0.65      0.53      0.54      2141\n",
      "\n",
      "Sensitivity: 0.7449856733524355\n",
      "Specificity: 0.43173943173943174\n",
      "Precision: 0.3880597014925373\n",
      "F1_Score: 0.5103042198233562\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 5177.1753029 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.5361305361305362, 0.5584112149532711, 0.5233644859813084, 0.5093457943925234, 0.5397196261682243]\n",
      "Avg accuracy: 0.5333943315251727\n",
      "Std of accuracy : \n",
      "0.016445489133942365\n",
      "\n",
      "[[670  28]\n",
      " [971 472]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.96      0.57       698\n",
      "           1       0.94      0.33      0.49      1443\n",
      "\n",
      "    accuracy                           0.53      2141\n",
      "   macro avg       0.68      0.64      0.53      2141\n",
      "weighted avg       0.77      0.53      0.51      2141\n",
      "\n",
      "Sensitivity: 0.9598853868194842\n",
      "Specificity: 0.3270963270963271\n",
      "Precision: 0.4082876294942108\n",
      "F1_Score: 0.572894399315947\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 5226.7170587 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.5081585081585082, 0.5, 0.5116822429906542, 0.5186915887850467, 0.4976635514018692]\n",
      "Avg accuracy: 0.5072391782672157\n",
      "Std of accuracy : \n",
      "0.007692100234701685\n",
      "\n",
      "[[540 158]\n",
      " [897 546]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.77      0.51       698\n",
      "           1       0.78      0.38      0.51      1443\n",
      "\n",
      "    accuracy                           0.51      2141\n",
      "   macro avg       0.58      0.58      0.51      2141\n",
      "weighted avg       0.65      0.51      0.51      2141\n",
      "\n",
      "Sensitivity: 0.7736389684813754\n",
      "Specificity: 0.3783783783783784\n",
      "Precision: 0.3757828810020877\n",
      "F1_Score: 0.5058548009367682\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2017 SRER TO 2017 JORN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 3416.8259887000004 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.4377358490566038, 0.7396226415094339, 0.690566037735849, 0.6320754716981132, 0.6937618147448015]\n",
      "Avg accuracy: 0.6387523629489603\n",
      "Std of accuracy : \n",
      "0.10614869713084527\n",
      "\n",
      "[[ 312  365]\n",
      " [ 592 1380]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.46      0.39       677\n",
      "           1       0.79      0.70      0.74      1972\n",
      "\n",
      "    accuracy                           0.64      2649\n",
      "   macro avg       0.57      0.58      0.57      2649\n",
      "weighted avg       0.68      0.64      0.65      2649\n",
      "\n",
      "Sensitivity: 0.4608567208271787\n",
      "Specificity: 0.6997971602434077\n",
      "Precision: 0.34513274336283184\n",
      "F1_Score: 0.3946869070208729\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 3525.7383508000003 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6547169811320754, 0.7, 0.6811320754716981, 0.720754716981132, 0.720226843100189]\n",
      "Avg accuracy: 0.695366123337019\n",
      "Std of accuracy : \n",
      "0.02505621586723127\n",
      "\n",
      "[[ 419  258]\n",
      " [ 549 1423]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.62      0.51       677\n",
      "           1       0.85      0.72      0.78      1972\n",
      "\n",
      "    accuracy                           0.70      2649\n",
      "   macro avg       0.64      0.67      0.64      2649\n",
      "weighted avg       0.74      0.70      0.71      2649\n",
      "\n",
      "Sensitivity: 0.6189069423929099\n",
      "Specificity: 0.7216024340770791\n",
      "Precision: 0.43285123966942146\n",
      "F1_Score: 0.5094224924012157\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 3520.0201810000003 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6773584905660377, 0.7169811320754716, 0.7018867924528301, 0.7037735849056603, 0.7391304347826086]\n",
      "Avg accuracy: 0.7078260869565216\n",
      "Std of accuracy : \n",
      "0.020217563977066547\n",
      "\n",
      "[[ 126  551]\n",
      " [ 223 1749]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.19      0.25       677\n",
      "           1       0.76      0.89      0.82      1972\n",
      "\n",
      "    accuracy                           0.71      2649\n",
      "   macro avg       0.56      0.54      0.53      2649\n",
      "weighted avg       0.66      0.71      0.67      2649\n",
      "\n",
      "Sensitivity: 0.1861152141802068\n",
      "Specificity: 0.8869168356997972\n",
      "Precision: 0.36103151862464183\n",
      "F1_Score: 0.2456140350877193\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 3485.9874314000003 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.4377358490566038, 0.44528301886792454, 0.39622641509433965, 0.439622641509434, 0.40831758034026466]\n",
      "Avg accuracy: 0.4254371009737133\n",
      "Std of accuracy : \n",
      "0.019456060125437852\n",
      "\n",
      "[[ 521  156]\n",
      " [1366  606]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.77      0.41       677\n",
      "           1       0.80      0.31      0.44      1972\n",
      "\n",
      "    accuracy                           0.43      2649\n",
      "   macro avg       0.54      0.54      0.42      2649\n",
      "weighted avg       0.66      0.43      0.43      2649\n",
      "\n",
      "Sensitivity: 0.7695716395864106\n",
      "Specificity: 0.30730223123732253\n",
      "Precision: 0.27609962904080554\n",
      "F1_Score: 0.4063962558502341\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 212.57865160000006 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7860125260960334, 0.7640918580375783, 0.7954070981210856, 0.7860125260960334, 0.7682672233820459]\n",
      "Avg accuracy: 0.7799582463465553\n",
      "Std of accuracy : \n",
      "0.011835522157042486\n",
      "\n",
      "[[ 735  640]\n",
      " [ 414 3001]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.64      0.53      0.58      1375\n",
      "       woody       0.82      0.88      0.85      3415\n",
      "\n",
      "    accuracy                           0.78      4790\n",
      "   macro avg       0.73      0.71      0.72      4790\n",
      "weighted avg       0.77      0.78      0.77      4790\n",
      "\n",
      "Sensitivity: 0.5345454545454545\n",
      "Specificity: 0.8787701317715959\n",
      "Precision: 0.639686684073107\n",
      "F1_Score: 0.5824088748019018\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfCombined): \n",
    "    \n",
    "    X_train = dfCombined.iloc[train_index, 5:19]\n",
    "    X_test = dfCombined.iloc[test_index, 5:19]\n",
    "    Y_train = dfCombined.iloc[train_index, -1]\n",
    "    Y_test = dfCombined.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1233.5199241 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.8037578288100209, 0.791231732776618, 0.7901878914405011, 0.778705636743215, 0.7755741127348643]\n",
      "Avg accuracy: 0.7878914405010439\n",
      "Std of accuracy : \n",
      "0.010042599975620623\n",
      "\n",
      "[[ 739  636]\n",
      " [ 380 3035]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.66      0.54      0.59      1375\n",
      "       woody       0.83      0.89      0.86      3415\n",
      "\n",
      "    accuracy                           0.79      4790\n",
      "   macro avg       0.74      0.71      0.72      4790\n",
      "weighted avg       0.78      0.79      0.78      4790\n",
      "\n",
      "Sensitivity: 0.5374545454545454\n",
      "Specificity: 0.8887262079062958\n",
      "Precision: 0.6604110813226095\n",
      "F1_Score: 0.5926222935044106\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfCombined): \n",
    "    \n",
    "    X_train = dfCombined.iloc[train_index, 5:19]\n",
    "    X_test = dfCombined.iloc[test_index, 5:19]\n",
    "    Y_train = dfCombined.iloc[train_index, -1]\n",
    "    Y_test = dfCombined.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1120.7830826999998 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7964509394572025, 0.8037578288100209, 0.8131524008350731, 0.7797494780793319, 0.7818371607515657]\n",
      "Avg accuracy: 0.7949895615866389\n",
      "Std of accuracy : \n",
      "0.01276050671393416\n",
      "\n",
      "[[ 707  668]\n",
      " [ 314 3101]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.69      0.51      0.59      1375\n",
      "       woody       0.82      0.91      0.86      3415\n",
      "\n",
      "    accuracy                           0.79      4790\n",
      "   macro avg       0.76      0.71      0.73      4790\n",
      "weighted avg       0.79      0.79      0.78      4790\n",
      "\n",
      "Sensitivity: 0.5141818181818182\n",
      "Specificity: 0.9080527086383602\n",
      "Precision: 0.692458374142997\n",
      "F1_Score: 0.5901502504173622\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfCombined): \n",
    "    \n",
    "    X_train = dfCombined.iloc[train_index, 5:19]\n",
    "    X_test = dfCombined.iloc[test_index, 5:19]\n",
    "    Y_train = dfCombined.iloc[train_index, -1]\n",
    "    Y_test = dfCombined.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1479.9852984 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.802713987473904, 0.791231732776618, 0.7964509394572025, 0.7933194154488518, 0.7901878914405011]\n",
      "Avg accuracy: 0.7947807933194154\n",
      "Std of accuracy : \n",
      "0.004506687504159273\n",
      "\n",
      "[[ 774  601]\n",
      " [ 382 3033]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.67      0.56      0.61      1375\n",
      "       woody       0.83      0.89      0.86      3415\n",
      "\n",
      "    accuracy                           0.79      4790\n",
      "   macro avg       0.75      0.73      0.74      4790\n",
      "weighted avg       0.79      0.79      0.79      4790\n",
      "\n",
      "Sensitivity: 0.5629090909090909\n",
      "Specificity: 0.8881405563689605\n",
      "Precision: 0.6695501730103807\n",
      "F1_Score: 0.611615962070328\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfCombined): \n",
    "    \n",
    "    X_train = dfCombined.iloc[train_index, 5:19]\n",
    "    X_test = dfCombined.iloc[test_index, 5:19]\n",
    "    Y_train = dfCombined.iloc[train_index, -1]\n",
    "    Y_test = dfCombined.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 12.276497200000023 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6889400921658986, 0.7108294930875576, 0.6785714285714286, 0.7131336405529954, 0.7231833910034602]\n",
      "Avg accuracy: 0.7029316090762681\n",
      "Std of accuracy : \n",
      "0.016527393194427125\n",
      "\n",
      "[[ 312 1147]\n",
      " [ 142 2738]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.21      0.33      1459\n",
      "           1       0.70      0.95      0.81      2880\n",
      "\n",
      "    accuracy                           0.70      4339\n",
      "   macro avg       0.70      0.58      0.57      4339\n",
      "weighted avg       0.70      0.70      0.65      4339\n",
      "\n",
      "0.21384509938313914\n",
      "0.9506944444444444\n",
      "0.6872246696035242\n",
      "0.3261892315734448\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 101.29513110000002 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7684331797235023, 0.7695852534562212, 0.7707373271889401, 0.7557603686635944, 0.7681660899653979]\n",
      "Avg accuracy: 0.7665364437995311\n",
      "Std of accuracy : \n",
      "0.005464792964069163\n",
      "\n",
      "[[ 720  739]\n",
      " [ 274 2606]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.49      0.59      1459\n",
      "           1       0.78      0.90      0.84      2880\n",
      "\n",
      "    accuracy                           0.77      4339\n",
      "   macro avg       0.75      0.70      0.71      4339\n",
      "weighted avg       0.76      0.77      0.75      4339\n",
      "\n",
      "0.49348869088416725\n",
      "0.9048611111111111\n",
      "0.7243460764587525\n",
      "0.5870362821035466\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 141.37268930000002 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.783410138248848, 0.7695852534562212, 0.7730414746543779, 0.738479262672811, 0.7358708189158016]\n",
      "Avg accuracy: 0.7600773895896119\n",
      "Std of accuracy : \n",
      "0.019263028250864164\n",
      "\n",
      "[[ 658  801]\n",
      " [ 240 2640]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.45      0.56      1459\n",
      "           1       0.77      0.92      0.84      2880\n",
      "\n",
      "    accuracy                           0.76      4339\n",
      "   macro avg       0.75      0.68      0.70      4339\n",
      "weighted avg       0.76      0.76      0.74      4339\n",
      "\n",
      "0.45099383139136395\n",
      "0.9166666666666666\n",
      "0.732739420935412\n",
      "0.5583368689011454\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging 2017 -> 2021 SRER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 803.2861290000001 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7684331797235023, 0.7695852534562212, 0.7707373271889401, 0.7557603686635944, 0.7681660899653979]\n",
      "Avg accuracy: 0.7665364437995311\n",
      "Std of accuracy : \n",
      "0.005464792964069163\n",
      "\n",
      "[[ 720  739]\n",
      " [ 274 2606]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.49      0.59      1459\n",
      "           1       0.78      0.90      0.84      2880\n",
      "\n",
      "    accuracy                           0.77      4339\n",
      "   macro avg       0.75      0.70      0.71      4339\n",
      "weighted avg       0.76      0.77      0.75      4339\n",
      "\n",
      "0.49348869088416725\n",
      "0.9048611111111111\n",
      "0.7243460764587525\n",
      "0.5870362821035466\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest 2017 -> 2021 SRER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 874.8300282 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.783410138248848, 0.7695852534562212, 0.7730414746543779, 0.738479262672811, 0.7358708189158016]\n",
      "Avg accuracy: 0.7600773895896119\n",
      "Std of accuracy : \n",
      "0.019263028250864164\n",
      "\n",
      "[[ 658  801]\n",
      " [ 240 2640]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.45      0.56      1459\n",
      "           1       0.77      0.92      0.84      2880\n",
      "\n",
      "    accuracy                           0.76      4339\n",
      "   macro avg       0.75      0.68      0.70      4339\n",
      "weighted avg       0.76      0.76      0.74      4339\n",
      "\n",
      "0.45099383139136395\n",
      "0.9166666666666666\n",
      "0.732739420935412\n",
      "0.5583368689011454\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging 2021 -> 2017 SRER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1032.5774026 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.4619815668202765, 0.4804147465437788, 0.5023041474654378, 0.4066820276497696, 0.44521337946943484]\n",
      "Avg accuracy: 0.45931917358973956\n",
      "Std of accuracy : \n",
      "0.0324637092013152\n",
      "\n",
      "[[ 413 1675]\n",
      " [ 671 1580]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.20      0.26      2088\n",
      "           1       0.49      0.70      0.57      2251\n",
      "\n",
      "    accuracy                           0.46      4339\n",
      "   macro avg       0.43      0.45      0.42      4339\n",
      "weighted avg       0.44      0.46      0.42      4339\n",
      "\n",
      "0.1977969348659004\n",
      "0.7019102621057308\n",
      "0.38099630996309963\n",
      "0.2604035308953342\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest 2021 -> 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1068.7303933 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.4827188940092166, 0.5357142857142857, 0.5506912442396313, 0.49193548387096775, 0.4867358708189158]\n",
      "Avg accuracy: 0.5095591557306035\n",
      "Std of accuracy : \n",
      "0.028027969326465837\n",
      "\n",
      "[[ 474 1614]\n",
      " [ 514 1737]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.23      0.31      2088\n",
      "           1       0.52      0.77      0.62      2251\n",
      "\n",
      "    accuracy                           0.51      4339\n",
      "   macro avg       0.50      0.50      0.46      4339\n",
      "weighted avg       0.50      0.51      0.47      4339\n",
      "\n",
      "0.22701149425287356\n",
      "0.7716570413149711\n",
      "0.4797570850202429\n",
      "0.3081924577373212\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging Combined SRER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 6733.0768624 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7546972860125261, 0.7985386221294363, 0.7933194154488518, 0.7860125260960334, 0.7807933194154488]\n",
      "Avg accuracy: 0.7826722338204593\n",
      "Std of accuracy : \n",
      "0.015247231429236091\n",
      "\n",
      "[[ 764  611]\n",
      " [ 430 2985]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.56      0.59      1375\n",
      "           1       0.83      0.87      0.85      3415\n",
      "\n",
      "    accuracy                           0.78      4790\n",
      "   macro avg       0.73      0.71      0.72      4790\n",
      "weighted avg       0.78      0.78      0.78      4790\n",
      "\n",
      "0.5556363636363636\n",
      "0.8740849194729137\n",
      "0.6398659966499163\n",
      "0.5947839626313741\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfCombined): \n",
    "    \n",
    "    X_train = dfCombined.iloc[train_index, 5:18]\n",
    "    X_test = dfCombined.iloc[test_index, 5:18]\n",
    "    Y_train = dfCombined.iloc[train_index, -1]\n",
    "    Y_test = dfCombined.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Models With Same Testing and Validation Sets (SRER 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1678.0443172 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6705069124423964, 0.6831797235023042, 0.663594470046083, 0.6947004608294931, 0.6724336793540946]\n",
      "Avg accuracy: 0.6768830492348743\n",
      "Std of accuracy : \n",
      "0.010900886315812387\n",
      "\n",
      "[[1431  657]\n",
      " [ 745 1506]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.66      0.69      0.67      2088\n",
      "       woody       0.70      0.67      0.68      2251\n",
      "\n",
      "    accuracy                           0.68      4339\n",
      "   macro avg       0.68      0.68      0.68      4339\n",
      "weighted avg       0.68      0.68      0.68      4339\n",
      "\n",
      "Sensitivity: 0.669035984007108\n",
      "Specificity: 0.6853448275862069\n",
      "precision: 0.6576286764705882\n",
      "f1_score: 0.671200750469043\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1906.2882643 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7684331797235023, 0.7695852534562212, 0.7707373271889401, 0.7557603686635944, 0.7681660899653979]\n",
      "Avg accuracy: 0.7665364437995311\n",
      "Std of accuracy : \n",
      "0.005464792964069163\n",
      "\n",
      "[[ 720  739]\n",
      " [ 274 2606]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.72      0.49      0.59      1459\n",
      "       woody       0.78      0.90      0.84      2880\n",
      "\n",
      "    accuracy                           0.77      4339\n",
      "   macro avg       0.75      0.70      0.71      4339\n",
      "weighted avg       0.76      0.77      0.75      4339\n",
      "\n",
      "Sensitivity: 0.9048611111111111\n",
      "Specificity: 0.49348869088416725\n",
      "precision: 0.7243460764587525\n",
      "f1_score: 0.5870362821035466\n"
     ]
    }
   ],
   "source": [
    "#Bagging Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 6983.7549239 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.728110599078341, 0.7223502304147466, 0.684331797235023, 0.6981566820276498, 0.6782006920415224]\n",
      "Avg accuracy: 0.7022300001594566\n",
      "Std of accuracy : \n",
      "0.019944795326499436\n",
      "\n",
      "[[ 229 1230]\n",
      " [  62 2818]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.16      0.26      1459\n",
      "           1       0.70      0.98      0.81      2880\n",
      "\n",
      "    accuracy                           0.70      4339\n",
      "   macro avg       0.74      0.57      0.54      4339\n",
      "weighted avg       0.73      0.70      0.63      4339\n",
      "\n",
      "0.15695681973954764\n",
      "0.9784722222222222\n",
      "0.7869415807560137\n",
      "0.26171428571428573\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 7030.386528200001 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.783410138248848, 0.7695852534562212, 0.7730414746543779, 0.738479262672811, 0.7358708189158016]\n",
      "Avg accuracy: 0.7600773895896119\n",
      "Std of accuracy : \n",
      "0.019263028250864164\n",
      "\n",
      "[[ 658  801]\n",
      " [ 240 2640]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.45      0.56      1459\n",
      "           1       0.77      0.92      0.84      2880\n",
      "\n",
      "    accuracy                           0.76      4339\n",
      "   macro avg       0.75      0.68      0.70      4339\n",
      "weighted avg       0.76      0.76      0.74      4339\n",
      "\n",
      "0.45099383139136395\n",
      "0.9166666666666666\n",
      "0.732739420935412\n",
      "0.5583368689011454\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 556.8389994000008 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6543778801843319, 0.6751152073732719, 0.6716589861751152, 0.684331797235023, 0.6620530565167243]\n",
      "Avg accuracy: 0.6695073854968933\n",
      "Std of accuracy : \n",
      "0.010395279569157593\n",
      "\n",
      "[[1376  712]\n",
      " [ 722 1529]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.66      0.66      0.66      2088\n",
      "       woody       0.68      0.68      0.68      2251\n",
      "\n",
      "    accuracy                           0.67      4339\n",
      "   macro avg       0.67      0.67      0.67      4339\n",
      "weighted avg       0.67      0.67      0.67      4339\n",
      "\n",
      "0.6590038314176245\n",
      "0.679253665037761\n",
      "0.655862726406101\n",
      "0.6574295269947443\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Models With Same Testing and Validation Sets (Jornada 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 599.1766062000006 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7358490566037735, 0.7566037735849057, 0.7509433962264151, 0.7396226415094339, 0.7580340264650284]\n",
      "Avg accuracy: 0.7482105788779113\n",
      "Std of accuracy : \n",
      "0.00895515529430432\n",
      "\n",
      "[[ 256  421]\n",
      " [ 246 1726]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.51      0.38      0.43       677\n",
      "       woody       0.80      0.88      0.84      1972\n",
      "\n",
      "    accuracy                           0.75      2649\n",
      "   macro avg       0.66      0.63      0.64      2649\n",
      "weighted avg       0.73      0.75      0.73      2649\n",
      "\n",
      "0.37813884785819796\n",
      "0.8752535496957403\n",
      "0.5099601593625498\n",
      "0.4342663273960984\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    #Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Truth.extend(Y_test.values)\n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 667.454091900001 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7490566037735849, 0.7943396226415095, 0.7849056603773585, 0.7584905660377359, 0.7731568998109641]\n",
      "Avg accuracy: 0.7719898705282305\n",
      "Std of accuracy : \n",
      "0.01658829075019749\n",
      "\n",
      "[[ 249  428]\n",
      " [ 176 1796]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.59      0.37      0.45       677\n",
      "       woody       0.81      0.91      0.86      1972\n",
      "\n",
      "    accuracy                           0.77      2649\n",
      "   macro avg       0.70      0.64      0.65      2649\n",
      "weighted avg       0.75      0.77      0.75      2649\n",
      "\n",
      "0.36779911373707536\n",
      "0.9107505070993914\n",
      "0.5858823529411765\n",
      "0.45190562613430135\n"
     ]
    }
   ],
   "source": [
    "#Bagging Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 805.6072431000011 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7490566037735849, 0.7716981132075472, 0.7849056603773585, 0.7433962264150943, 0.7334593572778828]\n",
      "Avg accuracy: 0.7565031922102936\n",
      "Std of accuracy : \n",
      "0.018951772382488987\n",
      "\n",
      "[[ 182  495]\n",
      " [ 150 1822]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.55      0.27      0.36       677\n",
      "       woody       0.79      0.92      0.85      1972\n",
      "\n",
      "    accuracy                           0.76      2649\n",
      "   macro avg       0.67      0.60      0.61      2649\n",
      "weighted avg       0.73      0.76      0.72      2649\n",
      "\n",
      "0.2688330871491876\n",
      "0.9239350912778904\n",
      "0.5481927710843374\n",
      "0.36075322101090185\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 919.0166708000015 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7509433962264151, 0.7849056603773585, 0.7849056603773585, 0.7433962264150943, 0.7693761814744802]\n",
      "Avg accuracy: 0.7667054249741414\n",
      "Std of accuracy : \n",
      "0.017096142052863108\n",
      "\n",
      "[[ 237  440]\n",
      " [ 178 1794]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.57      0.35      0.43       677\n",
      "       woody       0.80      0.91      0.85      1972\n",
      "\n",
      "    accuracy                           0.77      2649\n",
      "   macro avg       0.69      0.63      0.64      2649\n",
      "weighted avg       0.74      0.77      0.75      2649\n",
      "\n",
      "0.3500738552437223\n",
      "0.90973630831643\n",
      "0.5710843373493976\n",
      "0.43406593406593413\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1084.0229143000015 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7641509433962265, 0.7886792452830189, 0.7754716981132076, 0.7547169811320755, 0.7712665406427222]\n",
      "Avg accuracy: 0.7708570817134502\n",
      "Std of accuracy : \n",
      "0.011354003640467623\n",
      "\n",
      "[[ 278  399]\n",
      " [ 208 1764]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.57      0.41      0.48       677\n",
      "       woody       0.82      0.89      0.85      1972\n",
      "\n",
      "    accuracy                           0.77      2649\n",
      "   macro avg       0.69      0.65      0.67      2649\n",
      "weighted avg       0.75      0.77      0.76      2649\n",
      "\n",
      "0.41063515509601184\n",
      "0.8945233265720081\n",
      "0.5720164609053497\n",
      "0.47807394668959585\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting models (SRER 2017 -> SRER 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 2677.9494367 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6693548387096774, 0.6854838709677419, 0.663594470046083, 0.6877880184331797, 0.6735870818915801]\n",
      "Avg accuracy: 0.6759616560096524\n",
      "Std of accuracy : \n",
      "0.009303454752033341\n",
      "\n",
      "[[1429  659]\n",
      " [ 747 1504]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.66      0.68      0.67      2088\n",
      "       woody       0.70      0.67      0.68      2251\n",
      "\n",
      "    accuracy                           0.68      4339\n",
      "   macro avg       0.68      0.68      0.68      4339\n",
      "weighted avg       0.68      0.68      0.68      4339\n",
      "\n",
      "Sensitivity: 0.6681474900044425\n",
      "Specificity: 0.6843869731800766\n",
      "precision: 0.6567095588235294\n",
      "f1_score: 0.6702626641651033\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9759245470340034\n",
      "Specificity: 0.10873983739837398\n",
      "precision: 0.6881028938906752\n",
      "f1_score: 0.18780166739798157\n",
      "[0.6913456728364182]\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 4636.7896439 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6682027649769585, 0.6612903225806451, 0.6658986175115207, 0.7027649769585254, 0.6678200692041523]\n",
      "Avg accuracy: 0.6731953502463603\n",
      "Std of accuracy : \n",
      "0.01498769075059182\n",
      "\n",
      "[[1429  659]\n",
      " [ 759 1492]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.68      0.67      2088\n",
      "           1       0.69      0.66      0.68      2251\n",
      "\n",
      "    accuracy                           0.67      4339\n",
      "   macro avg       0.67      0.67      0.67      4339\n",
      "weighted avg       0.67      0.67      0.67      4339\n",
      "\n",
      "Sensitivity: 0.6628165259884495\n",
      "Specificity: 0.6843869731800766\n",
      "precision: 0.653107861060329\n",
      "f1_score: 0.6683816651075771\n"
     ]
    }
   ],
   "source": [
    "#Bagging Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9106478034251675\n",
      "Specificity: 0.47764227642276424\n",
      "precision: 0.7230769230769231\n",
      "f1_score: 0.5752753977968177\n",
      "[0.7685509421377356]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(acc_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 5315.482194800001 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.728110599078341, 0.7223502304147466, 0.684331797235023, 0.6981566820276498, 0.6782006920415224]\n",
      "Avg accuracy: 0.7022300001594566\n",
      "Std of accuracy : \n",
      "0.019944795326499436\n",
      "\n",
      "[[ 229 1230]\n",
      " [  62 2818]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.16      0.26      1459\n",
      "           1       0.70      0.98      0.81      2880\n",
      "\n",
      "    accuracy                           0.70      4339\n",
      "   macro avg       0.74      0.57      0.54      4339\n",
      "weighted avg       0.73      0.70      0.63      4339\n",
      "\n",
      "Sensitivity: 0.9784722222222222\n",
      "Specificity: 0.15695681973954764\n",
      "precision: 0.7869415807560137\n",
      "f1_score: 0.26171428571428573\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9731943410275503\n",
      "Specificity: 0.15853658536585366\n",
      "precision: 0.7428571428571429\n",
      "f1_score: 0.2613065326633166\n",
      "[0.7058529264632316]\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 5519.5439386 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6543778801843319, 0.6751152073732719, 0.6716589861751152, 0.684331797235023, 0.6620530565167243]\n",
      "Avg accuracy: 0.6695073854968933\n",
      "Std of accuracy : \n",
      "0.010395279569157593\n",
      "\n",
      "[[1376  712]\n",
      " [ 722 1529]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66      2088\n",
      "           1       0.68      0.68      0.68      2251\n",
      "\n",
      "    accuracy                           0.67      4339\n",
      "   macro avg       0.67      0.67      0.67      4339\n",
      "weighted avg       0.67      0.67      0.67      4339\n",
      "\n",
      "Sensitivity: 0.679253665037761\n",
      "Specificity: 0.6590038314176245\n",
      "precision: 0.655862726406101\n",
      "f1_score: 0.6574295269947443\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.8935219657483247\n",
      "Specificity: 0.6265243902439024\n",
      "precision: 0.7418772563176895\n",
      "f1_score: 0.6793388429752066\n",
      "Accuracy: [0.8059029514757379]\n"
     ]
    }
   ],
   "source": [
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting 17 JORN -> 21 SRER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1783.0200263 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7339622641509433, 0.7566037735849057, 0.7566037735849057, 0.7415094339622641, 0.7637051039697542]\n",
      "Avg accuracy: 0.7504768698505545\n",
      "Std of accuracy : \n",
      "0.010983726002378257\n",
      "\n",
      "[[ 258  419]\n",
      " [ 242 1730]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.38      0.44       677\n",
      "           1       0.81      0.88      0.84      1972\n",
      "\n",
      "    accuracy                           0.75      2649\n",
      "   macro avg       0.66      0.63      0.64      2649\n",
      "weighted avg       0.73      0.75      0.74      2649\n",
      "\n",
      "0.3810930576070901\n",
      "0.8772819472616633\n",
      "0.516\n",
      "0.43840271877655057\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    #Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Truth.extend(Y_test.values)\n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.6872673119880863\n",
      "Specificity: 0.3399390243902439\n",
      "precision: 0.3468118195956454\n",
      "f1_score: 0.34334103156274054\n",
      "Accuracy: [0.5732866433216608]\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 2370.8974209 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6682027649769585, 0.6612903225806451, 0.6658986175115207, 0.7027649769585254, 0.6678200692041523]\n",
      "Avg accuracy: 0.6731953502463603\n",
      "Std of accuracy : \n",
      "0.01498769075059182\n",
      "\n",
      "[[1429  659]\n",
      " [ 759 1492]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.68      0.67      2088\n",
      "           1       0.69      0.66      0.68      2251\n",
      "\n",
      "    accuracy                           0.67      4339\n",
      "   macro avg       0.67      0.67      0.67      4339\n",
      "weighted avg       0.67      0.67      0.67      4339\n",
      "\n",
      "Sensitivity: 0.6628165259884495\n",
      "Specificity: 0.6843869731800766\n",
      "precision: 0.653107861060329\n",
      "f1_score: 0.6683816651075771\n"
     ]
    }
   ],
   "source": [
    "#Bagging Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.8624968974931745\n",
      "Specificity: 0.2921747967479675\n",
      "precision: 0.5093002657218778\n",
      "f1_score: 0.37132709073296744\n",
      "Accuracy: [0.6753376688344173]\n"
     ]
    }
   ],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 2325.9936654 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7715617715617715, 0.8177570093457944, 0.7850467289719626, 0.7780373831775701, 0.8130841121495327]\n",
      "Avg accuracy: 0.7930974010413262\n",
      "Std of accuracy : \n",
      "0.01877746361231171\n",
      "\n",
      "[[  71  392]\n",
      " [  51 1627]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.15      0.24       463\n",
      "           1       0.81      0.97      0.88      1678\n",
      "\n",
      "    accuracy                           0.79      2141\n",
      "   macro avg       0.69      0.56      0.56      2141\n",
      "weighted avg       0.76      0.79      0.74      2141\n",
      "\n",
      "Sensitivity: 0.15334773218142547\n",
      "Specificity: 0.9696066746126341\n",
      "Precision: 0.5819672131147541\n",
      "F1_Score: 0.2427350427350427\n"
     ]
    }
   ],
   "source": [
    "#Ada boost\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9826259617771159\n",
      "Specificity: 0.0350609756097561\n",
      "precision: 0.49640287769784175\n",
      "f1_score: 0.06549596582819175\n",
      "Accuracy: [0.671669167917292]\n"
     ]
    }
   ],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 2085.4481198 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7641509433962265, 0.7886792452830189, 0.7754716981132076, 0.7547169811320755, 0.7712665406427222]\n",
      "Avg accuracy: 0.7708570817134502\n",
      "Std of accuracy : \n",
      "0.011354003640467623\n",
      "\n",
      "[[ 278  399]\n",
      " [ 208 1764]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.41      0.48       677\n",
      "           1       0.82      0.89      0.85      1972\n",
      "\n",
      "    accuracy                           0.77      2649\n",
      "   macro avg       0.69      0.65      0.67      2649\n",
      "weighted avg       0.75      0.77      0.76      2649\n",
      "\n",
      "Sensitivity: 0.41063515509601184\n",
      "Specificity: 0.8945233265720081\n",
      "Precision: 0.5720164609053497\n",
      "F1_Score: 0.47807394668959585\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.8870687515512534\n",
      "Specificity: 0.5492886178861789\n",
      "precision: 0.7037760416666666\n",
      "f1_score: 0.6170091324200914\n",
      "Accuracy: [0.776221444055361]\n"
     ]
    }
   ],
   "source": [
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17 Jornada -> 17 SRER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 3187.1873653 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7339622641509433, 0.7584905660377359, 0.7547169811320755, 0.7433962264150943, 0.7542533081285444]\n",
      "Avg accuracy: 0.7489638691728787\n",
      "Std of accuracy : \n",
      "0.009031956753086375\n",
      "\n",
      "[[ 252  425]\n",
      " [ 240 1732]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.51      0.37      0.43       677\n",
      "       woody       0.80      0.88      0.84      1972\n",
      "\n",
      "    accuracy                           0.75      2649\n",
      "   macro avg       0.66      0.63      0.64      2649\n",
      "weighted avg       0.73      0.75      0.73      2649\n",
      "\n",
      "0.3722304283604136\n",
      "0.8782961460446247\n",
      "0.5121951219512195\n",
      "0.4311377245508982\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    #Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Truth.extend(Y_test.values)\n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.6645935139937805\n",
      "Specificity: 0.49137931034482757\n",
      "precision: 0.5760808534531162\n",
      "f1_score: 0.5303696045489791\n",
      "Accuracy: [0.5812399170315741]\n"
     ]
    }
   ],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2017.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2017.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.seed(1000)\n",
    "bigtest_df = dffull2017\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 3205.9116194999997 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7490566037735849, 0.7943396226415095, 0.7849056603773585, 0.7584905660377359, 0.7731568998109641]\n",
      "Avg accuracy: 0.7719898705282305\n",
      "Std of accuracy : \n",
      "0.01658829075019749\n",
      "\n",
      "[[ 249  428]\n",
      " [ 176 1796]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.59      0.37      0.45       677\n",
      "       woody       0.81      0.91      0.86      1972\n",
      "\n",
      "    accuracy                           0.77      2649\n",
      "   macro avg       0.70      0.64      0.65      2649\n",
      "weighted avg       0.75      0.77      0.75      2649\n",
      "\n",
      "Sensitivity: 0.9107505070993914\n",
      "Specificity: 0.36779911373707536\n",
      "precision: 0.5858823529411765\n",
      "f1_score: 0.45190562613430135\n"
     ]
    }
   ],
   "source": [
    "#Bagging Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.7912039093736117\n",
      "Specificity: 0.33955938697318006\n",
      "precision: 0.6013570822731128\n",
      "f1_score: 0.43403734312825226\n",
      "Accuracy: [0.5738649458400553]\n"
     ]
    }
   ],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2017.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2017.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OID__x</th>\n",
       "      <th>Id</th>\n",
       "      <th>gridcode</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>CH_mean</th>\n",
       "      <th>ARVI_mean</th>\n",
       "      <th>ARVI_max</th>\n",
       "      <th>ARVI_med</th>\n",
       "      <th>EVI_mean</th>\n",
       "      <th>EVI_max</th>\n",
       "      <th>EVI_med</th>\n",
       "      <th>NDVI_mean</th>\n",
       "      <th>NDVI_max</th>\n",
       "      <th>NDVI_med</th>\n",
       "      <th>SAVI_mean</th>\n",
       "      <th>SAVI_max</th>\n",
       "      <th>SAVI_med</th>\n",
       "      <th>Veg_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.2</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.053054</td>\n",
       "      <td>0.252378</td>\n",
       "      <td>-0.007871</td>\n",
       "      <td>0.145961</td>\n",
       "      <td>0.234673</td>\n",
       "      <td>0.126273</td>\n",
       "      <td>0.221155</td>\n",
       "      <td>0.400501</td>\n",
       "      <td>0.170450</td>\n",
       "      <td>0.145235</td>\n",
       "      <td>0.234261</td>\n",
       "      <td>0.125838</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>5.73</td>\n",
       "      <td>0.016248</td>\n",
       "      <td>0.040314</td>\n",
       "      <td>0.252378</td>\n",
       "      <td>0.043301</td>\n",
       "      <td>0.141544</td>\n",
       "      <td>0.234673</td>\n",
       "      <td>0.135954</td>\n",
       "      <td>0.206992</td>\n",
       "      <td>0.400501</td>\n",
       "      <td>0.207240</td>\n",
       "      <td>0.139659</td>\n",
       "      <td>0.234261</td>\n",
       "      <td>0.134476</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.014410</td>\n",
       "      <td>0.111847</td>\n",
       "      <td>0.281741</td>\n",
       "      <td>0.085010</td>\n",
       "      <td>0.167371</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.152276</td>\n",
       "      <td>0.274226</td>\n",
       "      <td>0.432882</td>\n",
       "      <td>0.249348</td>\n",
       "      <td>0.167777</td>\n",
       "      <td>0.255698</td>\n",
       "      <td>0.152786</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.012281</td>\n",
       "      <td>0.204984</td>\n",
       "      <td>0.310943</td>\n",
       "      <td>0.281741</td>\n",
       "      <td>0.211967</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.361578</td>\n",
       "      <td>0.438509</td>\n",
       "      <td>0.432882</td>\n",
       "      <td>0.213450</td>\n",
       "      <td>0.255698</td>\n",
       "      <td>0.255698</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>18.2</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.260015</td>\n",
       "      <td>0.365155</td>\n",
       "      <td>0.256816</td>\n",
       "      <td>0.203891</td>\n",
       "      <td>0.232604</td>\n",
       "      <td>0.209083</td>\n",
       "      <td>0.403924</td>\n",
       "      <td>0.488291</td>\n",
       "      <td>0.410121</td>\n",
       "      <td>0.207537</td>\n",
       "      <td>0.236474</td>\n",
       "      <td>0.212997</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>19.6</td>\n",
       "      <td>6.48</td>\n",
       "      <td>0.349136</td>\n",
       "      <td>0.396705</td>\n",
       "      <td>0.553325</td>\n",
       "      <td>0.415179</td>\n",
       "      <td>0.297437</td>\n",
       "      <td>0.412089</td>\n",
       "      <td>0.271848</td>\n",
       "      <td>0.489838</td>\n",
       "      <td>0.605186</td>\n",
       "      <td>0.495871</td>\n",
       "      <td>0.279308</td>\n",
       "      <td>0.370356</td>\n",
       "      <td>0.261450</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>16.4</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.786814</td>\n",
       "      <td>0.390539</td>\n",
       "      <td>0.608516</td>\n",
       "      <td>0.415876</td>\n",
       "      <td>0.347824</td>\n",
       "      <td>0.428983</td>\n",
       "      <td>0.337136</td>\n",
       "      <td>0.501389</td>\n",
       "      <td>0.649485</td>\n",
       "      <td>0.516101</td>\n",
       "      <td>0.326165</td>\n",
       "      <td>0.387202</td>\n",
       "      <td>0.315911</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>13.4</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.011587</td>\n",
       "      <td>-0.035525</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>-0.050461</td>\n",
       "      <td>0.112072</td>\n",
       "      <td>0.219773</td>\n",
       "      <td>0.101062</td>\n",
       "      <td>0.165851</td>\n",
       "      <td>0.334746</td>\n",
       "      <td>0.150055</td>\n",
       "      <td>0.118955</td>\n",
       "      <td>0.216044</td>\n",
       "      <td>0.107423</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>21.2</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.005081</td>\n",
       "      <td>0.305866</td>\n",
       "      <td>-0.009422</td>\n",
       "      <td>0.129007</td>\n",
       "      <td>0.223172</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.193393</td>\n",
       "      <td>0.434599</td>\n",
       "      <td>0.175006</td>\n",
       "      <td>0.133077</td>\n",
       "      <td>0.223413</td>\n",
       "      <td>0.123026</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>19.6</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.007232</td>\n",
       "      <td>0.007860</td>\n",
       "      <td>0.178960</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.120775</td>\n",
       "      <td>0.179630</td>\n",
       "      <td>0.120114</td>\n",
       "      <td>0.200068</td>\n",
       "      <td>0.341360</td>\n",
       "      <td>0.199783</td>\n",
       "      <td>0.127418</td>\n",
       "      <td>0.184816</td>\n",
       "      <td>0.126625</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    OID__x   Id  gridcode  Shape_Length  Shape_Area   CH_mean  ARVI_mean  \\\n",
       "0        1    1         1          14.2        5.04  0.010000   0.053054   \n",
       "1        2    2         2          26.2        5.73  0.016248   0.040314   \n",
       "2        3    3         3          10.6        2.29  0.014410   0.111847   \n",
       "3        4    4         4          11.0        2.85  0.012281   0.204984   \n",
       "4        5    5         5          18.2        3.78  0.012011   0.260015   \n",
       "..     ...  ...       ...           ...         ...       ...        ...   \n",
       "95      96   96        96          19.6        6.48  0.349136   0.396705   \n",
       "96      97   97        97          16.4        2.95  0.786814   0.390539   \n",
       "97      98   98        98          13.4        2.52  0.011587  -0.035525   \n",
       "98      99   99        99          21.2        5.49  0.010419   0.005081   \n",
       "99     100  100       100          19.6        3.54  0.007232   0.007860   \n",
       "\n",
       "    ARVI_max  ARVI_med  EVI_mean   EVI_max   EVI_med  NDVI_mean  NDVI_max  \\\n",
       "0   0.252378 -0.007871  0.145961  0.234673  0.126273   0.221155  0.400501   \n",
       "1   0.252378  0.043301  0.141544  0.234673  0.135954   0.206992  0.400501   \n",
       "2   0.281741  0.085010  0.167371  0.254378  0.152276   0.274226  0.432882   \n",
       "3   0.310943  0.281741  0.211967  0.254378  0.254378   0.361578  0.438509   \n",
       "4   0.365155  0.256816  0.203891  0.232604  0.209083   0.403924  0.488291   \n",
       "..       ...       ...       ...       ...       ...        ...       ...   \n",
       "95  0.553325  0.415179  0.297437  0.412089  0.271848   0.489838  0.605186   \n",
       "96  0.608516  0.415876  0.347824  0.428983  0.337136   0.501389  0.649485   \n",
       "97  0.178947 -0.050461  0.112072  0.219773  0.101062   0.165851  0.334746   \n",
       "98  0.305866 -0.009422  0.129007  0.223172  0.118613   0.193393  0.434599   \n",
       "99  0.178960  0.008800  0.120775  0.179630  0.120114   0.200068  0.341360   \n",
       "\n",
       "    NDVI_med  SAVI_mean  SAVI_max  SAVI_med  Veg_class  \n",
       "0   0.170450   0.145235  0.234261  0.125838  non-woody  \n",
       "1   0.207240   0.139659  0.234261  0.134476  non-woody  \n",
       "2   0.249348   0.167777  0.255698  0.152786  non-woody  \n",
       "3   0.432882   0.213450  0.255698  0.255698      woody  \n",
       "4   0.410121   0.207537  0.236474  0.212997      woody  \n",
       "..       ...        ...       ...       ...        ...  \n",
       "95  0.495871   0.279308  0.370356  0.261450      woody  \n",
       "96  0.516101   0.326165  0.387202  0.315911      woody  \n",
       "97  0.150055   0.118955  0.216044  0.107423  non-woody  \n",
       "98  0.175006   0.133077  0.223413  0.123026  non-woody  \n",
       "99  0.199783   0.127418  0.184816  0.126625  non-woody  \n",
       "\n",
       "[100 rows x 19 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2017\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 3239.1602544999996 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7855477855477856, 0.7967289719626168, 0.7920560747663551, 0.8154205607476636, 0.7733644859813084]\n",
      "Avg accuracy: 0.792623575801146\n",
      "Std of accuracy : \n",
      "0.013840940763669067\n",
      "\n",
      "[[  76  387]\n",
      " [  57 1621]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.57      0.16      0.26       463\n",
      "       woody       0.81      0.97      0.88      1678\n",
      "\n",
      "    accuracy                           0.79      2141\n",
      "   macro avg       0.69      0.57      0.57      2141\n",
      "weighted avg       0.76      0.79      0.74      2141\n",
      "\n",
      "Sensitivity: 0.16414686825053995\n",
      "Specificity: 0.966030989272944\n",
      "Precision: 0.5714285714285714\n",
      "F1_Score: 0.2550335570469799\n"
     ]
    }
   ],
   "source": [
    "#Ada boost\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.7103509551310528\n",
      "Specificity: 0.5234674329501916\n",
      "precision: 0.6263610315186247\n",
      "f1_score: 0.5703104617792852\n",
      "Accuracy: [0.6204194514865177]\n"
     ]
    }
   ],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2017.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2017.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OID__x</th>\n",
       "      <th>Id</th>\n",
       "      <th>gridcode</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>CH_mean</th>\n",
       "      <th>ARVI_mean</th>\n",
       "      <th>ARVI_max</th>\n",
       "      <th>ARVI_med</th>\n",
       "      <th>EVI_mean</th>\n",
       "      <th>EVI_max</th>\n",
       "      <th>EVI_med</th>\n",
       "      <th>NDVI_mean</th>\n",
       "      <th>NDVI_max</th>\n",
       "      <th>NDVI_med</th>\n",
       "      <th>SAVI_mean</th>\n",
       "      <th>SAVI_max</th>\n",
       "      <th>SAVI_med</th>\n",
       "      <th>Veg_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.2</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.053054</td>\n",
       "      <td>0.252378</td>\n",
       "      <td>-0.007871</td>\n",
       "      <td>0.145961</td>\n",
       "      <td>0.234673</td>\n",
       "      <td>0.126273</td>\n",
       "      <td>0.221155</td>\n",
       "      <td>0.400501</td>\n",
       "      <td>0.170450</td>\n",
       "      <td>0.145235</td>\n",
       "      <td>0.234261</td>\n",
       "      <td>0.125838</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>5.73</td>\n",
       "      <td>0.016248</td>\n",
       "      <td>0.040314</td>\n",
       "      <td>0.252378</td>\n",
       "      <td>0.043301</td>\n",
       "      <td>0.141544</td>\n",
       "      <td>0.234673</td>\n",
       "      <td>0.135954</td>\n",
       "      <td>0.206992</td>\n",
       "      <td>0.400501</td>\n",
       "      <td>0.207240</td>\n",
       "      <td>0.139659</td>\n",
       "      <td>0.234261</td>\n",
       "      <td>0.134476</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.014410</td>\n",
       "      <td>0.111847</td>\n",
       "      <td>0.281741</td>\n",
       "      <td>0.085010</td>\n",
       "      <td>0.167371</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.152276</td>\n",
       "      <td>0.274226</td>\n",
       "      <td>0.432882</td>\n",
       "      <td>0.249348</td>\n",
       "      <td>0.167777</td>\n",
       "      <td>0.255698</td>\n",
       "      <td>0.152786</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.012281</td>\n",
       "      <td>0.204984</td>\n",
       "      <td>0.310943</td>\n",
       "      <td>0.281741</td>\n",
       "      <td>0.211967</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.361578</td>\n",
       "      <td>0.438509</td>\n",
       "      <td>0.432882</td>\n",
       "      <td>0.213450</td>\n",
       "      <td>0.255698</td>\n",
       "      <td>0.255698</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>18.2</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.260015</td>\n",
       "      <td>0.365155</td>\n",
       "      <td>0.256816</td>\n",
       "      <td>0.203891</td>\n",
       "      <td>0.232604</td>\n",
       "      <td>0.209083</td>\n",
       "      <td>0.403924</td>\n",
       "      <td>0.488291</td>\n",
       "      <td>0.410121</td>\n",
       "      <td>0.207537</td>\n",
       "      <td>0.236474</td>\n",
       "      <td>0.212997</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>19.6</td>\n",
       "      <td>6.48</td>\n",
       "      <td>0.349136</td>\n",
       "      <td>0.396705</td>\n",
       "      <td>0.553325</td>\n",
       "      <td>0.415179</td>\n",
       "      <td>0.297437</td>\n",
       "      <td>0.412089</td>\n",
       "      <td>0.271848</td>\n",
       "      <td>0.489838</td>\n",
       "      <td>0.605186</td>\n",
       "      <td>0.495871</td>\n",
       "      <td>0.279308</td>\n",
       "      <td>0.370356</td>\n",
       "      <td>0.261450</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>16.4</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.786814</td>\n",
       "      <td>0.390539</td>\n",
       "      <td>0.608516</td>\n",
       "      <td>0.415876</td>\n",
       "      <td>0.347824</td>\n",
       "      <td>0.428983</td>\n",
       "      <td>0.337136</td>\n",
       "      <td>0.501389</td>\n",
       "      <td>0.649485</td>\n",
       "      <td>0.516101</td>\n",
       "      <td>0.326165</td>\n",
       "      <td>0.387202</td>\n",
       "      <td>0.315911</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>13.4</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.011587</td>\n",
       "      <td>-0.035525</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>-0.050461</td>\n",
       "      <td>0.112072</td>\n",
       "      <td>0.219773</td>\n",
       "      <td>0.101062</td>\n",
       "      <td>0.165851</td>\n",
       "      <td>0.334746</td>\n",
       "      <td>0.150055</td>\n",
       "      <td>0.118955</td>\n",
       "      <td>0.216044</td>\n",
       "      <td>0.107423</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>21.2</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.005081</td>\n",
       "      <td>0.305866</td>\n",
       "      <td>-0.009422</td>\n",
       "      <td>0.129007</td>\n",
       "      <td>0.223172</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.193393</td>\n",
       "      <td>0.434599</td>\n",
       "      <td>0.175006</td>\n",
       "      <td>0.133077</td>\n",
       "      <td>0.223413</td>\n",
       "      <td>0.123026</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>19.6</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.007232</td>\n",
       "      <td>0.007860</td>\n",
       "      <td>0.178960</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.120775</td>\n",
       "      <td>0.179630</td>\n",
       "      <td>0.120114</td>\n",
       "      <td>0.200068</td>\n",
       "      <td>0.341360</td>\n",
       "      <td>0.199783</td>\n",
       "      <td>0.127418</td>\n",
       "      <td>0.184816</td>\n",
       "      <td>0.126625</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    OID__x   Id  gridcode  Shape_Length  Shape_Area   CH_mean  ARVI_mean  \\\n",
       "0        1    1         1          14.2        5.04  0.010000   0.053054   \n",
       "1        2    2         2          26.2        5.73  0.016248   0.040314   \n",
       "2        3    3         3          10.6        2.29  0.014410   0.111847   \n",
       "3        4    4         4          11.0        2.85  0.012281   0.204984   \n",
       "4        5    5         5          18.2        3.78  0.012011   0.260015   \n",
       "..     ...  ...       ...           ...         ...       ...        ...   \n",
       "95      96   96        96          19.6        6.48  0.349136   0.396705   \n",
       "96      97   97        97          16.4        2.95  0.786814   0.390539   \n",
       "97      98   98        98          13.4        2.52  0.011587  -0.035525   \n",
       "98      99   99        99          21.2        5.49  0.010419   0.005081   \n",
       "99     100  100       100          19.6        3.54  0.007232   0.007860   \n",
       "\n",
       "    ARVI_max  ARVI_med  EVI_mean   EVI_max   EVI_med  NDVI_mean  NDVI_max  \\\n",
       "0   0.252378 -0.007871  0.145961  0.234673  0.126273   0.221155  0.400501   \n",
       "1   0.252378  0.043301  0.141544  0.234673  0.135954   0.206992  0.400501   \n",
       "2   0.281741  0.085010  0.167371  0.254378  0.152276   0.274226  0.432882   \n",
       "3   0.310943  0.281741  0.211967  0.254378  0.254378   0.361578  0.438509   \n",
       "4   0.365155  0.256816  0.203891  0.232604  0.209083   0.403924  0.488291   \n",
       "..       ...       ...       ...       ...       ...        ...       ...   \n",
       "95  0.553325  0.415179  0.297437  0.412089  0.271848   0.489838  0.605186   \n",
       "96  0.608516  0.415876  0.347824  0.428983  0.337136   0.501389  0.649485   \n",
       "97  0.178947 -0.050461  0.112072  0.219773  0.101062   0.165851  0.334746   \n",
       "98  0.305866 -0.009422  0.129007  0.223172  0.118613   0.193393  0.434599   \n",
       "99  0.178960  0.008800  0.120775  0.179630  0.120114   0.200068  0.341360   \n",
       "\n",
       "    NDVI_med  SAVI_mean  SAVI_max  SAVI_med Veg_class  \n",
       "0   0.170450   0.145235  0.234261  0.125838     woody  \n",
       "1   0.207240   0.139659  0.234261  0.134476     woody  \n",
       "2   0.249348   0.167777  0.255698  0.152786     woody  \n",
       "3   0.432882   0.213450  0.255698  0.255698     woody  \n",
       "4   0.410121   0.207537  0.236474  0.212997     woody  \n",
       "..       ...        ...       ...       ...       ...  \n",
       "95  0.495871   0.279308  0.370356  0.261450     woody  \n",
       "96  0.516101   0.326165  0.387202  0.315911     woody  \n",
       "97  0.150055   0.118955  0.216044  0.107423     woody  \n",
       "98  0.175006   0.133077  0.223413  0.123026     woody  \n",
       "99  0.199783   0.127418  0.184816  0.126625     woody  \n",
       "\n",
       "[100 rows x 19 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2017\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OID__x</th>\n",
       "      <th>Id</th>\n",
       "      <th>gridcode</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>CH_mean</th>\n",
       "      <th>ARVI_mean</th>\n",
       "      <th>ARVI_max</th>\n",
       "      <th>ARVI_med</th>\n",
       "      <th>EVI_mean</th>\n",
       "      <th>EVI_max</th>\n",
       "      <th>EVI_med</th>\n",
       "      <th>NDVI_mean</th>\n",
       "      <th>NDVI_max</th>\n",
       "      <th>NDVI_med</th>\n",
       "      <th>SAVI_mean</th>\n",
       "      <th>SAVI_max</th>\n",
       "      <th>SAVI_med</th>\n",
       "      <th>Veg_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.2</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.053054</td>\n",
       "      <td>0.252378</td>\n",
       "      <td>-0.007871</td>\n",
       "      <td>0.145961</td>\n",
       "      <td>0.234673</td>\n",
       "      <td>0.126273</td>\n",
       "      <td>0.221155</td>\n",
       "      <td>0.400501</td>\n",
       "      <td>0.170450</td>\n",
       "      <td>0.145235</td>\n",
       "      <td>0.234261</td>\n",
       "      <td>0.125838</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>5.73</td>\n",
       "      <td>0.016248</td>\n",
       "      <td>0.040314</td>\n",
       "      <td>0.252378</td>\n",
       "      <td>0.043301</td>\n",
       "      <td>0.141544</td>\n",
       "      <td>0.234673</td>\n",
       "      <td>0.135954</td>\n",
       "      <td>0.206992</td>\n",
       "      <td>0.400501</td>\n",
       "      <td>0.207240</td>\n",
       "      <td>0.139659</td>\n",
       "      <td>0.234261</td>\n",
       "      <td>0.134476</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.014410</td>\n",
       "      <td>0.111847</td>\n",
       "      <td>0.281741</td>\n",
       "      <td>0.085010</td>\n",
       "      <td>0.167371</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.152276</td>\n",
       "      <td>0.274226</td>\n",
       "      <td>0.432882</td>\n",
       "      <td>0.249348</td>\n",
       "      <td>0.167777</td>\n",
       "      <td>0.255698</td>\n",
       "      <td>0.152786</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.012281</td>\n",
       "      <td>0.204984</td>\n",
       "      <td>0.310943</td>\n",
       "      <td>0.281741</td>\n",
       "      <td>0.211967</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.361578</td>\n",
       "      <td>0.438509</td>\n",
       "      <td>0.432882</td>\n",
       "      <td>0.213450</td>\n",
       "      <td>0.255698</td>\n",
       "      <td>0.255698</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>18.2</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.260015</td>\n",
       "      <td>0.365155</td>\n",
       "      <td>0.256816</td>\n",
       "      <td>0.203891</td>\n",
       "      <td>0.232604</td>\n",
       "      <td>0.209083</td>\n",
       "      <td>0.403924</td>\n",
       "      <td>0.488291</td>\n",
       "      <td>0.410121</td>\n",
       "      <td>0.207537</td>\n",
       "      <td>0.236474</td>\n",
       "      <td>0.212997</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>24.6</td>\n",
       "      <td>5.34</td>\n",
       "      <td>0.011760</td>\n",
       "      <td>0.143938</td>\n",
       "      <td>0.365155</td>\n",
       "      <td>0.159118</td>\n",
       "      <td>0.175792</td>\n",
       "      <td>0.227181</td>\n",
       "      <td>0.187912</td>\n",
       "      <td>0.301899</td>\n",
       "      <td>0.488291</td>\n",
       "      <td>0.313752</td>\n",
       "      <td>0.176406</td>\n",
       "      <td>0.231180</td>\n",
       "      <td>0.189477</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>13.6</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.011519</td>\n",
       "      <td>0.095585</td>\n",
       "      <td>0.153732</td>\n",
       "      <td>0.090776</td>\n",
       "      <td>0.153697</td>\n",
       "      <td>0.170868</td>\n",
       "      <td>0.159732</td>\n",
       "      <td>0.257637</td>\n",
       "      <td>0.308600</td>\n",
       "      <td>0.257738</td>\n",
       "      <td>0.154285</td>\n",
       "      <td>0.172316</td>\n",
       "      <td>0.160627</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>11.2</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.010956</td>\n",
       "      <td>-0.026384</td>\n",
       "      <td>0.061265</td>\n",
       "      <td>-0.028525</td>\n",
       "      <td>0.106579</td>\n",
       "      <td>0.162647</td>\n",
       "      <td>0.093855</td>\n",
       "      <td>0.143429</td>\n",
       "      <td>0.227148</td>\n",
       "      <td>0.133892</td>\n",
       "      <td>0.104317</td>\n",
       "      <td>0.159023</td>\n",
       "      <td>0.091390</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>20.2</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0.016120</td>\n",
       "      <td>0.037614</td>\n",
       "      <td>0.147766</td>\n",
       "      <td>0.024030</td>\n",
       "      <td>0.133307</td>\n",
       "      <td>0.192798</td>\n",
       "      <td>0.132834</td>\n",
       "      <td>0.201293</td>\n",
       "      <td>0.303233</td>\n",
       "      <td>0.191683</td>\n",
       "      <td>0.131542</td>\n",
       "      <td>0.190033</td>\n",
       "      <td>0.130989</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>9.8</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0.010954</td>\n",
       "      <td>0.074672</td>\n",
       "      <td>0.219833</td>\n",
       "      <td>0.006852</td>\n",
       "      <td>0.149436</td>\n",
       "      <td>0.205513</td>\n",
       "      <td>0.128831</td>\n",
       "      <td>0.237263</td>\n",
       "      <td>0.365430</td>\n",
       "      <td>0.180277</td>\n",
       "      <td>0.148550</td>\n",
       "      <td>0.205168</td>\n",
       "      <td>0.127821</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>0.007116</td>\n",
       "      <td>0.119118</td>\n",
       "      <td>0.166581</td>\n",
       "      <td>0.148597</td>\n",
       "      <td>0.157830</td>\n",
       "      <td>0.199108</td>\n",
       "      <td>0.166876</td>\n",
       "      <td>0.278053</td>\n",
       "      <td>0.323478</td>\n",
       "      <td>0.306013</td>\n",
       "      <td>0.159099</td>\n",
       "      <td>0.198107</td>\n",
       "      <td>0.169636</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12.2</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.006449</td>\n",
       "      <td>0.184311</td>\n",
       "      <td>0.260116</td>\n",
       "      <td>0.153391</td>\n",
       "      <td>0.176761</td>\n",
       "      <td>0.199108</td>\n",
       "      <td>0.185296</td>\n",
       "      <td>0.334926</td>\n",
       "      <td>0.400241</td>\n",
       "      <td>0.317424</td>\n",
       "      <td>0.179017</td>\n",
       "      <td>0.199640</td>\n",
       "      <td>0.189956</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>25.4</td>\n",
       "      <td>7.73</td>\n",
       "      <td>0.011488</td>\n",
       "      <td>0.083584</td>\n",
       "      <td>0.313300</td>\n",
       "      <td>0.064017</td>\n",
       "      <td>0.163076</td>\n",
       "      <td>0.294788</td>\n",
       "      <td>0.179881</td>\n",
       "      <td>0.244744</td>\n",
       "      <td>0.460422</td>\n",
       "      <td>0.235016</td>\n",
       "      <td>0.160641</td>\n",
       "      <td>0.291469</td>\n",
       "      <td>0.178410</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.72</td>\n",
       "      <td>0.014089</td>\n",
       "      <td>0.112868</td>\n",
       "      <td>0.274218</td>\n",
       "      <td>0.136700</td>\n",
       "      <td>0.182546</td>\n",
       "      <td>0.296699</td>\n",
       "      <td>0.170525</td>\n",
       "      <td>0.275507</td>\n",
       "      <td>0.423819</td>\n",
       "      <td>0.302716</td>\n",
       "      <td>0.180349</td>\n",
       "      <td>0.288260</td>\n",
       "      <td>0.169619</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>14.2</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.007575</td>\n",
       "      <td>0.038866</td>\n",
       "      <td>0.168312</td>\n",
       "      <td>-0.036044</td>\n",
       "      <td>0.152059</td>\n",
       "      <td>0.216828</td>\n",
       "      <td>0.104410</td>\n",
       "      <td>0.214989</td>\n",
       "      <td>0.328877</td>\n",
       "      <td>0.146489</td>\n",
       "      <td>0.151821</td>\n",
       "      <td>0.215239</td>\n",
       "      <td>0.105787</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>22.4</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0.015101</td>\n",
       "      <td>0.046345</td>\n",
       "      <td>0.168312</td>\n",
       "      <td>0.070719</td>\n",
       "      <td>0.150853</td>\n",
       "      <td>0.212692</td>\n",
       "      <td>0.168787</td>\n",
       "      <td>0.219572</td>\n",
       "      <td>0.328877</td>\n",
       "      <td>0.245933</td>\n",
       "      <td>0.150472</td>\n",
       "      <td>0.211098</td>\n",
       "      <td>0.169300</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>13.4</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.005809</td>\n",
       "      <td>0.013348</td>\n",
       "      <td>0.148023</td>\n",
       "      <td>-0.028996</td>\n",
       "      <td>0.114097</td>\n",
       "      <td>0.162246</td>\n",
       "      <td>0.098731</td>\n",
       "      <td>0.182217</td>\n",
       "      <td>0.293062</td>\n",
       "      <td>0.144341</td>\n",
       "      <td>0.114538</td>\n",
       "      <td>0.161264</td>\n",
       "      <td>0.099627</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.042143</td>\n",
       "      <td>0.257832</td>\n",
       "      <td>0.428718</td>\n",
       "      <td>0.316659</td>\n",
       "      <td>0.256057</td>\n",
       "      <td>0.357934</td>\n",
       "      <td>0.301488</td>\n",
       "      <td>0.390754</td>\n",
       "      <td>0.535192</td>\n",
       "      <td>0.445596</td>\n",
       "      <td>0.245053</td>\n",
       "      <td>0.337296</td>\n",
       "      <td>0.286712</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>25.2</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.035356</td>\n",
       "      <td>0.108002</td>\n",
       "      <td>0.316659</td>\n",
       "      <td>0.108019</td>\n",
       "      <td>0.197797</td>\n",
       "      <td>0.306705</td>\n",
       "      <td>0.227549</td>\n",
       "      <td>0.269519</td>\n",
       "      <td>0.445596</td>\n",
       "      <td>0.275968</td>\n",
       "      <td>0.191028</td>\n",
       "      <td>0.291196</td>\n",
       "      <td>0.217859</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>17.8</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.009596</td>\n",
       "      <td>0.133056</td>\n",
       "      <td>0.288478</td>\n",
       "      <td>0.092382</td>\n",
       "      <td>0.212656</td>\n",
       "      <td>0.306583</td>\n",
       "      <td>0.196943</td>\n",
       "      <td>0.298893</td>\n",
       "      <td>0.446080</td>\n",
       "      <td>0.260861</td>\n",
       "      <td>0.209324</td>\n",
       "      <td>0.303280</td>\n",
       "      <td>0.191876</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>26.2</td>\n",
       "      <td>7.43</td>\n",
       "      <td>0.011777</td>\n",
       "      <td>0.025322</td>\n",
       "      <td>0.097257</td>\n",
       "      <td>0.029236</td>\n",
       "      <td>0.151936</td>\n",
       "      <td>0.181421</td>\n",
       "      <td>0.154500</td>\n",
       "      <td>0.188980</td>\n",
       "      <td>0.245577</td>\n",
       "      <td>0.190851</td>\n",
       "      <td>0.144270</td>\n",
       "      <td>0.170758</td>\n",
       "      <td>0.145986</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>17.2</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.013991</td>\n",
       "      <td>-0.015447</td>\n",
       "      <td>0.018702</td>\n",
       "      <td>-0.012606</td>\n",
       "      <td>0.120471</td>\n",
       "      <td>0.134568</td>\n",
       "      <td>0.121533</td>\n",
       "      <td>0.154498</td>\n",
       "      <td>0.183592</td>\n",
       "      <td>0.157117</td>\n",
       "      <td>0.116768</td>\n",
       "      <td>0.130599</td>\n",
       "      <td>0.117944</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>38.2</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.102461</td>\n",
       "      <td>0.295392</td>\n",
       "      <td>0.557473</td>\n",
       "      <td>0.308901</td>\n",
       "      <td>0.307197</td>\n",
       "      <td>0.454190</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.416009</td>\n",
       "      <td>0.615263</td>\n",
       "      <td>0.434339</td>\n",
       "      <td>0.282442</td>\n",
       "      <td>0.398523</td>\n",
       "      <td>0.294950</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>10.8</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.208246</td>\n",
       "      <td>0.457222</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.362775</td>\n",
       "      <td>0.371315</td>\n",
       "      <td>0.493478</td>\n",
       "      <td>0.359784</td>\n",
       "      <td>0.543199</td>\n",
       "      <td>0.657822</td>\n",
       "      <td>0.480459</td>\n",
       "      <td>0.339856</td>\n",
       "      <td>0.435916</td>\n",
       "      <td>0.332093</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>13.6</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.330623</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.361712</td>\n",
       "      <td>0.328296</td>\n",
       "      <td>0.493478</td>\n",
       "      <td>0.334804</td>\n",
       "      <td>0.447074</td>\n",
       "      <td>0.657822</td>\n",
       "      <td>0.478059</td>\n",
       "      <td>0.302318</td>\n",
       "      <td>0.435916</td>\n",
       "      <td>0.313049</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>20.8</td>\n",
       "      <td>5.46</td>\n",
       "      <td>0.013974</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.182320</td>\n",
       "      <td>0.015251</td>\n",
       "      <td>0.155328</td>\n",
       "      <td>0.223899</td>\n",
       "      <td>0.149636</td>\n",
       "      <td>0.213109</td>\n",
       "      <td>0.327752</td>\n",
       "      <td>0.203654</td>\n",
       "      <td>0.156074</td>\n",
       "      <td>0.220245</td>\n",
       "      <td>0.151753</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>13.8</td>\n",
       "      <td>2.36</td>\n",
       "      <td>0.014407</td>\n",
       "      <td>0.201766</td>\n",
       "      <td>0.363996</td>\n",
       "      <td>0.182320</td>\n",
       "      <td>0.205465</td>\n",
       "      <td>0.284225</td>\n",
       "      <td>0.186606</td>\n",
       "      <td>0.353258</td>\n",
       "      <td>0.494660</td>\n",
       "      <td>0.326446</td>\n",
       "      <td>0.205606</td>\n",
       "      <td>0.281144</td>\n",
       "      <td>0.188285</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>12.6</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.058457</td>\n",
       "      <td>0.145125</td>\n",
       "      <td>0.363996</td>\n",
       "      <td>0.130551</td>\n",
       "      <td>0.197809</td>\n",
       "      <td>0.279496</td>\n",
       "      <td>0.200216</td>\n",
       "      <td>0.309093</td>\n",
       "      <td>0.494660</td>\n",
       "      <td>0.307795</td>\n",
       "      <td>0.197794</td>\n",
       "      <td>0.278014</td>\n",
       "      <td>0.200050</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>1371.0</td>\n",
       "      <td>749.93</td>\n",
       "      <td>0.012496</td>\n",
       "      <td>0.018962</td>\n",
       "      <td>0.439580</td>\n",
       "      <td>-0.012088</td>\n",
       "      <td>0.140440</td>\n",
       "      <td>0.330568</td>\n",
       "      <td>0.127855</td>\n",
       "      <td>0.193930</td>\n",
       "      <td>0.562144</td>\n",
       "      <td>0.169645</td>\n",
       "      <td>0.139306</td>\n",
       "      <td>0.310592</td>\n",
       "      <td>0.127701</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>20.6</td>\n",
       "      <td>9.33</td>\n",
       "      <td>0.014073</td>\n",
       "      <td>0.131478</td>\n",
       "      <td>0.234754</td>\n",
       "      <td>0.149698</td>\n",
       "      <td>0.181186</td>\n",
       "      <td>0.226339</td>\n",
       "      <td>0.180474</td>\n",
       "      <td>0.295470</td>\n",
       "      <td>0.395217</td>\n",
       "      <td>0.308200</td>\n",
       "      <td>0.181927</td>\n",
       "      <td>0.230374</td>\n",
       "      <td>0.178608</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>24.2</td>\n",
       "      <td>5.24</td>\n",
       "      <td>0.013874</td>\n",
       "      <td>0.147436</td>\n",
       "      <td>0.187865</td>\n",
       "      <td>0.177667</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.217872</td>\n",
       "      <td>0.192079</td>\n",
       "      <td>0.310904</td>\n",
       "      <td>0.349153</td>\n",
       "      <td>0.336923</td>\n",
       "      <td>0.190491</td>\n",
       "      <td>0.217187</td>\n",
       "      <td>0.192958</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>15.8</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.011239</td>\n",
       "      <td>0.138106</td>\n",
       "      <td>0.342986</td>\n",
       "      <td>0.103035</td>\n",
       "      <td>0.182457</td>\n",
       "      <td>0.257095</td>\n",
       "      <td>0.195399</td>\n",
       "      <td>0.292095</td>\n",
       "      <td>0.466620</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.179880</td>\n",
       "      <td>0.253887</td>\n",
       "      <td>0.190421</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>35.0</td>\n",
       "      <td>9.62</td>\n",
       "      <td>0.014407</td>\n",
       "      <td>0.098270</td>\n",
       "      <td>0.196747</td>\n",
       "      <td>0.101668</td>\n",
       "      <td>0.169825</td>\n",
       "      <td>0.218539</td>\n",
       "      <td>0.182440</td>\n",
       "      <td>0.265423</td>\n",
       "      <td>0.345762</td>\n",
       "      <td>0.273391</td>\n",
       "      <td>0.169878</td>\n",
       "      <td>0.216678</td>\n",
       "      <td>0.182371</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.013893</td>\n",
       "      <td>0.149403</td>\n",
       "      <td>0.188605</td>\n",
       "      <td>0.168533</td>\n",
       "      <td>0.192220</td>\n",
       "      <td>0.218539</td>\n",
       "      <td>0.191094</td>\n",
       "      <td>0.311343</td>\n",
       "      <td>0.345192</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.192421</td>\n",
       "      <td>0.216678</td>\n",
       "      <td>0.192242</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>19.2</td>\n",
       "      <td>5.55</td>\n",
       "      <td>0.016234</td>\n",
       "      <td>0.158602</td>\n",
       "      <td>0.263555</td>\n",
       "      <td>0.153176</td>\n",
       "      <td>0.184379</td>\n",
       "      <td>0.217031</td>\n",
       "      <td>0.187081</td>\n",
       "      <td>0.309208</td>\n",
       "      <td>0.396284</td>\n",
       "      <td>0.307171</td>\n",
       "      <td>0.182470</td>\n",
       "      <td>0.215067</td>\n",
       "      <td>0.188505</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.99</td>\n",
       "      <td>0.013957</td>\n",
       "      <td>0.006973</td>\n",
       "      <td>0.180380</td>\n",
       "      <td>-0.006413</td>\n",
       "      <td>0.126342</td>\n",
       "      <td>0.188161</td>\n",
       "      <td>0.123636</td>\n",
       "      <td>0.177138</td>\n",
       "      <td>0.331667</td>\n",
       "      <td>0.165239</td>\n",
       "      <td>0.124423</td>\n",
       "      <td>0.185417</td>\n",
       "      <td>0.121232</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>40.2</td>\n",
       "      <td>17.48</td>\n",
       "      <td>0.045240</td>\n",
       "      <td>0.213050</td>\n",
       "      <td>0.496593</td>\n",
       "      <td>0.238806</td>\n",
       "      <td>0.219402</td>\n",
       "      <td>0.316513</td>\n",
       "      <td>0.230157</td>\n",
       "      <td>0.361392</td>\n",
       "      <td>0.585740</td>\n",
       "      <td>0.389322</td>\n",
       "      <td>0.217084</td>\n",
       "      <td>0.304441</td>\n",
       "      <td>0.228538</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>9.2</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.010295</td>\n",
       "      <td>0.170523</td>\n",
       "      <td>0.251598</td>\n",
       "      <td>0.193757</td>\n",
       "      <td>0.203791</td>\n",
       "      <td>0.248121</td>\n",
       "      <td>0.226792</td>\n",
       "      <td>0.323056</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.345216</td>\n",
       "      <td>0.200444</td>\n",
       "      <td>0.238342</td>\n",
       "      <td>0.222003</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>11.2</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.015749</td>\n",
       "      <td>0.136608</td>\n",
       "      <td>0.277001</td>\n",
       "      <td>0.161328</td>\n",
       "      <td>0.170268</td>\n",
       "      <td>0.225477</td>\n",
       "      <td>0.178925</td>\n",
       "      <td>0.303697</td>\n",
       "      <td>0.413502</td>\n",
       "      <td>0.323598</td>\n",
       "      <td>0.174282</td>\n",
       "      <td>0.224885</td>\n",
       "      <td>0.182516</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>17.8</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.155247</td>\n",
       "      <td>0.330093</td>\n",
       "      <td>0.469003</td>\n",
       "      <td>0.356148</td>\n",
       "      <td>0.272169</td>\n",
       "      <td>0.337812</td>\n",
       "      <td>0.276678</td>\n",
       "      <td>0.457162</td>\n",
       "      <td>0.559928</td>\n",
       "      <td>0.486720</td>\n",
       "      <td>0.265642</td>\n",
       "      <td>0.320249</td>\n",
       "      <td>0.274463</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.52</td>\n",
       "      <td>0.018098</td>\n",
       "      <td>0.254692</td>\n",
       "      <td>0.516491</td>\n",
       "      <td>0.322875</td>\n",
       "      <td>0.201910</td>\n",
       "      <td>0.350779</td>\n",
       "      <td>0.223278</td>\n",
       "      <td>0.390866</td>\n",
       "      <td>0.587131</td>\n",
       "      <td>0.445774</td>\n",
       "      <td>0.202703</td>\n",
       "      <td>0.329158</td>\n",
       "      <td>0.223362</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>28.2</td>\n",
       "      <td>7.69</td>\n",
       "      <td>0.006567</td>\n",
       "      <td>0.146814</td>\n",
       "      <td>0.385433</td>\n",
       "      <td>0.125680</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.285411</td>\n",
       "      <td>0.154223</td>\n",
       "      <td>0.304794</td>\n",
       "      <td>0.499831</td>\n",
       "      <td>0.284385</td>\n",
       "      <td>0.175522</td>\n",
       "      <td>0.278504</td>\n",
       "      <td>0.156626</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>26.8</td>\n",
       "      <td>8.04</td>\n",
       "      <td>0.007438</td>\n",
       "      <td>0.207102</td>\n",
       "      <td>0.385433</td>\n",
       "      <td>0.193209</td>\n",
       "      <td>0.178283</td>\n",
       "      <td>0.285411</td>\n",
       "      <td>0.172225</td>\n",
       "      <td>0.344908</td>\n",
       "      <td>0.499831</td>\n",
       "      <td>0.334093</td>\n",
       "      <td>0.178442</td>\n",
       "      <td>0.278504</td>\n",
       "      <td>0.172911</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>27.6</td>\n",
       "      <td>6.24</td>\n",
       "      <td>0.009728</td>\n",
       "      <td>0.234730</td>\n",
       "      <td>0.350721</td>\n",
       "      <td>0.212616</td>\n",
       "      <td>0.183752</td>\n",
       "      <td>0.270642</td>\n",
       "      <td>0.168260</td>\n",
       "      <td>0.362963</td>\n",
       "      <td>0.455688</td>\n",
       "      <td>0.342646</td>\n",
       "      <td>0.182619</td>\n",
       "      <td>0.257602</td>\n",
       "      <td>0.168281</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>29.6</td>\n",
       "      <td>6.91</td>\n",
       "      <td>0.005166</td>\n",
       "      <td>0.193508</td>\n",
       "      <td>0.350721</td>\n",
       "      <td>0.200861</td>\n",
       "      <td>0.196017</td>\n",
       "      <td>0.270642</td>\n",
       "      <td>0.189007</td>\n",
       "      <td>0.342890</td>\n",
       "      <td>0.455688</td>\n",
       "      <td>0.349456</td>\n",
       "      <td>0.195746</td>\n",
       "      <td>0.257602</td>\n",
       "      <td>0.190441</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.082814</td>\n",
       "      <td>0.200861</td>\n",
       "      <td>0.107754</td>\n",
       "      <td>0.170405</td>\n",
       "      <td>0.191372</td>\n",
       "      <td>0.174886</td>\n",
       "      <td>0.268113</td>\n",
       "      <td>0.349456</td>\n",
       "      <td>0.291375</td>\n",
       "      <td>0.176252</td>\n",
       "      <td>0.194704</td>\n",
       "      <td>0.181641</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>31.6</td>\n",
       "      <td>6.58</td>\n",
       "      <td>0.008040</td>\n",
       "      <td>0.075691</td>\n",
       "      <td>0.286380</td>\n",
       "      <td>0.062003</td>\n",
       "      <td>0.154751</td>\n",
       "      <td>0.250764</td>\n",
       "      <td>0.156104</td>\n",
       "      <td>0.265341</td>\n",
       "      <td>0.447282</td>\n",
       "      <td>0.260717</td>\n",
       "      <td>0.163181</td>\n",
       "      <td>0.256500</td>\n",
       "      <td>0.166552</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>14.6</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.007542</td>\n",
       "      <td>0.318210</td>\n",
       "      <td>0.585155</td>\n",
       "      <td>0.322719</td>\n",
       "      <td>0.204563</td>\n",
       "      <td>0.314253</td>\n",
       "      <td>0.196393</td>\n",
       "      <td>0.453089</td>\n",
       "      <td>0.664790</td>\n",
       "      <td>0.458946</td>\n",
       "      <td>0.210641</td>\n",
       "      <td>0.315023</td>\n",
       "      <td>0.204095</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.06</td>\n",
       "      <td>0.258536</td>\n",
       "      <td>0.505007</td>\n",
       "      <td>0.680353</td>\n",
       "      <td>0.539452</td>\n",
       "      <td>0.337534</td>\n",
       "      <td>0.446219</td>\n",
       "      <td>0.321042</td>\n",
       "      <td>0.601547</td>\n",
       "      <td>0.722800</td>\n",
       "      <td>0.625850</td>\n",
       "      <td>0.329439</td>\n",
       "      <td>0.418778</td>\n",
       "      <td>0.315023</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>17.2</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0.109831</td>\n",
       "      <td>0.498591</td>\n",
       "      <td>0.680353</td>\n",
       "      <td>0.519791</td>\n",
       "      <td>0.337068</td>\n",
       "      <td>0.419392</td>\n",
       "      <td>0.335836</td>\n",
       "      <td>0.590118</td>\n",
       "      <td>0.715275</td>\n",
       "      <td>0.593978</td>\n",
       "      <td>0.326191</td>\n",
       "      <td>0.392735</td>\n",
       "      <td>0.326138</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    OID__x  Id  gridcode  Shape_Length  Shape_Area   CH_mean  ARVI_mean  \\\n",
       "0        1   1         1          14.2        5.04  0.010000   0.053054   \n",
       "1        2   2         2          26.2        5.73  0.016248   0.040314   \n",
       "2        3   3         3          10.6        2.29  0.014410   0.111847   \n",
       "3        4   4         4          11.0        2.85  0.012281   0.204984   \n",
       "4        5   5         5          18.2        3.78  0.012011   0.260015   \n",
       "5        6   6         6          24.6        5.34  0.011760   0.143938   \n",
       "6        7   7         7          13.6        2.37  0.011519   0.095585   \n",
       "7        8   8         8          11.2        2.51  0.010956  -0.026384   \n",
       "8        9   9         9          20.2        5.49  0.016120   0.037614   \n",
       "9       10  10        10           9.8        2.83  0.010954   0.074672   \n",
       "10      11  11        11          22.0        4.75  0.007116   0.119118   \n",
       "11      12  12        12          12.2        4.14  0.006449   0.184311   \n",
       "12      13  13        13          25.4        7.73  0.011488   0.083584   \n",
       "13      14  14        14          13.0        4.72  0.014089   0.112868   \n",
       "14      15  15        15          14.2        3.01  0.007575   0.038866   \n",
       "15      16  16        16          22.4        4.47  0.015101   0.046345   \n",
       "16      17  17        17          13.4        2.41  0.005809   0.013348   \n",
       "17      18  18        18          18.0        5.04  0.042143   0.257832   \n",
       "18      19  19        19          25.2        4.50  0.035356   0.108002   \n",
       "19      20  20        20          17.8        3.22  0.009596   0.133056   \n",
       "20      21  21        21          26.2        7.43  0.011777   0.025322   \n",
       "21      22  22        22          17.2        2.33  0.013991  -0.015447   \n",
       "22      23  23        23          38.2        5.16  0.102461   0.295392   \n",
       "23      24  24        24          10.8        2.85  0.208246   0.457222   \n",
       "24      25  25        25          13.6        4.23  0.322222   0.330623   \n",
       "25      26  26        26          20.8        5.46  0.013974   0.031600   \n",
       "26      27  27        27          13.8        2.36  0.014407   0.201766   \n",
       "27      28  28        28          12.6        4.86  0.058457   0.145125   \n",
       "28      29  29        29        1371.0      749.93  0.012496   0.018962   \n",
       "29      30  30        30          20.6        9.33  0.014073   0.131478   \n",
       "30      31  31        31          24.2        5.24  0.013874   0.147436   \n",
       "31      32  32        32          15.8        5.65  0.011239   0.138106   \n",
       "32      33  33        33          35.0        9.62  0.014407   0.098270   \n",
       "33      34  34        34           8.6        2.44  0.013893   0.149403   \n",
       "34      35  35        35          19.2        5.55  0.016234   0.158602   \n",
       "35      36  36        36          19.0        5.99  0.013957   0.006973   \n",
       "36      37  37        37          40.2       17.48  0.045240   0.213050   \n",
       "37      38  38        38           9.2        3.05  0.010295   0.170523   \n",
       "38      39  39        39          11.2        2.07  0.015749   0.136608   \n",
       "39      40  40        40          17.8        8.69  0.155247   0.330093   \n",
       "40      41  41        41          17.0        6.52  0.018098   0.254692   \n",
       "41      42  42        42          28.2        7.69  0.006567   0.146814   \n",
       "42      43  43        43          26.8        8.04  0.007438   0.207102   \n",
       "43      44  44        44          27.6        6.24  0.009728   0.234730   \n",
       "44      45  45        45          29.6        6.91  0.005166   0.193508   \n",
       "45      46  46        46          11.0        2.43  0.010700   0.082814   \n",
       "46      47  47        47          31.6        6.58  0.008040   0.075691   \n",
       "47      48  48        48          14.6        2.40  0.007542   0.318210   \n",
       "48      49  49        49          20.0        8.06  0.258536   0.505007   \n",
       "49      50  50        50          17.2        3.55  0.109831   0.498591   \n",
       "\n",
       "    ARVI_max  ARVI_med  EVI_mean   EVI_max   EVI_med  NDVI_mean  NDVI_max  \\\n",
       "0   0.252378 -0.007871  0.145961  0.234673  0.126273   0.221155  0.400501   \n",
       "1   0.252378  0.043301  0.141544  0.234673  0.135954   0.206992  0.400501   \n",
       "2   0.281741  0.085010  0.167371  0.254378  0.152276   0.274226  0.432882   \n",
       "3   0.310943  0.281741  0.211967  0.254378  0.254378   0.361578  0.438509   \n",
       "4   0.365155  0.256816  0.203891  0.232604  0.209083   0.403924  0.488291   \n",
       "5   0.365155  0.159118  0.175792  0.227181  0.187912   0.301899  0.488291   \n",
       "6   0.153732  0.090776  0.153697  0.170868  0.159732   0.257637  0.308600   \n",
       "7   0.061265 -0.028525  0.106579  0.162647  0.093855   0.143429  0.227148   \n",
       "8   0.147766  0.024030  0.133307  0.192798  0.132834   0.201293  0.303233   \n",
       "9   0.219833  0.006852  0.149436  0.205513  0.128831   0.237263  0.365430   \n",
       "10  0.166581  0.148597  0.157830  0.199108  0.166876   0.278053  0.323478   \n",
       "11  0.260116  0.153391  0.176761  0.199108  0.185296   0.334926  0.400241   \n",
       "12  0.313300  0.064017  0.163076  0.294788  0.179881   0.244744  0.460422   \n",
       "13  0.274218  0.136700  0.182546  0.296699  0.170525   0.275507  0.423819   \n",
       "14  0.168312 -0.036044  0.152059  0.216828  0.104410   0.214989  0.328877   \n",
       "15  0.168312  0.070719  0.150853  0.212692  0.168787   0.219572  0.328877   \n",
       "16  0.148023 -0.028996  0.114097  0.162246  0.098731   0.182217  0.293062   \n",
       "17  0.428718  0.316659  0.256057  0.357934  0.301488   0.390754  0.535192   \n",
       "18  0.316659  0.108019  0.197797  0.306705  0.227549   0.269519  0.445596   \n",
       "19  0.288478  0.092382  0.212656  0.306583  0.196943   0.298893  0.446080   \n",
       "20  0.097257  0.029236  0.151936  0.181421  0.154500   0.188980  0.245577   \n",
       "21  0.018702 -0.012606  0.120471  0.134568  0.121533   0.154498  0.183592   \n",
       "22  0.557473  0.308901  0.307197  0.454190  0.307692   0.416009  0.615263   \n",
       "23  0.607843  0.362775  0.371315  0.493478  0.359784   0.543199  0.657822   \n",
       "24  0.607843  0.361712  0.328296  0.493478  0.334804   0.447074  0.657822   \n",
       "25  0.182320  0.015251  0.155328  0.223899  0.149636   0.213109  0.327752   \n",
       "26  0.363996  0.182320  0.205465  0.284225  0.186606   0.353258  0.494660   \n",
       "27  0.363996  0.130551  0.197809  0.279496  0.200216   0.309093  0.494660   \n",
       "28  0.439580 -0.012088  0.140440  0.330568  0.127855   0.193930  0.562144   \n",
       "29  0.234754  0.149698  0.181186  0.226339  0.180474   0.295470  0.395217   \n",
       "30  0.187865  0.177667  0.189405  0.217872  0.192079   0.310904  0.349153   \n",
       "31  0.342986  0.103035  0.182457  0.257095  0.195399   0.292095  0.466620   \n",
       "32  0.196747  0.101668  0.169825  0.218539  0.182440   0.265423  0.345762   \n",
       "33  0.188605  0.168533  0.192220  0.218539  0.191094   0.311343  0.345192   \n",
       "34  0.263555  0.153176  0.184379  0.217031  0.187081   0.309208  0.396284   \n",
       "35  0.180380 -0.006413  0.126342  0.188161  0.123636   0.177138  0.331667   \n",
       "36  0.496593  0.238806  0.219402  0.316513  0.230157   0.361392  0.585740   \n",
       "37  0.251598  0.193757  0.203791  0.248121  0.226792   0.323056  0.386100   \n",
       "38  0.277001  0.161328  0.170268  0.225477  0.178925   0.303697  0.413502   \n",
       "39  0.469003  0.356148  0.272169  0.337812  0.276678   0.457162  0.559928   \n",
       "40  0.516491  0.322875  0.201910  0.350779  0.223278   0.390866  0.587131   \n",
       "41  0.385433  0.125680  0.174118  0.285411  0.154223   0.304794  0.499831   \n",
       "42  0.385433  0.193209  0.178283  0.285411  0.172225   0.344908  0.499831   \n",
       "43  0.350721  0.212616  0.183752  0.270642  0.168260   0.362963  0.455688   \n",
       "44  0.350721  0.200861  0.196017  0.270642  0.189007   0.342890  0.455688   \n",
       "45  0.200861  0.107754  0.170405  0.191372  0.174886   0.268113  0.349456   \n",
       "46  0.286380  0.062003  0.154751  0.250764  0.156104   0.265341  0.447282   \n",
       "47  0.585155  0.322719  0.204563  0.314253  0.196393   0.453089  0.664790   \n",
       "48  0.680353  0.539452  0.337534  0.446219  0.321042   0.601547  0.722800   \n",
       "49  0.680353  0.519791  0.337068  0.419392  0.335836   0.590118  0.715275   \n",
       "\n",
       "    NDVI_med  SAVI_mean  SAVI_max  SAVI_med  Veg_class  \n",
       "0   0.170450   0.145235  0.234261  0.125838      woody  \n",
       "1   0.207240   0.139659  0.234261  0.134476      woody  \n",
       "2   0.249348   0.167777  0.255698  0.152786      woody  \n",
       "3   0.432882   0.213450  0.255698  0.255698      woody  \n",
       "4   0.410121   0.207537  0.236474  0.212997      woody  \n",
       "5   0.313752   0.176406  0.231180  0.189477      woody  \n",
       "6   0.257738   0.154285  0.172316  0.160627  non-woody  \n",
       "7   0.133892   0.104317  0.159023  0.091390  non-woody  \n",
       "8   0.191683   0.131542  0.190033  0.130989      woody  \n",
       "9   0.180277   0.148550  0.205168  0.127821      woody  \n",
       "10  0.306013   0.159099  0.198107  0.169636      woody  \n",
       "11  0.317424   0.179017  0.199640  0.189956      woody  \n",
       "12  0.235016   0.160641  0.291469  0.178410      woody  \n",
       "13  0.302716   0.180349  0.288260  0.169619      woody  \n",
       "14  0.146489   0.151821  0.215239  0.105787      woody  \n",
       "15  0.245933   0.150472  0.211098  0.169300      woody  \n",
       "16  0.144341   0.114538  0.161264  0.099627  non-woody  \n",
       "17  0.445596   0.245053  0.337296  0.286712      woody  \n",
       "18  0.275968   0.191028  0.291196  0.217859      woody  \n",
       "19  0.260861   0.209324  0.303280  0.191876      woody  \n",
       "20  0.190851   0.144270  0.170758  0.145986      woody  \n",
       "21  0.157117   0.116768  0.130599  0.117944  non-woody  \n",
       "22  0.434339   0.282442  0.398523  0.294950  non-woody  \n",
       "23  0.480459   0.339856  0.435916  0.332093  non-woody  \n",
       "24  0.478059   0.302318  0.435916  0.313049  non-woody  \n",
       "25  0.203654   0.156074  0.220245  0.151753      woody  \n",
       "26  0.326446   0.205606  0.281144  0.188285      woody  \n",
       "27  0.307795   0.197794  0.278014  0.200050      woody  \n",
       "28  0.169645   0.139306  0.310592  0.127701      woody  \n",
       "29  0.308200   0.181927  0.230374  0.178608      woody  \n",
       "30  0.336923   0.190491  0.217187  0.192958  non-woody  \n",
       "31  0.268000   0.179880  0.253887  0.190421      woody  \n",
       "32  0.273391   0.169878  0.216678  0.182371      woody  \n",
       "33  0.326923   0.192421  0.216678  0.192242  non-woody  \n",
       "34  0.307171   0.182470  0.215067  0.188505      woody  \n",
       "35  0.165239   0.124423  0.185417  0.121232      woody  \n",
       "36  0.389322   0.217084  0.304441  0.228538      woody  \n",
       "37  0.345216   0.200444  0.238342  0.222003      woody  \n",
       "38  0.323598   0.174282  0.224885  0.182516      woody  \n",
       "39  0.486720   0.265642  0.320249  0.274463      woody  \n",
       "40  0.445774   0.202703  0.329158  0.223362      woody  \n",
       "41  0.284385   0.175522  0.278504  0.156626      woody  \n",
       "42  0.334093   0.178442  0.278504  0.172911      woody  \n",
       "43  0.342646   0.182619  0.257602  0.168281      woody  \n",
       "44  0.349456   0.195746  0.257602  0.190441      woody  \n",
       "45  0.291375   0.176252  0.194704  0.181641      woody  \n",
       "46  0.260717   0.163181  0.256500  0.166552      woody  \n",
       "47  0.458946   0.210641  0.315023  0.204095  non-woody  \n",
       "48  0.625850   0.329439  0.418778  0.315023      woody  \n",
       "49  0.593978   0.326191  0.392735  0.326138      woody  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigtest_df.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 3427.4583202999997 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7641509433962265, 0.7886792452830189, 0.7754716981132076, 0.7547169811320755, 0.7712665406427222]\n",
      "Avg accuracy: 0.7708570817134502\n",
      "Std of accuracy : \n",
      "0.011354003640467623\n",
      "\n",
      "[[ 278  399]\n",
      " [ 208 1764]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.57      0.41      0.48       677\n",
      "       woody       0.82      0.89      0.85      1972\n",
      "\n",
      "    accuracy                           0.77      2649\n",
      "   macro avg       0.69      0.65      0.67      2649\n",
      "weighted avg       0.75      0.77      0.76      2649\n",
      "\n",
      "Sensitivity: 0.41063515509601184\n",
      "Specificity: 0.8945233265720081\n",
      "Precision: 0.5720164609053497\n",
      "F1_Score: 0.47807394668959585\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.8160817414482452\n",
      "Specificity: 0.25191570881226055\n",
      "precision: 0.5595744680851064\n",
      "f1_score: 0.3474240422721268\n",
      "Accuracy: [0.5445955289237151]\n"
     ]
    }
   ],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2017.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2017.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OID__x</th>\n",
       "      <th>Id</th>\n",
       "      <th>gridcode</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>CH_mean</th>\n",
       "      <th>ARVI_mean</th>\n",
       "      <th>ARVI_max</th>\n",
       "      <th>ARVI_med</th>\n",
       "      <th>EVI_mean</th>\n",
       "      <th>EVI_max</th>\n",
       "      <th>EVI_med</th>\n",
       "      <th>NDVI_mean</th>\n",
       "      <th>NDVI_max</th>\n",
       "      <th>NDVI_med</th>\n",
       "      <th>SAVI_mean</th>\n",
       "      <th>SAVI_max</th>\n",
       "      <th>SAVI_med</th>\n",
       "      <th>Veg_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.2</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.053054</td>\n",
       "      <td>0.252378</td>\n",
       "      <td>-0.007871</td>\n",
       "      <td>0.145961</td>\n",
       "      <td>0.234673</td>\n",
       "      <td>0.126273</td>\n",
       "      <td>0.221155</td>\n",
       "      <td>0.400501</td>\n",
       "      <td>0.170450</td>\n",
       "      <td>0.145235</td>\n",
       "      <td>0.234261</td>\n",
       "      <td>0.125838</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>5.73</td>\n",
       "      <td>0.016248</td>\n",
       "      <td>0.040314</td>\n",
       "      <td>0.252378</td>\n",
       "      <td>0.043301</td>\n",
       "      <td>0.141544</td>\n",
       "      <td>0.234673</td>\n",
       "      <td>0.135954</td>\n",
       "      <td>0.206992</td>\n",
       "      <td>0.400501</td>\n",
       "      <td>0.207240</td>\n",
       "      <td>0.139659</td>\n",
       "      <td>0.234261</td>\n",
       "      <td>0.134476</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.014410</td>\n",
       "      <td>0.111847</td>\n",
       "      <td>0.281741</td>\n",
       "      <td>0.085010</td>\n",
       "      <td>0.167371</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.152276</td>\n",
       "      <td>0.274226</td>\n",
       "      <td>0.432882</td>\n",
       "      <td>0.249348</td>\n",
       "      <td>0.167777</td>\n",
       "      <td>0.255698</td>\n",
       "      <td>0.152786</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.012281</td>\n",
       "      <td>0.204984</td>\n",
       "      <td>0.310943</td>\n",
       "      <td>0.281741</td>\n",
       "      <td>0.211967</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.361578</td>\n",
       "      <td>0.438509</td>\n",
       "      <td>0.432882</td>\n",
       "      <td>0.213450</td>\n",
       "      <td>0.255698</td>\n",
       "      <td>0.255698</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>18.2</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.260015</td>\n",
       "      <td>0.365155</td>\n",
       "      <td>0.256816</td>\n",
       "      <td>0.203891</td>\n",
       "      <td>0.232604</td>\n",
       "      <td>0.209083</td>\n",
       "      <td>0.403924</td>\n",
       "      <td>0.488291</td>\n",
       "      <td>0.410121</td>\n",
       "      <td>0.207537</td>\n",
       "      <td>0.236474</td>\n",
       "      <td>0.212997</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>24.6</td>\n",
       "      <td>5.34</td>\n",
       "      <td>0.011760</td>\n",
       "      <td>0.143938</td>\n",
       "      <td>0.365155</td>\n",
       "      <td>0.159118</td>\n",
       "      <td>0.175792</td>\n",
       "      <td>0.227181</td>\n",
       "      <td>0.187912</td>\n",
       "      <td>0.301899</td>\n",
       "      <td>0.488291</td>\n",
       "      <td>0.313752</td>\n",
       "      <td>0.176406</td>\n",
       "      <td>0.231180</td>\n",
       "      <td>0.189477</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>13.6</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.011519</td>\n",
       "      <td>0.095585</td>\n",
       "      <td>0.153732</td>\n",
       "      <td>0.090776</td>\n",
       "      <td>0.153697</td>\n",
       "      <td>0.170868</td>\n",
       "      <td>0.159732</td>\n",
       "      <td>0.257637</td>\n",
       "      <td>0.308600</td>\n",
       "      <td>0.257738</td>\n",
       "      <td>0.154285</td>\n",
       "      <td>0.172316</td>\n",
       "      <td>0.160627</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>11.2</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.010956</td>\n",
       "      <td>-0.026384</td>\n",
       "      <td>0.061265</td>\n",
       "      <td>-0.028525</td>\n",
       "      <td>0.106579</td>\n",
       "      <td>0.162647</td>\n",
       "      <td>0.093855</td>\n",
       "      <td>0.143429</td>\n",
       "      <td>0.227148</td>\n",
       "      <td>0.133892</td>\n",
       "      <td>0.104317</td>\n",
       "      <td>0.159023</td>\n",
       "      <td>0.091390</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>20.2</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0.016120</td>\n",
       "      <td>0.037614</td>\n",
       "      <td>0.147766</td>\n",
       "      <td>0.024030</td>\n",
       "      <td>0.133307</td>\n",
       "      <td>0.192798</td>\n",
       "      <td>0.132834</td>\n",
       "      <td>0.201293</td>\n",
       "      <td>0.303233</td>\n",
       "      <td>0.191683</td>\n",
       "      <td>0.131542</td>\n",
       "      <td>0.190033</td>\n",
       "      <td>0.130989</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>9.8</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0.010954</td>\n",
       "      <td>0.074672</td>\n",
       "      <td>0.219833</td>\n",
       "      <td>0.006852</td>\n",
       "      <td>0.149436</td>\n",
       "      <td>0.205513</td>\n",
       "      <td>0.128831</td>\n",
       "      <td>0.237263</td>\n",
       "      <td>0.365430</td>\n",
       "      <td>0.180277</td>\n",
       "      <td>0.148550</td>\n",
       "      <td>0.205168</td>\n",
       "      <td>0.127821</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>0.007116</td>\n",
       "      <td>0.119118</td>\n",
       "      <td>0.166581</td>\n",
       "      <td>0.148597</td>\n",
       "      <td>0.157830</td>\n",
       "      <td>0.199108</td>\n",
       "      <td>0.166876</td>\n",
       "      <td>0.278053</td>\n",
       "      <td>0.323478</td>\n",
       "      <td>0.306013</td>\n",
       "      <td>0.159099</td>\n",
       "      <td>0.198107</td>\n",
       "      <td>0.169636</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12.2</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.006449</td>\n",
       "      <td>0.184311</td>\n",
       "      <td>0.260116</td>\n",
       "      <td>0.153391</td>\n",
       "      <td>0.176761</td>\n",
       "      <td>0.199108</td>\n",
       "      <td>0.185296</td>\n",
       "      <td>0.334926</td>\n",
       "      <td>0.400241</td>\n",
       "      <td>0.317424</td>\n",
       "      <td>0.179017</td>\n",
       "      <td>0.199640</td>\n",
       "      <td>0.189956</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>25.4</td>\n",
       "      <td>7.73</td>\n",
       "      <td>0.011488</td>\n",
       "      <td>0.083584</td>\n",
       "      <td>0.313300</td>\n",
       "      <td>0.064017</td>\n",
       "      <td>0.163076</td>\n",
       "      <td>0.294788</td>\n",
       "      <td>0.179881</td>\n",
       "      <td>0.244744</td>\n",
       "      <td>0.460422</td>\n",
       "      <td>0.235016</td>\n",
       "      <td>0.160641</td>\n",
       "      <td>0.291469</td>\n",
       "      <td>0.178410</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.72</td>\n",
       "      <td>0.014089</td>\n",
       "      <td>0.112868</td>\n",
       "      <td>0.274218</td>\n",
       "      <td>0.136700</td>\n",
       "      <td>0.182546</td>\n",
       "      <td>0.296699</td>\n",
       "      <td>0.170525</td>\n",
       "      <td>0.275507</td>\n",
       "      <td>0.423819</td>\n",
       "      <td>0.302716</td>\n",
       "      <td>0.180349</td>\n",
       "      <td>0.288260</td>\n",
       "      <td>0.169619</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>14.2</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.007575</td>\n",
       "      <td>0.038866</td>\n",
       "      <td>0.168312</td>\n",
       "      <td>-0.036044</td>\n",
       "      <td>0.152059</td>\n",
       "      <td>0.216828</td>\n",
       "      <td>0.104410</td>\n",
       "      <td>0.214989</td>\n",
       "      <td>0.328877</td>\n",
       "      <td>0.146489</td>\n",
       "      <td>0.151821</td>\n",
       "      <td>0.215239</td>\n",
       "      <td>0.105787</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>22.4</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0.015101</td>\n",
       "      <td>0.046345</td>\n",
       "      <td>0.168312</td>\n",
       "      <td>0.070719</td>\n",
       "      <td>0.150853</td>\n",
       "      <td>0.212692</td>\n",
       "      <td>0.168787</td>\n",
       "      <td>0.219572</td>\n",
       "      <td>0.328877</td>\n",
       "      <td>0.245933</td>\n",
       "      <td>0.150472</td>\n",
       "      <td>0.211098</td>\n",
       "      <td>0.169300</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>13.4</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.005809</td>\n",
       "      <td>0.013348</td>\n",
       "      <td>0.148023</td>\n",
       "      <td>-0.028996</td>\n",
       "      <td>0.114097</td>\n",
       "      <td>0.162246</td>\n",
       "      <td>0.098731</td>\n",
       "      <td>0.182217</td>\n",
       "      <td>0.293062</td>\n",
       "      <td>0.144341</td>\n",
       "      <td>0.114538</td>\n",
       "      <td>0.161264</td>\n",
       "      <td>0.099627</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.042143</td>\n",
       "      <td>0.257832</td>\n",
       "      <td>0.428718</td>\n",
       "      <td>0.316659</td>\n",
       "      <td>0.256057</td>\n",
       "      <td>0.357934</td>\n",
       "      <td>0.301488</td>\n",
       "      <td>0.390754</td>\n",
       "      <td>0.535192</td>\n",
       "      <td>0.445596</td>\n",
       "      <td>0.245053</td>\n",
       "      <td>0.337296</td>\n",
       "      <td>0.286712</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>25.2</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.035356</td>\n",
       "      <td>0.108002</td>\n",
       "      <td>0.316659</td>\n",
       "      <td>0.108019</td>\n",
       "      <td>0.197797</td>\n",
       "      <td>0.306705</td>\n",
       "      <td>0.227549</td>\n",
       "      <td>0.269519</td>\n",
       "      <td>0.445596</td>\n",
       "      <td>0.275968</td>\n",
       "      <td>0.191028</td>\n",
       "      <td>0.291196</td>\n",
       "      <td>0.217859</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>17.8</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.009596</td>\n",
       "      <td>0.133056</td>\n",
       "      <td>0.288478</td>\n",
       "      <td>0.092382</td>\n",
       "      <td>0.212656</td>\n",
       "      <td>0.306583</td>\n",
       "      <td>0.196943</td>\n",
       "      <td>0.298893</td>\n",
       "      <td>0.446080</td>\n",
       "      <td>0.260861</td>\n",
       "      <td>0.209324</td>\n",
       "      <td>0.303280</td>\n",
       "      <td>0.191876</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>26.2</td>\n",
       "      <td>7.43</td>\n",
       "      <td>0.011777</td>\n",
       "      <td>0.025322</td>\n",
       "      <td>0.097257</td>\n",
       "      <td>0.029236</td>\n",
       "      <td>0.151936</td>\n",
       "      <td>0.181421</td>\n",
       "      <td>0.154500</td>\n",
       "      <td>0.188980</td>\n",
       "      <td>0.245577</td>\n",
       "      <td>0.190851</td>\n",
       "      <td>0.144270</td>\n",
       "      <td>0.170758</td>\n",
       "      <td>0.145986</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>17.2</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.013991</td>\n",
       "      <td>-0.015447</td>\n",
       "      <td>0.018702</td>\n",
       "      <td>-0.012606</td>\n",
       "      <td>0.120471</td>\n",
       "      <td>0.134568</td>\n",
       "      <td>0.121533</td>\n",
       "      <td>0.154498</td>\n",
       "      <td>0.183592</td>\n",
       "      <td>0.157117</td>\n",
       "      <td>0.116768</td>\n",
       "      <td>0.130599</td>\n",
       "      <td>0.117944</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>38.2</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.102461</td>\n",
       "      <td>0.295392</td>\n",
       "      <td>0.557473</td>\n",
       "      <td>0.308901</td>\n",
       "      <td>0.307197</td>\n",
       "      <td>0.454190</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.416009</td>\n",
       "      <td>0.615263</td>\n",
       "      <td>0.434339</td>\n",
       "      <td>0.282442</td>\n",
       "      <td>0.398523</td>\n",
       "      <td>0.294950</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>10.8</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.208246</td>\n",
       "      <td>0.457222</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.362775</td>\n",
       "      <td>0.371315</td>\n",
       "      <td>0.493478</td>\n",
       "      <td>0.359784</td>\n",
       "      <td>0.543199</td>\n",
       "      <td>0.657822</td>\n",
       "      <td>0.480459</td>\n",
       "      <td>0.339856</td>\n",
       "      <td>0.435916</td>\n",
       "      <td>0.332093</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>13.6</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.330623</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.361712</td>\n",
       "      <td>0.328296</td>\n",
       "      <td>0.493478</td>\n",
       "      <td>0.334804</td>\n",
       "      <td>0.447074</td>\n",
       "      <td>0.657822</td>\n",
       "      <td>0.478059</td>\n",
       "      <td>0.302318</td>\n",
       "      <td>0.435916</td>\n",
       "      <td>0.313049</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>20.8</td>\n",
       "      <td>5.46</td>\n",
       "      <td>0.013974</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.182320</td>\n",
       "      <td>0.015251</td>\n",
       "      <td>0.155328</td>\n",
       "      <td>0.223899</td>\n",
       "      <td>0.149636</td>\n",
       "      <td>0.213109</td>\n",
       "      <td>0.327752</td>\n",
       "      <td>0.203654</td>\n",
       "      <td>0.156074</td>\n",
       "      <td>0.220245</td>\n",
       "      <td>0.151753</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>13.8</td>\n",
       "      <td>2.36</td>\n",
       "      <td>0.014407</td>\n",
       "      <td>0.201766</td>\n",
       "      <td>0.363996</td>\n",
       "      <td>0.182320</td>\n",
       "      <td>0.205465</td>\n",
       "      <td>0.284225</td>\n",
       "      <td>0.186606</td>\n",
       "      <td>0.353258</td>\n",
       "      <td>0.494660</td>\n",
       "      <td>0.326446</td>\n",
       "      <td>0.205606</td>\n",
       "      <td>0.281144</td>\n",
       "      <td>0.188285</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>12.6</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.058457</td>\n",
       "      <td>0.145125</td>\n",
       "      <td>0.363996</td>\n",
       "      <td>0.130551</td>\n",
       "      <td>0.197809</td>\n",
       "      <td>0.279496</td>\n",
       "      <td>0.200216</td>\n",
       "      <td>0.309093</td>\n",
       "      <td>0.494660</td>\n",
       "      <td>0.307795</td>\n",
       "      <td>0.197794</td>\n",
       "      <td>0.278014</td>\n",
       "      <td>0.200050</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>1371.0</td>\n",
       "      <td>749.93</td>\n",
       "      <td>0.012496</td>\n",
       "      <td>0.018962</td>\n",
       "      <td>0.439580</td>\n",
       "      <td>-0.012088</td>\n",
       "      <td>0.140440</td>\n",
       "      <td>0.330568</td>\n",
       "      <td>0.127855</td>\n",
       "      <td>0.193930</td>\n",
       "      <td>0.562144</td>\n",
       "      <td>0.169645</td>\n",
       "      <td>0.139306</td>\n",
       "      <td>0.310592</td>\n",
       "      <td>0.127701</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>20.6</td>\n",
       "      <td>9.33</td>\n",
       "      <td>0.014073</td>\n",
       "      <td>0.131478</td>\n",
       "      <td>0.234754</td>\n",
       "      <td>0.149698</td>\n",
       "      <td>0.181186</td>\n",
       "      <td>0.226339</td>\n",
       "      <td>0.180474</td>\n",
       "      <td>0.295470</td>\n",
       "      <td>0.395217</td>\n",
       "      <td>0.308200</td>\n",
       "      <td>0.181927</td>\n",
       "      <td>0.230374</td>\n",
       "      <td>0.178608</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>24.2</td>\n",
       "      <td>5.24</td>\n",
       "      <td>0.013874</td>\n",
       "      <td>0.147436</td>\n",
       "      <td>0.187865</td>\n",
       "      <td>0.177667</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.217872</td>\n",
       "      <td>0.192079</td>\n",
       "      <td>0.310904</td>\n",
       "      <td>0.349153</td>\n",
       "      <td>0.336923</td>\n",
       "      <td>0.190491</td>\n",
       "      <td>0.217187</td>\n",
       "      <td>0.192958</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>15.8</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.011239</td>\n",
       "      <td>0.138106</td>\n",
       "      <td>0.342986</td>\n",
       "      <td>0.103035</td>\n",
       "      <td>0.182457</td>\n",
       "      <td>0.257095</td>\n",
       "      <td>0.195399</td>\n",
       "      <td>0.292095</td>\n",
       "      <td>0.466620</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.179880</td>\n",
       "      <td>0.253887</td>\n",
       "      <td>0.190421</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>35.0</td>\n",
       "      <td>9.62</td>\n",
       "      <td>0.014407</td>\n",
       "      <td>0.098270</td>\n",
       "      <td>0.196747</td>\n",
       "      <td>0.101668</td>\n",
       "      <td>0.169825</td>\n",
       "      <td>0.218539</td>\n",
       "      <td>0.182440</td>\n",
       "      <td>0.265423</td>\n",
       "      <td>0.345762</td>\n",
       "      <td>0.273391</td>\n",
       "      <td>0.169878</td>\n",
       "      <td>0.216678</td>\n",
       "      <td>0.182371</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.013893</td>\n",
       "      <td>0.149403</td>\n",
       "      <td>0.188605</td>\n",
       "      <td>0.168533</td>\n",
       "      <td>0.192220</td>\n",
       "      <td>0.218539</td>\n",
       "      <td>0.191094</td>\n",
       "      <td>0.311343</td>\n",
       "      <td>0.345192</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.192421</td>\n",
       "      <td>0.216678</td>\n",
       "      <td>0.192242</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>19.2</td>\n",
       "      <td>5.55</td>\n",
       "      <td>0.016234</td>\n",
       "      <td>0.158602</td>\n",
       "      <td>0.263555</td>\n",
       "      <td>0.153176</td>\n",
       "      <td>0.184379</td>\n",
       "      <td>0.217031</td>\n",
       "      <td>0.187081</td>\n",
       "      <td>0.309208</td>\n",
       "      <td>0.396284</td>\n",
       "      <td>0.307171</td>\n",
       "      <td>0.182470</td>\n",
       "      <td>0.215067</td>\n",
       "      <td>0.188505</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.99</td>\n",
       "      <td>0.013957</td>\n",
       "      <td>0.006973</td>\n",
       "      <td>0.180380</td>\n",
       "      <td>-0.006413</td>\n",
       "      <td>0.126342</td>\n",
       "      <td>0.188161</td>\n",
       "      <td>0.123636</td>\n",
       "      <td>0.177138</td>\n",
       "      <td>0.331667</td>\n",
       "      <td>0.165239</td>\n",
       "      <td>0.124423</td>\n",
       "      <td>0.185417</td>\n",
       "      <td>0.121232</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>40.2</td>\n",
       "      <td>17.48</td>\n",
       "      <td>0.045240</td>\n",
       "      <td>0.213050</td>\n",
       "      <td>0.496593</td>\n",
       "      <td>0.238806</td>\n",
       "      <td>0.219402</td>\n",
       "      <td>0.316513</td>\n",
       "      <td>0.230157</td>\n",
       "      <td>0.361392</td>\n",
       "      <td>0.585740</td>\n",
       "      <td>0.389322</td>\n",
       "      <td>0.217084</td>\n",
       "      <td>0.304441</td>\n",
       "      <td>0.228538</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>9.2</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.010295</td>\n",
       "      <td>0.170523</td>\n",
       "      <td>0.251598</td>\n",
       "      <td>0.193757</td>\n",
       "      <td>0.203791</td>\n",
       "      <td>0.248121</td>\n",
       "      <td>0.226792</td>\n",
       "      <td>0.323056</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.345216</td>\n",
       "      <td>0.200444</td>\n",
       "      <td>0.238342</td>\n",
       "      <td>0.222003</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>11.2</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.015749</td>\n",
       "      <td>0.136608</td>\n",
       "      <td>0.277001</td>\n",
       "      <td>0.161328</td>\n",
       "      <td>0.170268</td>\n",
       "      <td>0.225477</td>\n",
       "      <td>0.178925</td>\n",
       "      <td>0.303697</td>\n",
       "      <td>0.413502</td>\n",
       "      <td>0.323598</td>\n",
       "      <td>0.174282</td>\n",
       "      <td>0.224885</td>\n",
       "      <td>0.182516</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>17.8</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.155247</td>\n",
       "      <td>0.330093</td>\n",
       "      <td>0.469003</td>\n",
       "      <td>0.356148</td>\n",
       "      <td>0.272169</td>\n",
       "      <td>0.337812</td>\n",
       "      <td>0.276678</td>\n",
       "      <td>0.457162</td>\n",
       "      <td>0.559928</td>\n",
       "      <td>0.486720</td>\n",
       "      <td>0.265642</td>\n",
       "      <td>0.320249</td>\n",
       "      <td>0.274463</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.52</td>\n",
       "      <td>0.018098</td>\n",
       "      <td>0.254692</td>\n",
       "      <td>0.516491</td>\n",
       "      <td>0.322875</td>\n",
       "      <td>0.201910</td>\n",
       "      <td>0.350779</td>\n",
       "      <td>0.223278</td>\n",
       "      <td>0.390866</td>\n",
       "      <td>0.587131</td>\n",
       "      <td>0.445774</td>\n",
       "      <td>0.202703</td>\n",
       "      <td>0.329158</td>\n",
       "      <td>0.223362</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>28.2</td>\n",
       "      <td>7.69</td>\n",
       "      <td>0.006567</td>\n",
       "      <td>0.146814</td>\n",
       "      <td>0.385433</td>\n",
       "      <td>0.125680</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.285411</td>\n",
       "      <td>0.154223</td>\n",
       "      <td>0.304794</td>\n",
       "      <td>0.499831</td>\n",
       "      <td>0.284385</td>\n",
       "      <td>0.175522</td>\n",
       "      <td>0.278504</td>\n",
       "      <td>0.156626</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>26.8</td>\n",
       "      <td>8.04</td>\n",
       "      <td>0.007438</td>\n",
       "      <td>0.207102</td>\n",
       "      <td>0.385433</td>\n",
       "      <td>0.193209</td>\n",
       "      <td>0.178283</td>\n",
       "      <td>0.285411</td>\n",
       "      <td>0.172225</td>\n",
       "      <td>0.344908</td>\n",
       "      <td>0.499831</td>\n",
       "      <td>0.334093</td>\n",
       "      <td>0.178442</td>\n",
       "      <td>0.278504</td>\n",
       "      <td>0.172911</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>27.6</td>\n",
       "      <td>6.24</td>\n",
       "      <td>0.009728</td>\n",
       "      <td>0.234730</td>\n",
       "      <td>0.350721</td>\n",
       "      <td>0.212616</td>\n",
       "      <td>0.183752</td>\n",
       "      <td>0.270642</td>\n",
       "      <td>0.168260</td>\n",
       "      <td>0.362963</td>\n",
       "      <td>0.455688</td>\n",
       "      <td>0.342646</td>\n",
       "      <td>0.182619</td>\n",
       "      <td>0.257602</td>\n",
       "      <td>0.168281</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>29.6</td>\n",
       "      <td>6.91</td>\n",
       "      <td>0.005166</td>\n",
       "      <td>0.193508</td>\n",
       "      <td>0.350721</td>\n",
       "      <td>0.200861</td>\n",
       "      <td>0.196017</td>\n",
       "      <td>0.270642</td>\n",
       "      <td>0.189007</td>\n",
       "      <td>0.342890</td>\n",
       "      <td>0.455688</td>\n",
       "      <td>0.349456</td>\n",
       "      <td>0.195746</td>\n",
       "      <td>0.257602</td>\n",
       "      <td>0.190441</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.082814</td>\n",
       "      <td>0.200861</td>\n",
       "      <td>0.107754</td>\n",
       "      <td>0.170405</td>\n",
       "      <td>0.191372</td>\n",
       "      <td>0.174886</td>\n",
       "      <td>0.268113</td>\n",
       "      <td>0.349456</td>\n",
       "      <td>0.291375</td>\n",
       "      <td>0.176252</td>\n",
       "      <td>0.194704</td>\n",
       "      <td>0.181641</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>31.6</td>\n",
       "      <td>6.58</td>\n",
       "      <td>0.008040</td>\n",
       "      <td>0.075691</td>\n",
       "      <td>0.286380</td>\n",
       "      <td>0.062003</td>\n",
       "      <td>0.154751</td>\n",
       "      <td>0.250764</td>\n",
       "      <td>0.156104</td>\n",
       "      <td>0.265341</td>\n",
       "      <td>0.447282</td>\n",
       "      <td>0.260717</td>\n",
       "      <td>0.163181</td>\n",
       "      <td>0.256500</td>\n",
       "      <td>0.166552</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>14.6</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.007542</td>\n",
       "      <td>0.318210</td>\n",
       "      <td>0.585155</td>\n",
       "      <td>0.322719</td>\n",
       "      <td>0.204563</td>\n",
       "      <td>0.314253</td>\n",
       "      <td>0.196393</td>\n",
       "      <td>0.453089</td>\n",
       "      <td>0.664790</td>\n",
       "      <td>0.458946</td>\n",
       "      <td>0.210641</td>\n",
       "      <td>0.315023</td>\n",
       "      <td>0.204095</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.06</td>\n",
       "      <td>0.258536</td>\n",
       "      <td>0.505007</td>\n",
       "      <td>0.680353</td>\n",
       "      <td>0.539452</td>\n",
       "      <td>0.337534</td>\n",
       "      <td>0.446219</td>\n",
       "      <td>0.321042</td>\n",
       "      <td>0.601547</td>\n",
       "      <td>0.722800</td>\n",
       "      <td>0.625850</td>\n",
       "      <td>0.329439</td>\n",
       "      <td>0.418778</td>\n",
       "      <td>0.315023</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>17.2</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0.109831</td>\n",
       "      <td>0.498591</td>\n",
       "      <td>0.680353</td>\n",
       "      <td>0.519791</td>\n",
       "      <td>0.337068</td>\n",
       "      <td>0.419392</td>\n",
       "      <td>0.335836</td>\n",
       "      <td>0.590118</td>\n",
       "      <td>0.715275</td>\n",
       "      <td>0.593978</td>\n",
       "      <td>0.326191</td>\n",
       "      <td>0.392735</td>\n",
       "      <td>0.326138</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    OID__x  Id  gridcode  Shape_Length  Shape_Area   CH_mean  ARVI_mean  \\\n",
       "0        1   1         1          14.2        5.04  0.010000   0.053054   \n",
       "1        2   2         2          26.2        5.73  0.016248   0.040314   \n",
       "2        3   3         3          10.6        2.29  0.014410   0.111847   \n",
       "3        4   4         4          11.0        2.85  0.012281   0.204984   \n",
       "4        5   5         5          18.2        3.78  0.012011   0.260015   \n",
       "5        6   6         6          24.6        5.34  0.011760   0.143938   \n",
       "6        7   7         7          13.6        2.37  0.011519   0.095585   \n",
       "7        8   8         8          11.2        2.51  0.010956  -0.026384   \n",
       "8        9   9         9          20.2        5.49  0.016120   0.037614   \n",
       "9       10  10        10           9.8        2.83  0.010954   0.074672   \n",
       "10      11  11        11          22.0        4.75  0.007116   0.119118   \n",
       "11      12  12        12          12.2        4.14  0.006449   0.184311   \n",
       "12      13  13        13          25.4        7.73  0.011488   0.083584   \n",
       "13      14  14        14          13.0        4.72  0.014089   0.112868   \n",
       "14      15  15        15          14.2        3.01  0.007575   0.038866   \n",
       "15      16  16        16          22.4        4.47  0.015101   0.046345   \n",
       "16      17  17        17          13.4        2.41  0.005809   0.013348   \n",
       "17      18  18        18          18.0        5.04  0.042143   0.257832   \n",
       "18      19  19        19          25.2        4.50  0.035356   0.108002   \n",
       "19      20  20        20          17.8        3.22  0.009596   0.133056   \n",
       "20      21  21        21          26.2        7.43  0.011777   0.025322   \n",
       "21      22  22        22          17.2        2.33  0.013991  -0.015447   \n",
       "22      23  23        23          38.2        5.16  0.102461   0.295392   \n",
       "23      24  24        24          10.8        2.85  0.208246   0.457222   \n",
       "24      25  25        25          13.6        4.23  0.322222   0.330623   \n",
       "25      26  26        26          20.8        5.46  0.013974   0.031600   \n",
       "26      27  27        27          13.8        2.36  0.014407   0.201766   \n",
       "27      28  28        28          12.6        4.86  0.058457   0.145125   \n",
       "28      29  29        29        1371.0      749.93  0.012496   0.018962   \n",
       "29      30  30        30          20.6        9.33  0.014073   0.131478   \n",
       "30      31  31        31          24.2        5.24  0.013874   0.147436   \n",
       "31      32  32        32          15.8        5.65  0.011239   0.138106   \n",
       "32      33  33        33          35.0        9.62  0.014407   0.098270   \n",
       "33      34  34        34           8.6        2.44  0.013893   0.149403   \n",
       "34      35  35        35          19.2        5.55  0.016234   0.158602   \n",
       "35      36  36        36          19.0        5.99  0.013957   0.006973   \n",
       "36      37  37        37          40.2       17.48  0.045240   0.213050   \n",
       "37      38  38        38           9.2        3.05  0.010295   0.170523   \n",
       "38      39  39        39          11.2        2.07  0.015749   0.136608   \n",
       "39      40  40        40          17.8        8.69  0.155247   0.330093   \n",
       "40      41  41        41          17.0        6.52  0.018098   0.254692   \n",
       "41      42  42        42          28.2        7.69  0.006567   0.146814   \n",
       "42      43  43        43          26.8        8.04  0.007438   0.207102   \n",
       "43      44  44        44          27.6        6.24  0.009728   0.234730   \n",
       "44      45  45        45          29.6        6.91  0.005166   0.193508   \n",
       "45      46  46        46          11.0        2.43  0.010700   0.082814   \n",
       "46      47  47        47          31.6        6.58  0.008040   0.075691   \n",
       "47      48  48        48          14.6        2.40  0.007542   0.318210   \n",
       "48      49  49        49          20.0        8.06  0.258536   0.505007   \n",
       "49      50  50        50          17.2        3.55  0.109831   0.498591   \n",
       "\n",
       "    ARVI_max  ARVI_med  EVI_mean   EVI_max   EVI_med  NDVI_mean  NDVI_max  \\\n",
       "0   0.252378 -0.007871  0.145961  0.234673  0.126273   0.221155  0.400501   \n",
       "1   0.252378  0.043301  0.141544  0.234673  0.135954   0.206992  0.400501   \n",
       "2   0.281741  0.085010  0.167371  0.254378  0.152276   0.274226  0.432882   \n",
       "3   0.310943  0.281741  0.211967  0.254378  0.254378   0.361578  0.438509   \n",
       "4   0.365155  0.256816  0.203891  0.232604  0.209083   0.403924  0.488291   \n",
       "5   0.365155  0.159118  0.175792  0.227181  0.187912   0.301899  0.488291   \n",
       "6   0.153732  0.090776  0.153697  0.170868  0.159732   0.257637  0.308600   \n",
       "7   0.061265 -0.028525  0.106579  0.162647  0.093855   0.143429  0.227148   \n",
       "8   0.147766  0.024030  0.133307  0.192798  0.132834   0.201293  0.303233   \n",
       "9   0.219833  0.006852  0.149436  0.205513  0.128831   0.237263  0.365430   \n",
       "10  0.166581  0.148597  0.157830  0.199108  0.166876   0.278053  0.323478   \n",
       "11  0.260116  0.153391  0.176761  0.199108  0.185296   0.334926  0.400241   \n",
       "12  0.313300  0.064017  0.163076  0.294788  0.179881   0.244744  0.460422   \n",
       "13  0.274218  0.136700  0.182546  0.296699  0.170525   0.275507  0.423819   \n",
       "14  0.168312 -0.036044  0.152059  0.216828  0.104410   0.214989  0.328877   \n",
       "15  0.168312  0.070719  0.150853  0.212692  0.168787   0.219572  0.328877   \n",
       "16  0.148023 -0.028996  0.114097  0.162246  0.098731   0.182217  0.293062   \n",
       "17  0.428718  0.316659  0.256057  0.357934  0.301488   0.390754  0.535192   \n",
       "18  0.316659  0.108019  0.197797  0.306705  0.227549   0.269519  0.445596   \n",
       "19  0.288478  0.092382  0.212656  0.306583  0.196943   0.298893  0.446080   \n",
       "20  0.097257  0.029236  0.151936  0.181421  0.154500   0.188980  0.245577   \n",
       "21  0.018702 -0.012606  0.120471  0.134568  0.121533   0.154498  0.183592   \n",
       "22  0.557473  0.308901  0.307197  0.454190  0.307692   0.416009  0.615263   \n",
       "23  0.607843  0.362775  0.371315  0.493478  0.359784   0.543199  0.657822   \n",
       "24  0.607843  0.361712  0.328296  0.493478  0.334804   0.447074  0.657822   \n",
       "25  0.182320  0.015251  0.155328  0.223899  0.149636   0.213109  0.327752   \n",
       "26  0.363996  0.182320  0.205465  0.284225  0.186606   0.353258  0.494660   \n",
       "27  0.363996  0.130551  0.197809  0.279496  0.200216   0.309093  0.494660   \n",
       "28  0.439580 -0.012088  0.140440  0.330568  0.127855   0.193930  0.562144   \n",
       "29  0.234754  0.149698  0.181186  0.226339  0.180474   0.295470  0.395217   \n",
       "30  0.187865  0.177667  0.189405  0.217872  0.192079   0.310904  0.349153   \n",
       "31  0.342986  0.103035  0.182457  0.257095  0.195399   0.292095  0.466620   \n",
       "32  0.196747  0.101668  0.169825  0.218539  0.182440   0.265423  0.345762   \n",
       "33  0.188605  0.168533  0.192220  0.218539  0.191094   0.311343  0.345192   \n",
       "34  0.263555  0.153176  0.184379  0.217031  0.187081   0.309208  0.396284   \n",
       "35  0.180380 -0.006413  0.126342  0.188161  0.123636   0.177138  0.331667   \n",
       "36  0.496593  0.238806  0.219402  0.316513  0.230157   0.361392  0.585740   \n",
       "37  0.251598  0.193757  0.203791  0.248121  0.226792   0.323056  0.386100   \n",
       "38  0.277001  0.161328  0.170268  0.225477  0.178925   0.303697  0.413502   \n",
       "39  0.469003  0.356148  0.272169  0.337812  0.276678   0.457162  0.559928   \n",
       "40  0.516491  0.322875  0.201910  0.350779  0.223278   0.390866  0.587131   \n",
       "41  0.385433  0.125680  0.174118  0.285411  0.154223   0.304794  0.499831   \n",
       "42  0.385433  0.193209  0.178283  0.285411  0.172225   0.344908  0.499831   \n",
       "43  0.350721  0.212616  0.183752  0.270642  0.168260   0.362963  0.455688   \n",
       "44  0.350721  0.200861  0.196017  0.270642  0.189007   0.342890  0.455688   \n",
       "45  0.200861  0.107754  0.170405  0.191372  0.174886   0.268113  0.349456   \n",
       "46  0.286380  0.062003  0.154751  0.250764  0.156104   0.265341  0.447282   \n",
       "47  0.585155  0.322719  0.204563  0.314253  0.196393   0.453089  0.664790   \n",
       "48  0.680353  0.539452  0.337534  0.446219  0.321042   0.601547  0.722800   \n",
       "49  0.680353  0.519791  0.337068  0.419392  0.335836   0.590118  0.715275   \n",
       "\n",
       "    NDVI_med  SAVI_mean  SAVI_max  SAVI_med  Veg_class  \n",
       "0   0.170450   0.145235  0.234261  0.125838      woody  \n",
       "1   0.207240   0.139659  0.234261  0.134476      woody  \n",
       "2   0.249348   0.167777  0.255698  0.152786  non-woody  \n",
       "3   0.432882   0.213450  0.255698  0.255698  non-woody  \n",
       "4   0.410121   0.207537  0.236474  0.212997  non-woody  \n",
       "5   0.313752   0.176406  0.231180  0.189477  non-woody  \n",
       "6   0.257738   0.154285  0.172316  0.160627      woody  \n",
       "7   0.133892   0.104317  0.159023  0.091390  non-woody  \n",
       "8   0.191683   0.131542  0.190033  0.130989      woody  \n",
       "9   0.180277   0.148550  0.205168  0.127821      woody  \n",
       "10  0.306013   0.159099  0.198107  0.169636  non-woody  \n",
       "11  0.317424   0.179017  0.199640  0.189956  non-woody  \n",
       "12  0.235016   0.160641  0.291469  0.178410  non-woody  \n",
       "13  0.302716   0.180349  0.288260  0.169619  non-woody  \n",
       "14  0.146489   0.151821  0.215239  0.105787      woody  \n",
       "15  0.245933   0.150472  0.211098  0.169300      woody  \n",
       "16  0.144341   0.114538  0.161264  0.099627      woody  \n",
       "17  0.445596   0.245053  0.337296  0.286712  non-woody  \n",
       "18  0.275968   0.191028  0.291196  0.217859  non-woody  \n",
       "19  0.260861   0.209324  0.303280  0.191876  non-woody  \n",
       "20  0.190851   0.144270  0.170758  0.145986      woody  \n",
       "21  0.157117   0.116768  0.130599  0.117944      woody  \n",
       "22  0.434339   0.282442  0.398523  0.294950  non-woody  \n",
       "23  0.480459   0.339856  0.435916  0.332093  non-woody  \n",
       "24  0.478059   0.302318  0.435916  0.313049  non-woody  \n",
       "25  0.203654   0.156074  0.220245  0.151753      woody  \n",
       "26  0.326446   0.205606  0.281144  0.188285  non-woody  \n",
       "27  0.307795   0.197794  0.278014  0.200050  non-woody  \n",
       "28  0.169645   0.139306  0.310592  0.127701  non-woody  \n",
       "29  0.308200   0.181927  0.230374  0.178608  non-woody  \n",
       "30  0.336923   0.190491  0.217187  0.192958  non-woody  \n",
       "31  0.268000   0.179880  0.253887  0.190421  non-woody  \n",
       "32  0.273391   0.169878  0.216678  0.182371      woody  \n",
       "33  0.326923   0.192421  0.216678  0.192242  non-woody  \n",
       "34  0.307171   0.182470  0.215067  0.188505  non-woody  \n",
       "35  0.165239   0.124423  0.185417  0.121232      woody  \n",
       "36  0.389322   0.217084  0.304441  0.228538  non-woody  \n",
       "37  0.345216   0.200444  0.238342  0.222003  non-woody  \n",
       "38  0.323598   0.174282  0.224885  0.182516  non-woody  \n",
       "39  0.486720   0.265642  0.320249  0.274463  non-woody  \n",
       "40  0.445774   0.202703  0.329158  0.223362  non-woody  \n",
       "41  0.284385   0.175522  0.278504  0.156626  non-woody  \n",
       "42  0.334093   0.178442  0.278504  0.172911  non-woody  \n",
       "43  0.342646   0.182619  0.257602  0.168281  non-woody  \n",
       "44  0.349456   0.195746  0.257602  0.190441  non-woody  \n",
       "45  0.291375   0.176252  0.194704  0.181641      woody  \n",
       "46  0.260717   0.163181  0.256500  0.166552  non-woody  \n",
       "47  0.458946   0.210641  0.315023  0.204095  non-woody  \n",
       "48  0.625850   0.329439  0.418778  0.315023  non-woody  \n",
       "49  0.593978   0.326191  0.392735  0.326138  non-woody  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2017\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21 JORN -> 21 SRER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 3489.9128235 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.8321678321678322, 0.8387850467289719, 0.8457943925233645, 0.8341121495327103, 0.822429906542056]\n",
      "Avg accuracy: 0.8346578654989869\n",
      "Std of accuracy : \n",
      "0.007708137404436038\n",
      "\n",
      "[[ 501  197]\n",
      " [ 157 1286]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.76      0.72      0.74       698\n",
      "       woody       0.87      0.89      0.88      1443\n",
      "\n",
      "    accuracy                           0.83      2141\n",
      "   macro avg       0.81      0.80      0.81      2141\n",
      "weighted avg       0.83      0.83      0.83      2141\n",
      "\n",
      "Sensitivity: 0.7177650429799427\n",
      "Specificity: 0.8911988911988912\n",
      "Precision: 0.7613981762917933\n",
      "F1_Score: 0.738938053097345\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9391908662199057\n",
      "Specificity: 0.2886178861788618\n",
      "precision: 0.6986469864698647\n",
      "f1_score: 0.4084861560589716\n",
      "Accuracy: [0.7256961814240454]\n"
     ]
    }
   ],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2021.iloc[:, 5:18], dfJornada2021.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-72096d3917d8>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bigtest_df[\"Veg_class\"] = finalPredicted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OID_</th>\n",
       "      <th>Id</th>\n",
       "      <th>gridcode</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>CH_mean</th>\n",
       "      <th>ARVI_mean</th>\n",
       "      <th>ARVI_med</th>\n",
       "      <th>ARVI_max</th>\n",
       "      <th>EVI_mean</th>\n",
       "      <th>EVI_med</th>\n",
       "      <th>EVI_max</th>\n",
       "      <th>NDVI_mean</th>\n",
       "      <th>NDVI_med</th>\n",
       "      <th>NDVI_max</th>\n",
       "      <th>SAVI_mean</th>\n",
       "      <th>SAVI_med</th>\n",
       "      <th>SAVI_max</th>\n",
       "      <th>Veg_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.8</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207123</td>\n",
       "      <td>0.167813</td>\n",
       "      <td>0.335815</td>\n",
       "      <td>0.212394</td>\n",
       "      <td>0.186195</td>\n",
       "      <td>0.291992</td>\n",
       "      <td>0.350170</td>\n",
       "      <td>0.315030</td>\n",
       "      <td>0.463047</td>\n",
       "      <td>0.208236</td>\n",
       "      <td>0.183683</td>\n",
       "      <td>0.282265</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.319239</td>\n",
       "      <td>0.334353</td>\n",
       "      <td>0.347863</td>\n",
       "      <td>0.275198</td>\n",
       "      <td>0.279712</td>\n",
       "      <td>0.299364</td>\n",
       "      <td>0.440041</td>\n",
       "      <td>0.453643</td>\n",
       "      <td>0.467307</td>\n",
       "      <td>0.263501</td>\n",
       "      <td>0.269123</td>\n",
       "      <td>0.285889</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3486.2</td>\n",
       "      <td>1942.77</td>\n",
       "      <td>0.120091</td>\n",
       "      <td>0.014902</td>\n",
       "      <td>-0.002058</td>\n",
       "      <td>0.388165</td>\n",
       "      <td>0.155599</td>\n",
       "      <td>0.143094</td>\n",
       "      <td>0.441733</td>\n",
       "      <td>0.199209</td>\n",
       "      <td>0.184422</td>\n",
       "      <td>0.503913</td>\n",
       "      <td>0.155182</td>\n",
       "      <td>0.144957</td>\n",
       "      <td>0.397375</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>31.6</td>\n",
       "      <td>9.23</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.419381</td>\n",
       "      <td>0.443782</td>\n",
       "      <td>0.507713</td>\n",
       "      <td>0.344480</td>\n",
       "      <td>0.337613</td>\n",
       "      <td>0.405390</td>\n",
       "      <td>0.527309</td>\n",
       "      <td>0.540501</td>\n",
       "      <td>0.596006</td>\n",
       "      <td>0.326518</td>\n",
       "      <td>0.321309</td>\n",
       "      <td>0.381685</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>31.8</td>\n",
       "      <td>11.82</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.465333</td>\n",
       "      <td>0.440135</td>\n",
       "      <td>0.565654</td>\n",
       "      <td>0.335128</td>\n",
       "      <td>0.329047</td>\n",
       "      <td>0.380147</td>\n",
       "      <td>0.562001</td>\n",
       "      <td>0.551508</td>\n",
       "      <td>0.639035</td>\n",
       "      <td>0.321567</td>\n",
       "      <td>0.314126</td>\n",
       "      <td>0.359091</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>61.0</td>\n",
       "      <td>15.73</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.388299</td>\n",
       "      <td>0.392301</td>\n",
       "      <td>0.558201</td>\n",
       "      <td>0.340119</td>\n",
       "      <td>0.329843</td>\n",
       "      <td>0.417976</td>\n",
       "      <td>0.506015</td>\n",
       "      <td>0.510729</td>\n",
       "      <td>0.640907</td>\n",
       "      <td>0.323304</td>\n",
       "      <td>0.316266</td>\n",
       "      <td>0.390960</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>11.6</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.435153</td>\n",
       "      <td>0.501700</td>\n",
       "      <td>0.508019</td>\n",
       "      <td>0.350849</td>\n",
       "      <td>0.352031</td>\n",
       "      <td>0.410666</td>\n",
       "      <td>0.542913</td>\n",
       "      <td>0.591679</td>\n",
       "      <td>0.603258</td>\n",
       "      <td>0.334208</td>\n",
       "      <td>0.337273</td>\n",
       "      <td>0.385493</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>55.2</td>\n",
       "      <td>36.59</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.094267</td>\n",
       "      <td>0.073039</td>\n",
       "      <td>0.375784</td>\n",
       "      <td>0.202749</td>\n",
       "      <td>0.197483</td>\n",
       "      <td>0.331198</td>\n",
       "      <td>0.270160</td>\n",
       "      <td>0.256369</td>\n",
       "      <td>0.487022</td>\n",
       "      <td>0.200563</td>\n",
       "      <td>0.196467</td>\n",
       "      <td>0.316553</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>33.4</td>\n",
       "      <td>10.51</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.299233</td>\n",
       "      <td>0.296807</td>\n",
       "      <td>0.441674</td>\n",
       "      <td>0.286510</td>\n",
       "      <td>0.296106</td>\n",
       "      <td>0.360564</td>\n",
       "      <td>0.431976</td>\n",
       "      <td>0.435374</td>\n",
       "      <td>0.545392</td>\n",
       "      <td>0.275263</td>\n",
       "      <td>0.284235</td>\n",
       "      <td>0.340146</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.91</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.382419</td>\n",
       "      <td>0.383198</td>\n",
       "      <td>0.510562</td>\n",
       "      <td>0.302928</td>\n",
       "      <td>0.308982</td>\n",
       "      <td>0.372048</td>\n",
       "      <td>0.496885</td>\n",
       "      <td>0.503172</td>\n",
       "      <td>0.601409</td>\n",
       "      <td>0.291949</td>\n",
       "      <td>0.297567</td>\n",
       "      <td>0.354577</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>38.4</td>\n",
       "      <td>10.31</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.169491</td>\n",
       "      <td>0.177157</td>\n",
       "      <td>0.213876</td>\n",
       "      <td>0.249109</td>\n",
       "      <td>0.261426</td>\n",
       "      <td>0.290336</td>\n",
       "      <td>0.333254</td>\n",
       "      <td>0.342163</td>\n",
       "      <td>0.371908</td>\n",
       "      <td>0.242211</td>\n",
       "      <td>0.253511</td>\n",
       "      <td>0.277432</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12.4</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.416340</td>\n",
       "      <td>0.448779</td>\n",
       "      <td>0.511665</td>\n",
       "      <td>0.357268</td>\n",
       "      <td>0.363871</td>\n",
       "      <td>0.418755</td>\n",
       "      <td>0.530844</td>\n",
       "      <td>0.553736</td>\n",
       "      <td>0.608591</td>\n",
       "      <td>0.339472</td>\n",
       "      <td>0.344724</td>\n",
       "      <td>0.393341</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12.18</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.423449</td>\n",
       "      <td>0.434017</td>\n",
       "      <td>0.527901</td>\n",
       "      <td>0.319252</td>\n",
       "      <td>0.333124</td>\n",
       "      <td>0.359998</td>\n",
       "      <td>0.530337</td>\n",
       "      <td>0.537509</td>\n",
       "      <td>0.609810</td>\n",
       "      <td>0.307631</td>\n",
       "      <td>0.319725</td>\n",
       "      <td>0.343846</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>16.4</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.120560</td>\n",
       "      <td>0.124836</td>\n",
       "      <td>0.177133</td>\n",
       "      <td>0.224723</td>\n",
       "      <td>0.233874</td>\n",
       "      <td>0.244178</td>\n",
       "      <td>0.287848</td>\n",
       "      <td>0.294703</td>\n",
       "      <td>0.329086</td>\n",
       "      <td>0.217404</td>\n",
       "      <td>0.225176</td>\n",
       "      <td>0.235975</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.398133</td>\n",
       "      <td>0.398133</td>\n",
       "      <td>0.436209</td>\n",
       "      <td>0.316781</td>\n",
       "      <td>0.316781</td>\n",
       "      <td>0.344585</td>\n",
       "      <td>0.504631</td>\n",
       "      <td>0.504631</td>\n",
       "      <td>0.535921</td>\n",
       "      <td>0.300922</td>\n",
       "      <td>0.300922</td>\n",
       "      <td>0.324960</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>25.2</td>\n",
       "      <td>7.42</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.281256</td>\n",
       "      <td>0.303723</td>\n",
       "      <td>0.460821</td>\n",
       "      <td>0.290412</td>\n",
       "      <td>0.288456</td>\n",
       "      <td>0.414814</td>\n",
       "      <td>0.419559</td>\n",
       "      <td>0.441908</td>\n",
       "      <td>0.568959</td>\n",
       "      <td>0.278438</td>\n",
       "      <td>0.274564</td>\n",
       "      <td>0.386818</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>11.2</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.313890</td>\n",
       "      <td>0.310232</td>\n",
       "      <td>0.331245</td>\n",
       "      <td>0.325886</td>\n",
       "      <td>0.325597</td>\n",
       "      <td>0.371669</td>\n",
       "      <td>0.449804</td>\n",
       "      <td>0.440257</td>\n",
       "      <td>0.469840</td>\n",
       "      <td>0.309330</td>\n",
       "      <td>0.309161</td>\n",
       "      <td>0.348700</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>33.8</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.168590</td>\n",
       "      <td>0.143379</td>\n",
       "      <td>0.295723</td>\n",
       "      <td>0.256061</td>\n",
       "      <td>0.255124</td>\n",
       "      <td>0.324244</td>\n",
       "      <td>0.334354</td>\n",
       "      <td>0.322331</td>\n",
       "      <td>0.432587</td>\n",
       "      <td>0.248646</td>\n",
       "      <td>0.252359</td>\n",
       "      <td>0.305183</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.53</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.384646</td>\n",
       "      <td>0.467158</td>\n",
       "      <td>0.553504</td>\n",
       "      <td>0.331591</td>\n",
       "      <td>0.363822</td>\n",
       "      <td>0.472948</td>\n",
       "      <td>0.494661</td>\n",
       "      <td>0.565477</td>\n",
       "      <td>0.634859</td>\n",
       "      <td>0.311251</td>\n",
       "      <td>0.343133</td>\n",
       "      <td>0.430946</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>19.2</td>\n",
       "      <td>4.84</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.315022</td>\n",
       "      <td>0.279372</td>\n",
       "      <td>0.453170</td>\n",
       "      <td>0.324405</td>\n",
       "      <td>0.289330</td>\n",
       "      <td>0.391028</td>\n",
       "      <td>0.432035</td>\n",
       "      <td>0.402736</td>\n",
       "      <td>0.535287</td>\n",
       "      <td>0.296618</td>\n",
       "      <td>0.268295</td>\n",
       "      <td>0.350672</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>36.8</td>\n",
       "      <td>15.32</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.130060</td>\n",
       "      <td>0.107600</td>\n",
       "      <td>0.299342</td>\n",
       "      <td>0.245448</td>\n",
       "      <td>0.243397</td>\n",
       "      <td>0.378971</td>\n",
       "      <td>0.296571</td>\n",
       "      <td>0.281376</td>\n",
       "      <td>0.436190</td>\n",
       "      <td>0.234413</td>\n",
       "      <td>0.233731</td>\n",
       "      <td>0.345058</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.152072</td>\n",
       "      <td>0.163493</td>\n",
       "      <td>0.219467</td>\n",
       "      <td>0.257766</td>\n",
       "      <td>0.274820</td>\n",
       "      <td>0.282380</td>\n",
       "      <td>0.318086</td>\n",
       "      <td>0.332958</td>\n",
       "      <td>0.368765</td>\n",
       "      <td>0.247614</td>\n",
       "      <td>0.261819</td>\n",
       "      <td>0.270756</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>21.2</td>\n",
       "      <td>5.80</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.415082</td>\n",
       "      <td>0.412476</td>\n",
       "      <td>0.538134</td>\n",
       "      <td>0.400424</td>\n",
       "      <td>0.416252</td>\n",
       "      <td>0.465685</td>\n",
       "      <td>0.525296</td>\n",
       "      <td>0.529354</td>\n",
       "      <td>0.621921</td>\n",
       "      <td>0.367831</td>\n",
       "      <td>0.382417</td>\n",
       "      <td>0.423768</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>16.2</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252461</td>\n",
       "      <td>0.251237</td>\n",
       "      <td>0.279508</td>\n",
       "      <td>0.327898</td>\n",
       "      <td>0.326480</td>\n",
       "      <td>0.332338</td>\n",
       "      <td>0.397750</td>\n",
       "      <td>0.395355</td>\n",
       "      <td>0.415763</td>\n",
       "      <td>0.304878</td>\n",
       "      <td>0.305889</td>\n",
       "      <td>0.307276</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>17.6</td>\n",
       "      <td>4.56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141727</td>\n",
       "      <td>0.068750</td>\n",
       "      <td>0.313742</td>\n",
       "      <td>0.257259</td>\n",
       "      <td>0.212822</td>\n",
       "      <td>0.364320</td>\n",
       "      <td>0.308694</td>\n",
       "      <td>0.247167</td>\n",
       "      <td>0.452860</td>\n",
       "      <td>0.245734</td>\n",
       "      <td>0.206406</td>\n",
       "      <td>0.339876</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.350948</td>\n",
       "      <td>0.350948</td>\n",
       "      <td>0.388065</td>\n",
       "      <td>0.343742</td>\n",
       "      <td>0.343742</td>\n",
       "      <td>0.365579</td>\n",
       "      <td>0.470256</td>\n",
       "      <td>0.470256</td>\n",
       "      <td>0.487547</td>\n",
       "      <td>0.319981</td>\n",
       "      <td>0.319981</td>\n",
       "      <td>0.340820</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>19.2</td>\n",
       "      <td>3.83</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312604</td>\n",
       "      <td>0.297165</td>\n",
       "      <td>0.362959</td>\n",
       "      <td>0.394734</td>\n",
       "      <td>0.375427</td>\n",
       "      <td>0.435092</td>\n",
       "      <td>0.451739</td>\n",
       "      <td>0.436235</td>\n",
       "      <td>0.494764</td>\n",
       "      <td>0.361324</td>\n",
       "      <td>0.344176</td>\n",
       "      <td>0.395902</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>29.0</td>\n",
       "      <td>6.21</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.188918</td>\n",
       "      <td>0.184180</td>\n",
       "      <td>0.241035</td>\n",
       "      <td>0.273385</td>\n",
       "      <td>0.269901</td>\n",
       "      <td>0.291320</td>\n",
       "      <td>0.341352</td>\n",
       "      <td>0.338998</td>\n",
       "      <td>0.370679</td>\n",
       "      <td>0.257757</td>\n",
       "      <td>0.254692</td>\n",
       "      <td>0.275291</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.404309</td>\n",
       "      <td>0.393741</td>\n",
       "      <td>0.461162</td>\n",
       "      <td>0.386490</td>\n",
       "      <td>0.387376</td>\n",
       "      <td>0.411743</td>\n",
       "      <td>0.518868</td>\n",
       "      <td>0.520124</td>\n",
       "      <td>0.556754</td>\n",
       "      <td>0.358513</td>\n",
       "      <td>0.358695</td>\n",
       "      <td>0.382649</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>22.2</td>\n",
       "      <td>8.09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.492786</td>\n",
       "      <td>0.512655</td>\n",
       "      <td>0.573169</td>\n",
       "      <td>0.408834</td>\n",
       "      <td>0.438698</td>\n",
       "      <td>0.465039</td>\n",
       "      <td>0.571545</td>\n",
       "      <td>0.592584</td>\n",
       "      <td>0.630793</td>\n",
       "      <td>0.369780</td>\n",
       "      <td>0.393846</td>\n",
       "      <td>0.413376</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>41.6</td>\n",
       "      <td>7.76</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.328362</td>\n",
       "      <td>0.382958</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>0.338825</td>\n",
       "      <td>0.375302</td>\n",
       "      <td>0.401676</td>\n",
       "      <td>0.458736</td>\n",
       "      <td>0.505553</td>\n",
       "      <td>0.535566</td>\n",
       "      <td>0.318235</td>\n",
       "      <td>0.349560</td>\n",
       "      <td>0.371397</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>12.8</td>\n",
       "      <td>3.29</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.227574</td>\n",
       "      <td>0.238002</td>\n",
       "      <td>0.247942</td>\n",
       "      <td>0.273818</td>\n",
       "      <td>0.280698</td>\n",
       "      <td>0.296173</td>\n",
       "      <td>0.382336</td>\n",
       "      <td>0.385882</td>\n",
       "      <td>0.396380</td>\n",
       "      <td>0.266165</td>\n",
       "      <td>0.274243</td>\n",
       "      <td>0.287170</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>13.4</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.281904</td>\n",
       "      <td>0.276365</td>\n",
       "      <td>0.364988</td>\n",
       "      <td>0.275119</td>\n",
       "      <td>0.261059</td>\n",
       "      <td>0.330693</td>\n",
       "      <td>0.417784</td>\n",
       "      <td>0.411919</td>\n",
       "      <td>0.485940</td>\n",
       "      <td>0.264960</td>\n",
       "      <td>0.253060</td>\n",
       "      <td>0.313398</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>20.2</td>\n",
       "      <td>4.59</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.290423</td>\n",
       "      <td>0.277397</td>\n",
       "      <td>0.419258</td>\n",
       "      <td>0.330330</td>\n",
       "      <td>0.312093</td>\n",
       "      <td>0.410279</td>\n",
       "      <td>0.434970</td>\n",
       "      <td>0.423044</td>\n",
       "      <td>0.536594</td>\n",
       "      <td>0.314152</td>\n",
       "      <td>0.299274</td>\n",
       "      <td>0.380663</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>24.4</td>\n",
       "      <td>9.10</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.311381</td>\n",
       "      <td>0.362915</td>\n",
       "      <td>0.425066</td>\n",
       "      <td>0.293078</td>\n",
       "      <td>0.319590</td>\n",
       "      <td>0.336020</td>\n",
       "      <td>0.445148</td>\n",
       "      <td>0.482294</td>\n",
       "      <td>0.531889</td>\n",
       "      <td>0.283054</td>\n",
       "      <td>0.303697</td>\n",
       "      <td>0.320402</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>18.8</td>\n",
       "      <td>6.49</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.373892</td>\n",
       "      <td>0.386283</td>\n",
       "      <td>0.465458</td>\n",
       "      <td>0.340807</td>\n",
       "      <td>0.347495</td>\n",
       "      <td>0.374692</td>\n",
       "      <td>0.494702</td>\n",
       "      <td>0.504953</td>\n",
       "      <td>0.559874</td>\n",
       "      <td>0.322934</td>\n",
       "      <td>0.328859</td>\n",
       "      <td>0.351576</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.283266</td>\n",
       "      <td>0.283266</td>\n",
       "      <td>0.427136</td>\n",
       "      <td>0.303877</td>\n",
       "      <td>0.303877</td>\n",
       "      <td>0.369945</td>\n",
       "      <td>0.418918</td>\n",
       "      <td>0.418918</td>\n",
       "      <td>0.530131</td>\n",
       "      <td>0.287415</td>\n",
       "      <td>0.287415</td>\n",
       "      <td>0.343604</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.360702</td>\n",
       "      <td>0.385186</td>\n",
       "      <td>0.400871</td>\n",
       "      <td>0.333509</td>\n",
       "      <td>0.310663</td>\n",
       "      <td>0.390474</td>\n",
       "      <td>0.478968</td>\n",
       "      <td>0.488077</td>\n",
       "      <td>0.518276</td>\n",
       "      <td>0.313268</td>\n",
       "      <td>0.293675</td>\n",
       "      <td>0.362275</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>12.4</td>\n",
       "      <td>4.41</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.242958</td>\n",
       "      <td>0.201612</td>\n",
       "      <td>0.507250</td>\n",
       "      <td>0.264696</td>\n",
       "      <td>0.240809</td>\n",
       "      <td>0.398074</td>\n",
       "      <td>0.378762</td>\n",
       "      <td>0.346888</td>\n",
       "      <td>0.590100</td>\n",
       "      <td>0.251192</td>\n",
       "      <td>0.231369</td>\n",
       "      <td>0.368346</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>466.4</td>\n",
       "      <td>186.13</td>\n",
       "      <td>0.187879</td>\n",
       "      <td>0.062758</td>\n",
       "      <td>0.049637</td>\n",
       "      <td>0.429891</td>\n",
       "      <td>0.190005</td>\n",
       "      <td>0.180602</td>\n",
       "      <td>0.405218</td>\n",
       "      <td>0.243089</td>\n",
       "      <td>0.231975</td>\n",
       "      <td>0.550108</td>\n",
       "      <td>0.187930</td>\n",
       "      <td>0.180471</td>\n",
       "      <td>0.381515</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>27.8</td>\n",
       "      <td>6.88</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.361025</td>\n",
       "      <td>0.377786</td>\n",
       "      <td>0.480135</td>\n",
       "      <td>0.337188</td>\n",
       "      <td>0.308959</td>\n",
       "      <td>0.435479</td>\n",
       "      <td>0.484090</td>\n",
       "      <td>0.504825</td>\n",
       "      <td>0.579429</td>\n",
       "      <td>0.318694</td>\n",
       "      <td>0.297134</td>\n",
       "      <td>0.399701</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>48.4</td>\n",
       "      <td>6.57</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.248178</td>\n",
       "      <td>0.202775</td>\n",
       "      <td>0.480135</td>\n",
       "      <td>0.318533</td>\n",
       "      <td>0.303350</td>\n",
       "      <td>0.435479</td>\n",
       "      <td>0.400547</td>\n",
       "      <td>0.368751</td>\n",
       "      <td>0.579429</td>\n",
       "      <td>0.302659</td>\n",
       "      <td>0.291452</td>\n",
       "      <td>0.399701</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444253</td>\n",
       "      <td>0.444253</td>\n",
       "      <td>0.444253</td>\n",
       "      <td>0.393488</td>\n",
       "      <td>0.393488</td>\n",
       "      <td>0.393488</td>\n",
       "      <td>0.551413</td>\n",
       "      <td>0.551413</td>\n",
       "      <td>0.551413</td>\n",
       "      <td>0.366987</td>\n",
       "      <td>0.366987</td>\n",
       "      <td>0.366987</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214485</td>\n",
       "      <td>0.214485</td>\n",
       "      <td>0.214485</td>\n",
       "      <td>0.300848</td>\n",
       "      <td>0.300848</td>\n",
       "      <td>0.300848</td>\n",
       "      <td>0.377561</td>\n",
       "      <td>0.377561</td>\n",
       "      <td>0.377561</td>\n",
       "      <td>0.289803</td>\n",
       "      <td>0.289803</td>\n",
       "      <td>0.289803</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>27.6</td>\n",
       "      <td>10.80</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.464255</td>\n",
       "      <td>0.477141</td>\n",
       "      <td>0.558502</td>\n",
       "      <td>0.356517</td>\n",
       "      <td>0.363904</td>\n",
       "      <td>0.401534</td>\n",
       "      <td>0.562546</td>\n",
       "      <td>0.572964</td>\n",
       "      <td>0.628476</td>\n",
       "      <td>0.338703</td>\n",
       "      <td>0.344916</td>\n",
       "      <td>0.377282</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.440126</td>\n",
       "      <td>0.440126</td>\n",
       "      <td>0.550888</td>\n",
       "      <td>0.363652</td>\n",
       "      <td>0.363652</td>\n",
       "      <td>0.411871</td>\n",
       "      <td>0.548031</td>\n",
       "      <td>0.548031</td>\n",
       "      <td>0.633970</td>\n",
       "      <td>0.345064</td>\n",
       "      <td>0.345064</td>\n",
       "      <td>0.387839</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>26.2</td>\n",
       "      <td>8.16</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.276702</td>\n",
       "      <td>0.244499</td>\n",
       "      <td>0.368611</td>\n",
       "      <td>0.291235</td>\n",
       "      <td>0.286940</td>\n",
       "      <td>0.341009</td>\n",
       "      <td>0.417822</td>\n",
       "      <td>0.392729</td>\n",
       "      <td>0.492908</td>\n",
       "      <td>0.279603</td>\n",
       "      <td>0.275455</td>\n",
       "      <td>0.323080</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>15.6</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.269959</td>\n",
       "      <td>0.269959</td>\n",
       "      <td>0.384884</td>\n",
       "      <td>0.251648</td>\n",
       "      <td>0.251648</td>\n",
       "      <td>0.320422</td>\n",
       "      <td>0.404455</td>\n",
       "      <td>0.404455</td>\n",
       "      <td>0.503882</td>\n",
       "      <td>0.244367</td>\n",
       "      <td>0.244367</td>\n",
       "      <td>0.308053</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>51.4</td>\n",
       "      <td>21.00</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.475089</td>\n",
       "      <td>0.518756</td>\n",
       "      <td>0.631120</td>\n",
       "      <td>0.386631</td>\n",
       "      <td>0.412891</td>\n",
       "      <td>0.521432</td>\n",
       "      <td>0.567800</td>\n",
       "      <td>0.604626</td>\n",
       "      <td>0.681882</td>\n",
       "      <td>0.359393</td>\n",
       "      <td>0.383088</td>\n",
       "      <td>0.461404</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>19.8</td>\n",
       "      <td>8.28</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.376230</td>\n",
       "      <td>0.374740</td>\n",
       "      <td>0.525694</td>\n",
       "      <td>0.322963</td>\n",
       "      <td>0.334060</td>\n",
       "      <td>0.394663</td>\n",
       "      <td>0.488621</td>\n",
       "      <td>0.483475</td>\n",
       "      <td>0.596370</td>\n",
       "      <td>0.305448</td>\n",
       "      <td>0.314458</td>\n",
       "      <td>0.366329</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    OID_  Id  gridcode  Shape_Length  Shape_Area   CH_mean  ARVI_mean  \\\n",
       "0      1   1         1          15.8        3.36  0.000000   0.207123   \n",
       "1      2   2         2          24.0        6.93  1.000000   0.319239   \n",
       "2      3   3         3        3486.2     1942.77  0.120091   0.014902   \n",
       "3      4   4         4          31.6        9.23  1.000000   0.419381   \n",
       "4      5   5         5          31.8       11.82  1.000000   0.465333   \n",
       "5      6   6         6          61.0       15.73  1.000000   0.388299   \n",
       "6      7   7         7          11.6        2.33  0.666667   0.435153   \n",
       "7      8   8         8          55.2       36.59  0.235294   0.094267   \n",
       "8      9   9         9          33.4       10.51  0.600000   0.299233   \n",
       "9     10  10        10          64.0       32.91  0.903226   0.382419   \n",
       "10    11  11        11          38.4       10.31  0.428571   0.169491   \n",
       "11    12  12        12          12.4        4.10  0.750000   0.416340   \n",
       "12    13  13        13          33.0       12.18  0.909091   0.423449   \n",
       "13    14  14        14          16.4        5.16  0.250000   0.120560   \n",
       "14    15  15        15          15.0        2.92  0.500000   0.398133   \n",
       "15    16  16        16          25.2        7.42  0.714286   0.281256   \n",
       "16    17  17        17          11.2        3.59  0.666667   0.313890   \n",
       "17    18  18        18          33.8        7.88  0.285714   0.168590   \n",
       "18    19  19        19          22.0       11.53  0.800000   0.384646   \n",
       "19    20  20        20          19.2        4.84  1.000000   0.315022   \n",
       "20    21  21        21          36.8       15.32  0.357143   0.130060   \n",
       "21    22  22        22          10.6        2.05  0.333333   0.152072   \n",
       "22    23  23        23          21.2        5.80  0.833333   0.415082   \n",
       "23    24  24        24          16.2        3.29  0.000000   0.252461   \n",
       "24    25  25        25          17.6        4.56  0.000000   0.141727   \n",
       "25    26  26        26          19.0        3.12  0.500000   0.350948   \n",
       "26    27  27        27          19.2        3.83  0.000000   0.312604   \n",
       "27    28  28        28          29.0        6.21  0.250000   0.188918   \n",
       "28    29  29        29          24.0        5.14  1.000000   0.404309   \n",
       "29    30  30        30          22.2        8.09  1.000000   0.492786   \n",
       "30    31  31        31          41.6        7.76  0.666667   0.328362   \n",
       "31    32  32        32          12.8        3.29  1.000000   0.227574   \n",
       "32    33  33        33          13.4        2.30  0.333333   0.281904   \n",
       "33    34  34        34          20.2        4.59  0.500000   0.290423   \n",
       "34    35  35        35          24.4        9.10  0.666667   0.311381   \n",
       "35    36  36        36          18.8        6.49  0.833333   0.373892   \n",
       "36    37  37        37          13.0        2.86  1.000000   0.283266   \n",
       "37    38  38        38           9.0        2.89  1.000000   0.360702   \n",
       "38    39  39        39          12.4        4.41  0.666667   0.242958   \n",
       "39    40  40        40         466.4      186.13  0.187879   0.062758   \n",
       "40    41  41        41          27.8        6.88  0.285714   0.361025   \n",
       "41    42  42        42          48.4        6.57  0.500000   0.248178   \n",
       "42    43  43        43          10.6        2.21  0.000000   0.444253   \n",
       "43    44  44        44          15.0        2.29  0.000000   0.214485   \n",
       "44    45  45        45          27.6       10.80  0.888889   0.464255   \n",
       "45    46  46        46          28.0        4.40  1.000000   0.440126   \n",
       "46    47  47        47          26.2        8.16  0.777778   0.276702   \n",
       "47    48  48        48          15.6        2.28  0.500000   0.269959   \n",
       "48    49  49        49          51.4       21.00  0.904762   0.475089   \n",
       "49    50  50        50          19.8        8.28  1.000000   0.376230   \n",
       "\n",
       "    ARVI_med  ARVI_max  EVI_mean   EVI_med   EVI_max  NDVI_mean  NDVI_med  \\\n",
       "0   0.167813  0.335815  0.212394  0.186195  0.291992   0.350170  0.315030   \n",
       "1   0.334353  0.347863  0.275198  0.279712  0.299364   0.440041  0.453643   \n",
       "2  -0.002058  0.388165  0.155599  0.143094  0.441733   0.199209  0.184422   \n",
       "3   0.443782  0.507713  0.344480  0.337613  0.405390   0.527309  0.540501   \n",
       "4   0.440135  0.565654  0.335128  0.329047  0.380147   0.562001  0.551508   \n",
       "5   0.392301  0.558201  0.340119  0.329843  0.417976   0.506015  0.510729   \n",
       "6   0.501700  0.508019  0.350849  0.352031  0.410666   0.542913  0.591679   \n",
       "7   0.073039  0.375784  0.202749  0.197483  0.331198   0.270160  0.256369   \n",
       "8   0.296807  0.441674  0.286510  0.296106  0.360564   0.431976  0.435374   \n",
       "9   0.383198  0.510562  0.302928  0.308982  0.372048   0.496885  0.503172   \n",
       "10  0.177157  0.213876  0.249109  0.261426  0.290336   0.333254  0.342163   \n",
       "11  0.448779  0.511665  0.357268  0.363871  0.418755   0.530844  0.553736   \n",
       "12  0.434017  0.527901  0.319252  0.333124  0.359998   0.530337  0.537509   \n",
       "13  0.124836  0.177133  0.224723  0.233874  0.244178   0.287848  0.294703   \n",
       "14  0.398133  0.436209  0.316781  0.316781  0.344585   0.504631  0.504631   \n",
       "15  0.303723  0.460821  0.290412  0.288456  0.414814   0.419559  0.441908   \n",
       "16  0.310232  0.331245  0.325886  0.325597  0.371669   0.449804  0.440257   \n",
       "17  0.143379  0.295723  0.256061  0.255124  0.324244   0.334354  0.322331   \n",
       "18  0.467158  0.553504  0.331591  0.363822  0.472948   0.494661  0.565477   \n",
       "19  0.279372  0.453170  0.324405  0.289330  0.391028   0.432035  0.402736   \n",
       "20  0.107600  0.299342  0.245448  0.243397  0.378971   0.296571  0.281376   \n",
       "21  0.163493  0.219467  0.257766  0.274820  0.282380   0.318086  0.332958   \n",
       "22  0.412476  0.538134  0.400424  0.416252  0.465685   0.525296  0.529354   \n",
       "23  0.251237  0.279508  0.327898  0.326480  0.332338   0.397750  0.395355   \n",
       "24  0.068750  0.313742  0.257259  0.212822  0.364320   0.308694  0.247167   \n",
       "25  0.350948  0.388065  0.343742  0.343742  0.365579   0.470256  0.470256   \n",
       "26  0.297165  0.362959  0.394734  0.375427  0.435092   0.451739  0.436235   \n",
       "27  0.184180  0.241035  0.273385  0.269901  0.291320   0.341352  0.338998   \n",
       "28  0.393741  0.461162  0.386490  0.387376  0.411743   0.518868  0.520124   \n",
       "29  0.512655  0.573169  0.408834  0.438698  0.465039   0.571545  0.592584   \n",
       "30  0.382958  0.432454  0.338825  0.375302  0.401676   0.458736  0.505553   \n",
       "31  0.238002  0.247942  0.273818  0.280698  0.296173   0.382336  0.385882   \n",
       "32  0.276365  0.364988  0.275119  0.261059  0.330693   0.417784  0.411919   \n",
       "33  0.277397  0.419258  0.330330  0.312093  0.410279   0.434970  0.423044   \n",
       "34  0.362915  0.425066  0.293078  0.319590  0.336020   0.445148  0.482294   \n",
       "35  0.386283  0.465458  0.340807  0.347495  0.374692   0.494702  0.504953   \n",
       "36  0.283266  0.427136  0.303877  0.303877  0.369945   0.418918  0.418918   \n",
       "37  0.385186  0.400871  0.333509  0.310663  0.390474   0.478968  0.488077   \n",
       "38  0.201612  0.507250  0.264696  0.240809  0.398074   0.378762  0.346888   \n",
       "39  0.049637  0.429891  0.190005  0.180602  0.405218   0.243089  0.231975   \n",
       "40  0.377786  0.480135  0.337188  0.308959  0.435479   0.484090  0.504825   \n",
       "41  0.202775  0.480135  0.318533  0.303350  0.435479   0.400547  0.368751   \n",
       "42  0.444253  0.444253  0.393488  0.393488  0.393488   0.551413  0.551413   \n",
       "43  0.214485  0.214485  0.300848  0.300848  0.300848   0.377561  0.377561   \n",
       "44  0.477141  0.558502  0.356517  0.363904  0.401534   0.562546  0.572964   \n",
       "45  0.440126  0.550888  0.363652  0.363652  0.411871   0.548031  0.548031   \n",
       "46  0.244499  0.368611  0.291235  0.286940  0.341009   0.417822  0.392729   \n",
       "47  0.269959  0.384884  0.251648  0.251648  0.320422   0.404455  0.404455   \n",
       "48  0.518756  0.631120  0.386631  0.412891  0.521432   0.567800  0.604626   \n",
       "49  0.374740  0.525694  0.322963  0.334060  0.394663   0.488621  0.483475   \n",
       "\n",
       "    NDVI_max  SAVI_mean  SAVI_med  SAVI_max  Veg_class  \n",
       "0   0.463047   0.208236  0.183683  0.282265  non-woody  \n",
       "1   0.467307   0.263501  0.269123  0.285889      woody  \n",
       "2   0.503913   0.155182  0.144957  0.397375      woody  \n",
       "3   0.596006   0.326518  0.321309  0.381685      woody  \n",
       "4   0.639035   0.321567  0.314126  0.359091      woody  \n",
       "5   0.640907   0.323304  0.316266  0.390960      woody  \n",
       "6   0.603258   0.334208  0.337273  0.385493      woody  \n",
       "7   0.487022   0.200563  0.196467  0.316553      woody  \n",
       "8   0.545392   0.275263  0.284235  0.340146      woody  \n",
       "9   0.601409   0.291949  0.297567  0.354577      woody  \n",
       "10  0.371908   0.242211  0.253511  0.277432      woody  \n",
       "11  0.608591   0.339472  0.344724  0.393341      woody  \n",
       "12  0.609810   0.307631  0.319725  0.343846      woody  \n",
       "13  0.329086   0.217404  0.225176  0.235975      woody  \n",
       "14  0.535921   0.300922  0.300922  0.324960      woody  \n",
       "15  0.568959   0.278438  0.274564  0.386818      woody  \n",
       "16  0.469840   0.309330  0.309161  0.348700      woody  \n",
       "17  0.432587   0.248646  0.252359  0.305183      woody  \n",
       "18  0.634859   0.311251  0.343133  0.430946      woody  \n",
       "19  0.535287   0.296618  0.268295  0.350672      woody  \n",
       "20  0.436190   0.234413  0.233731  0.345058      woody  \n",
       "21  0.368765   0.247614  0.261819  0.270756      woody  \n",
       "22  0.621921   0.367831  0.382417  0.423768  non-woody  \n",
       "23  0.415763   0.304878  0.305889  0.307276      woody  \n",
       "24  0.452860   0.245734  0.206406  0.339876      woody  \n",
       "25  0.487547   0.319981  0.319981  0.340820      woody  \n",
       "26  0.494764   0.361324  0.344176  0.395902      woody  \n",
       "27  0.370679   0.257757  0.254692  0.275291      woody  \n",
       "28  0.556754   0.358513  0.358695  0.382649      woody  \n",
       "29  0.630793   0.369780  0.393846  0.413376      woody  \n",
       "30  0.535566   0.318235  0.349560  0.371397      woody  \n",
       "31  0.396380   0.266165  0.274243  0.287170      woody  \n",
       "32  0.485940   0.264960  0.253060  0.313398      woody  \n",
       "33  0.536594   0.314152  0.299274  0.380663      woody  \n",
       "34  0.531889   0.283054  0.303697  0.320402      woody  \n",
       "35  0.559874   0.322934  0.328859  0.351576      woody  \n",
       "36  0.530131   0.287415  0.287415  0.343604      woody  \n",
       "37  0.518276   0.313268  0.293675  0.362275      woody  \n",
       "38  0.590100   0.251192  0.231369  0.368346      woody  \n",
       "39  0.550108   0.187930  0.180471  0.381515      woody  \n",
       "40  0.579429   0.318694  0.297134  0.399701      woody  \n",
       "41  0.579429   0.302659  0.291452  0.399701      woody  \n",
       "42  0.551413   0.366987  0.366987  0.366987      woody  \n",
       "43  0.377561   0.289803  0.289803  0.289803  non-woody  \n",
       "44  0.628476   0.338703  0.344916  0.377282      woody  \n",
       "45  0.633970   0.345064  0.345064  0.387839      woody  \n",
       "46  0.492908   0.279603  0.275455  0.323080      woody  \n",
       "47  0.503882   0.244367  0.244367  0.308053      woody  \n",
       "48  0.681882   0.359393  0.383088  0.461404      woody  \n",
       "49  0.596370   0.305448  0.314458  0.366329      woody  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ada boost\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2021.iloc[:, 5:18], dfJornada2021.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2021.iloc[:, 5:18], dfJornada2021.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    #Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Truth.extend(Y_test.values)\n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2021.iloc[:, 5:18], dfJornada2021.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17 J -> 17 S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 5122.614422600001 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7509433962264151, 0.7849056603773585, 0.7849056603773585, 0.7433962264150943, 0.7693761814744802]\n",
      "Avg accuracy: 0.7667054249741414\n",
      "Std of accuracy : \n",
      "0.017096142052863108\n",
      "\n",
      "[[ 237  440]\n",
      " [ 178 1794]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.35      0.43       677\n",
      "           1       0.80      0.91      0.85      1972\n",
      "\n",
      "    accuracy                           0.77      2649\n",
      "   macro avg       0.69      0.63      0.64      2649\n",
      "weighted avg       0.74      0.77      0.75      2649\n",
      "\n",
      "Sensitivity: 0.90973630831643\n",
      "Specificity: 0.3500738552437223\n",
      "precision: 0.5710843373493976\n",
      "f1_score: 0.43406593406593413\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.8196357174589072\n",
      "Specificity: 0.28975095785440613\n",
      "precision: 0.5984174085064293\n",
      "f1_score: 0.3904485317844466\n",
      "Accuracy: [0.5646462318506569]\n"
     ]
    }
   ],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2017.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2017.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2017\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17 J -> 21 S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 5276.0522585 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7509433962264151, 0.7849056603773585, 0.7849056603773585, 0.7433962264150943, 0.7693761814744802]\n",
      "Avg accuracy: 0.7667054249741414\n",
      "Std of accuracy : \n",
      "0.017096142052863108\n",
      "\n",
      "[[ 237  440]\n",
      " [ 178 1794]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.35      0.43       677\n",
      "           1       0.80      0.91      0.85      1972\n",
      "\n",
      "    accuracy                           0.77      2649\n",
      "   macro avg       0.69      0.63      0.64      2649\n",
      "weighted avg       0.74      0.77      0.75      2649\n",
      "\n",
      "Sensitivity: 0.90973630831643\n",
      "Specificity: 0.3500738552437223\n",
      "precision: 0.5710843373493976\n",
      "f1_score: 0.43406593406593413\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9528418962521717\n",
      "Specificity: 0.12804878048780488\n",
      "precision: 0.5701357466063348\n",
      "f1_score: 0.2091286307053942\n",
      "Accuracy: [0.6821744205436051]\n"
     ]
    }
   ],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21 J -> 21 S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 5397.7341306 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.8251748251748252, 0.8434579439252337, 0.8271028037383178, 0.8341121495327103, 0.822429906542056]\n",
      "Avg accuracy: 0.8304555257826285\n",
      "Std of accuracy : \n",
      "0.007562683161618203\n",
      "\n",
      "[[ 482  216]\n",
      " [ 147 1296]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.69      0.73       698\n",
      "           1       0.86      0.90      0.88      1443\n",
      "\n",
      "    accuracy                           0.83      2141\n",
      "   macro avg       0.81      0.79      0.80      2141\n",
      "weighted avg       0.83      0.83      0.83      2141\n",
      "\n",
      "Sensitivity: 0.8981288981288982\n",
      "Specificity: 0.6905444126074498\n",
      "precision: 0.766295707472178\n",
      "f1_score: 0.7264506405425772\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9769173492181683\n",
      "Specificity: 0.07164634146341463\n",
      "precision: 0.6025641025641025\n",
      "f1_score: 0.12806539509536782\n",
      "Accuracy: [0.6798399199599799]\n"
     ]
    }
   ],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2021.iloc[:, 5:18], dfJornada2021.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17 S -> 21 S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 5610.7482668 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6808755760368663, 0.6877880184331797, 0.6751152073732719, 0.6993087557603687, 0.6655132641291811]\n",
      "Avg accuracy: 0.6817201643465736\n",
      "Std of accuracy : \n",
      "0.011429608933173047\n",
      "\n",
      "[[1445  643]\n",
      " [ 738 1513]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.68      2088\n",
      "           1       0.70      0.67      0.69      2251\n",
      "\n",
      "    accuracy                           0.68      4339\n",
      "   macro avg       0.68      0.68      0.68      4339\n",
      "weighted avg       0.68      0.68      0.68      4339\n",
      "\n",
      "Sensitivity: 0.6721457130164371\n",
      "Specificity: 0.6920498084291188\n",
      "precision: 0.6619331195602383\n",
      "f1_score: 0.6766565207211426\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9133780094316207\n",
      "Specificity: 0.44308943089430897\n",
      "precision: 0.7141687141687142\n",
      "f1_score: 0.5468798996550643\n",
      "Accuracy: [0.7590461897615475]\n"
     ]
    }
   ],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], dfJornada2021.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
