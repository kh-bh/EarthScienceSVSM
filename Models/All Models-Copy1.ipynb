{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "---Run time is 0.0003375999999946089 seconds ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 70 #display 70 dpi in Jupyter Notebook, may consider100 dpi \n",
    "plt.rcParams['savefig.dpi'] = 300 #define 300 dpi for saving figures\n",
    "\n",
    "import seaborn as sns\n",
    "## here are some settings \n",
    "sns.set_style('whitegrid')\n",
    "sns.set(rc={\"figure.dpi\":70, 'savefig.dpi':300}) #defining dpi setting\n",
    "sns.set_context('notebook')\n",
    "sns.set_style(\"ticks\")\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "# Tells matplotlib to display images inline instead of a new window\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "random.seed(1000)\n",
    "\n",
    "from time import time\n",
    "import timeit #imports timeit module\n",
    "start_time = timeit.default_timer() #defines start time so computational time can be calculated\n",
    "print(\"Hello World\")\n",
    "elapsed = timeit.default_timer() - start_time #gives total computation time\n",
    "print(\"---Run time is %s seconds ---\" % elapsed) #prints computation time\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import neighbors\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OID_</th>\n",
       "      <th>Id</th>\n",
       "      <th>gridcode</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>CH_mean</th>\n",
       "      <th>ARVI_max</th>\n",
       "      <th>ARVI_mean</th>\n",
       "      <th>ARVI_med</th>\n",
       "      <th>EVI_max</th>\n",
       "      <th>EVI_mean</th>\n",
       "      <th>EVI_med</th>\n",
       "      <th>NDVI_max</th>\n",
       "      <th>NDVI_mean</th>\n",
       "      <th>NDVI_med</th>\n",
       "      <th>SAVI_max</th>\n",
       "      <th>SAVI_mean</th>\n",
       "      <th>SAVI_med</th>\n",
       "      <th>Year</th>\n",
       "      <th>Veg_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.79</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.239161</td>\n",
       "      <td>0.223962</td>\n",
       "      <td>0.226896</td>\n",
       "      <td>0.307885</td>\n",
       "      <td>0.273236</td>\n",
       "      <td>0.274394</td>\n",
       "      <td>0.427636</td>\n",
       "      <td>0.402131</td>\n",
       "      <td>0.406066</td>\n",
       "      <td>0.316995</td>\n",
       "      <td>0.278782</td>\n",
       "      <td>0.280285</td>\n",
       "      <td>2017</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.316384</td>\n",
       "      <td>0.287822</td>\n",
       "      <td>0.287822</td>\n",
       "      <td>0.305906</td>\n",
       "      <td>0.302193</td>\n",
       "      <td>0.302193</td>\n",
       "      <td>0.469259</td>\n",
       "      <td>0.448325</td>\n",
       "      <td>0.448325</td>\n",
       "      <td>0.304224</td>\n",
       "      <td>0.301407</td>\n",
       "      <td>0.301407</td>\n",
       "      <td>2017</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>27.8</td>\n",
       "      <td>11.12</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.459671</td>\n",
       "      <td>0.289236</td>\n",
       "      <td>0.313074</td>\n",
       "      <td>0.374641</td>\n",
       "      <td>0.300200</td>\n",
       "      <td>0.318621</td>\n",
       "      <td>0.570786</td>\n",
       "      <td>0.447470</td>\n",
       "      <td>0.466219</td>\n",
       "      <td>0.358302</td>\n",
       "      <td>0.298798</td>\n",
       "      <td>0.314429</td>\n",
       "      <td>2017</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>24.2</td>\n",
       "      <td>10.62</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.498250</td>\n",
       "      <td>0.330255</td>\n",
       "      <td>0.375986</td>\n",
       "      <td>0.386369</td>\n",
       "      <td>0.313174</td>\n",
       "      <td>0.335601</td>\n",
       "      <td>0.598966</td>\n",
       "      <td>0.477596</td>\n",
       "      <td>0.517136</td>\n",
       "      <td>0.368766</td>\n",
       "      <td>0.309534</td>\n",
       "      <td>0.325886</td>\n",
       "      <td>2017</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>17.4</td>\n",
       "      <td>6.10</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.416294</td>\n",
       "      <td>0.275800</td>\n",
       "      <td>0.341526</td>\n",
       "      <td>0.314841</td>\n",
       "      <td>0.264112</td>\n",
       "      <td>0.280268</td>\n",
       "      <td>0.518614</td>\n",
       "      <td>0.431575</td>\n",
       "      <td>0.487791</td>\n",
       "      <td>0.312146</td>\n",
       "      <td>0.266352</td>\n",
       "      <td>0.273629</td>\n",
       "      <td>2017</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226</th>\n",
       "      <td>2278</td>\n",
       "      <td>2278</td>\n",
       "      <td>2278</td>\n",
       "      <td>25.2</td>\n",
       "      <td>6.63</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.167685</td>\n",
       "      <td>0.128259</td>\n",
       "      <td>0.134121</td>\n",
       "      <td>0.256763</td>\n",
       "      <td>0.237659</td>\n",
       "      <td>0.245145</td>\n",
       "      <td>0.365527</td>\n",
       "      <td>0.334798</td>\n",
       "      <td>0.337580</td>\n",
       "      <td>0.268680</td>\n",
       "      <td>0.252164</td>\n",
       "      <td>0.257355</td>\n",
       "      <td>2021</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>2279</td>\n",
       "      <td>2279</td>\n",
       "      <td>2279</td>\n",
       "      <td>15.2</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.519026</td>\n",
       "      <td>0.485618</td>\n",
       "      <td>0.489819</td>\n",
       "      <td>0.441772</td>\n",
       "      <td>0.416102</td>\n",
       "      <td>0.438885</td>\n",
       "      <td>0.603520</td>\n",
       "      <td>0.583201</td>\n",
       "      <td>0.585438</td>\n",
       "      <td>0.410255</td>\n",
       "      <td>0.386171</td>\n",
       "      <td>0.403764</td>\n",
       "      <td>2021</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2228</th>\n",
       "      <td>2280</td>\n",
       "      <td>2280</td>\n",
       "      <td>2280</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>0.474006</td>\n",
       "      <td>0.313633</td>\n",
       "      <td>0.318283</td>\n",
       "      <td>0.436805</td>\n",
       "      <td>0.350407</td>\n",
       "      <td>0.341923</td>\n",
       "      <td>0.591361</td>\n",
       "      <td>0.483337</td>\n",
       "      <td>0.490012</td>\n",
       "      <td>0.412744</td>\n",
       "      <td>0.350956</td>\n",
       "      <td>0.346441</td>\n",
       "      <td>2021</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2229</th>\n",
       "      <td>2281</td>\n",
       "      <td>2281</td>\n",
       "      <td>2281</td>\n",
       "      <td>19.8</td>\n",
       "      <td>6.49</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.198284</td>\n",
       "      <td>0.119920</td>\n",
       "      <td>0.108993</td>\n",
       "      <td>0.271254</td>\n",
       "      <td>0.230958</td>\n",
       "      <td>0.227932</td>\n",
       "      <td>0.384514</td>\n",
       "      <td>0.323523</td>\n",
       "      <td>0.317326</td>\n",
       "      <td>0.278123</td>\n",
       "      <td>0.243325</td>\n",
       "      <td>0.242532</td>\n",
       "      <td>2021</td>\n",
       "      <td>non-woody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2230</th>\n",
       "      <td>2282</td>\n",
       "      <td>2282</td>\n",
       "      <td>2282</td>\n",
       "      <td>14.8</td>\n",
       "      <td>4.36</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.415063</td>\n",
       "      <td>0.358501</td>\n",
       "      <td>0.379463</td>\n",
       "      <td>0.365760</td>\n",
       "      <td>0.324313</td>\n",
       "      <td>0.338486</td>\n",
       "      <td>0.529712</td>\n",
       "      <td>0.493172</td>\n",
       "      <td>0.516169</td>\n",
       "      <td>0.349468</td>\n",
       "      <td>0.315355</td>\n",
       "      <td>0.326305</td>\n",
       "      <td>2021</td>\n",
       "      <td>woody</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4790 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      OID_    Id  gridcode  Shape_Length  Shape_Area   CH_mean  ARVI_max  \\\n",
       "0        2     2         2          16.0        4.79  0.010000  0.239161   \n",
       "1        3     3         3           8.6        2.24  0.010000  0.316384   \n",
       "2        4     4         4          27.8       11.12  0.012500  0.459671   \n",
       "3        5     5         5          24.2       10.62  0.060000  0.498250   \n",
       "4        6     6         6          17.4        6.10  0.012000  0.416294   \n",
       "...    ...   ...       ...           ...         ...       ...       ...   \n",
       "2226  2278  2278      2278          25.2        6.63  0.015000  0.167685   \n",
       "2227  2279  2279      2279          15.2        5.33  0.134000  0.519026   \n",
       "2228  2280  2280      2280          17.0        4.98  0.114000  0.474006   \n",
       "2229  2281  2281      2281          19.8        6.49  0.011667  0.198284   \n",
       "2230  2282  2282      2282          14.8        4.36  0.018000  0.415063   \n",
       "\n",
       "      ARVI_mean  ARVI_med   EVI_max  EVI_mean   EVI_med  NDVI_max  NDVI_mean  \\\n",
       "0      0.223962  0.226896  0.307885  0.273236  0.274394  0.427636   0.402131   \n",
       "1      0.287822  0.287822  0.305906  0.302193  0.302193  0.469259   0.448325   \n",
       "2      0.289236  0.313074  0.374641  0.300200  0.318621  0.570786   0.447470   \n",
       "3      0.330255  0.375986  0.386369  0.313174  0.335601  0.598966   0.477596   \n",
       "4      0.275800  0.341526  0.314841  0.264112  0.280268  0.518614   0.431575   \n",
       "...         ...       ...       ...       ...       ...       ...        ...   \n",
       "2226   0.128259  0.134121  0.256763  0.237659  0.245145  0.365527   0.334798   \n",
       "2227   0.485618  0.489819  0.441772  0.416102  0.438885  0.603520   0.583201   \n",
       "2228   0.313633  0.318283  0.436805  0.350407  0.341923  0.591361   0.483337   \n",
       "2229   0.119920  0.108993  0.271254  0.230958  0.227932  0.384514   0.323523   \n",
       "2230   0.358501  0.379463  0.365760  0.324313  0.338486  0.529712   0.493172   \n",
       "\n",
       "      NDVI_med  SAVI_max  SAVI_mean  SAVI_med  Year  Veg_class  \n",
       "0     0.406066  0.316995   0.278782  0.280285  2017      woody  \n",
       "1     0.448325  0.304224   0.301407  0.301407  2017      woody  \n",
       "2     0.466219  0.358302   0.298798  0.314429  2017      woody  \n",
       "3     0.517136  0.368766   0.309534  0.325886  2017      woody  \n",
       "4     0.487791  0.312146   0.266352  0.273629  2017      woody  \n",
       "...        ...       ...        ...       ...   ...        ...  \n",
       "2226  0.337580  0.268680   0.252164  0.257355  2021  non-woody  \n",
       "2227  0.585438  0.410255   0.386171  0.403764  2021      woody  \n",
       "2228  0.490012  0.412744   0.350956  0.346441  2021  non-woody  \n",
       "2229  0.317326  0.278123   0.243325  0.242532  2021  non-woody  \n",
       "2230  0.516169  0.349468   0.315355  0.326305  2021      woody  \n",
       "\n",
       "[4790 rows x 20 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2017 = pd.read_csv('SRER_2017_training_bi.csv', na_values = '?').dropna()\n",
    "df2021 = pd.read_csv(\"SRER21_dataset_v1.csv\", na_values = '?').dropna()\n",
    "dfJornada2017 = pd.read_csv(\"JORN17_dataset_v1.csv\", na_values = '?').dropna()\n",
    "dfJornada2021 = pd.read_csv(\"JORN21_dataset_v1.csv\", na_values = '?').dropna()\n",
    "dfJornada2017[\"Year\"] = '2017'\n",
    "dfJornada2021[\"Year\"] = '2021'\n",
    "\n",
    "frames = [dfJornada2017,dfJornada2021]\n",
    "\n",
    "dfCombined = pd.concat(frames)\n",
    "dfCombined = dfCombined.reindex(columns=['OID_','Id', 'gridcode','Shape_Length','Shape_Area', 'CH_mean', 'ARVI_max', 'ARVI_mean', 'ARVI_med', 'EVI_max', 'EVI_mean', 'EVI_med', 'NDVI_max', 'NDVI_mean', 'NDVI_med', 'SAVI_max', 'SAVI_mean', 'SAVI_med', 'Year', 'Veg_class'])\n",
    "\n",
    "dfCombined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2017.Veg_class = df2017.Veg_class.map({'non-woody':0, 'woody':1})\n",
    "df2021.Veg_class = df2021.Veg_class.map({'non-woody':0, 'woody':1})\n",
    "dfJornada2017.Veg_class = dfJornada2017.Veg_class.map({'non-woody':0, 'woody':1})\n",
    "dfJornada2021.Veg_class = dfJornada2021.Veg_class.map({'non-woody':0, 'woody':1})\n",
    "dfCombined.Veg_class = dfCombined.Veg_class.map({'non-woody':0, 'woody':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2649, 19)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfJornada2017.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2017 Jorn (Training) to 2021 Jorn (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1007.5522134999999 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7016317016317016, 0.7266355140186916, 0.6892523364485982, 0.6845794392523364, 0.7009345794392523]\n",
      "Avg accuracy: 0.700606714158116\n",
      "Std of accuracy : \n",
      "0.014590763867743942\n",
      "\n",
      "[[ 158  540]\n",
      " [ 101 1342]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.23      0.33       698\n",
      "           1       0.71      0.93      0.81      1443\n",
      "\n",
      "    accuracy                           0.70      2141\n",
      "   macro avg       0.66      0.58      0.57      2141\n",
      "weighted avg       0.68      0.70      0.65      2141\n",
      "\n",
      "Sensitivity: 0.22636103151862463\n",
      "Specificity: 0.93000693000693\n",
      "Precision: 0.61003861003861\n",
      "F1_Score: 0.33019853709508884\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 830.5387425 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7505827505827506, 0.7079439252336449, 0.7406542056074766, 0.7219626168224299, 0.7406542056074766]\n",
      "Avg accuracy: 0.7323595407707557\n",
      "Std of accuracy : \n",
      "0.015322575599437144\n",
      "\n",
      "[[ 182  516]\n",
      " [  57 1386]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.26      0.39       698\n",
      "           1       0.73      0.96      0.83      1443\n",
      "\n",
      "    accuracy                           0.73      2141\n",
      "   macro avg       0.75      0.61      0.61      2141\n",
      "weighted avg       0.74      0.73      0.69      2141\n",
      "\n",
      "Sensitivity: 0.2607449856733524\n",
      "Specificity: 0.9604989604989606\n",
      "Precision: 0.7615062761506276\n",
      "F1_Score: 0.38847385272145135\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 617.0945138 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7435897435897436, 0.6752336448598131, 0.7219626168224299, 0.719626168224299, 0.6892523364485982]\n",
      "Avg accuracy: 0.7099329019889768\n",
      "Std of accuracy : \n",
      "0.024507879027805986\n",
      "\n",
      "[[  90  608]\n",
      " [  13 1430]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.13      0.22       698\n",
      "           1       0.70      0.99      0.82      1443\n",
      "\n",
      "    accuracy                           0.71      2141\n",
      "   macro avg       0.79      0.56      0.52      2141\n",
      "weighted avg       0.76      0.71      0.63      2141\n",
      "\n",
      "Sensitivity: 0.12893982808022922\n",
      "Specificity: 0.990990990990991\n",
      "Precision: 0.8737864077669902\n",
      "F1_Score: 0.22471910112359547\n"
     ]
    }
   ],
   "source": [
    "#Ada boost\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 397.25527950000003 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7435897435897436, 0.7102803738317757, 0.7429906542056075, 0.7149532710280374, 0.7383177570093458]\n",
      "Avg accuracy: 0.730026359932902\n",
      "Std of accuracy : \n",
      "0.014407550408942316\n",
      "\n",
      "[[ 192  506]\n",
      " [  72 1371]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.28      0.40       698\n",
      "           1       0.73      0.95      0.83      1443\n",
      "\n",
      "    accuracy                           0.73      2141\n",
      "   macro avg       0.73      0.61      0.61      2141\n",
      "weighted avg       0.73      0.73      0.69      2141\n",
      "\n",
      "Sensitivity: 0.27507163323782235\n",
      "Specificity: 0.9501039501039501\n",
      "Precision: 0.7272727272727273\n",
      "F1_Score: 0.39916839916839914\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2021 Jorn (Training) to 2017 Jorn (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1852.6496797 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6503496503496503, 0.7313084112149533, 0.7126168224299065, 0.7219626168224299, 0.6612149532710281]\n",
      "Avg accuracy: 0.6954904908175936\n",
      "Std of accuracy : \n",
      "0.03313462256782401\n",
      "\n",
      "[[ 322  141]\n",
      " [ 511 1167]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.70      0.50       463\n",
      "           1       0.89      0.70      0.78      1678\n",
      "\n",
      "    accuracy                           0.70      2141\n",
      "   macro avg       0.64      0.70      0.64      2141\n",
      "weighted avg       0.78      0.70      0.72      2141\n",
      "\n",
      "Sensitivity: 0.6954643628509719\n",
      "Specificity: 0.6954707985697258\n",
      "Precision: 0.3865546218487395\n",
      "F1_Score: 0.4969135802469136\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1502.5698974 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.668997668997669, 0.7009345794392523, 0.7242990654205608, 0.7266355140186916, 0.7126168224299065]\n",
      "Avg accuracy: 0.706696730061216\n",
      "Std of accuracy : \n",
      "0.02096123276108921\n",
      "\n",
      "[[ 317  146]\n",
      " [ 482 1196]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.68      0.50       463\n",
      "           1       0.89      0.71      0.79      1678\n",
      "\n",
      "    accuracy                           0.71      2141\n",
      "   macro avg       0.64      0.70      0.65      2141\n",
      "weighted avg       0.78      0.71      0.73      2141\n",
      "\n",
      "Sensitivity: 0.6846652267818575\n",
      "Specificity: 0.7127532777115614\n",
      "Precision: 0.3967459324155194\n",
      "F1_Score: 0.502377179080824\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1412.8882977 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6643356643356644, 0.6682242990654206, 0.7219626168224299, 0.7313084112149533, 0.6939252336448598]\n",
      "Avg accuracy: 0.6959512450166656\n",
      "Std of accuracy : \n",
      "0.02719983036877213\n",
      "\n",
      "[[ 310  153]\n",
      " [ 498 1180]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.67      0.49       463\n",
      "           1       0.89      0.70      0.78      1678\n",
      "\n",
      "    accuracy                           0.70      2141\n",
      "   macro avg       0.63      0.69      0.64      2141\n",
      "weighted avg       0.78      0.70      0.72      2141\n",
      "\n",
      "Sensitivity: 0.6695464362850972\n",
      "Specificity: 0.7032181168057211\n",
      "Precision: 0.38366336633663367\n",
      "F1_Score: 0.48780487804878053\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1272.9465154 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7435897435897436, 0.7102803738317757, 0.7429906542056075, 0.7149532710280374, 0.7383177570093458]\n",
      "Avg accuracy: 0.730026359932902\n",
      "Std of accuracy : \n",
      "0.014407550408942316\n",
      "\n",
      "[[ 192  506]\n",
      " [  72 1371]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.28      0.40       698\n",
      "           1       0.73      0.95      0.83      1443\n",
      "\n",
      "    accuracy                           0.73      2141\n",
      "   macro avg       0.73      0.61      0.61      2141\n",
      "weighted avg       0.73      0.73      0.69      2141\n",
      "\n",
      "Sensitivity: 0.27507163323782235\n",
      "Specificity: 0.9501039501039501\n",
      "Precision: 0.7272727272727273\n",
      "F1_Score: 0.39916839916839914\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2017 SRER (Training) to 2021 Jorn (Testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 3984.7700815000003 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7202797202797203, 0.7289719626168224, 0.7827102803738317, 0.705607476635514, 0.6448598130841121]\n",
      "Avg accuracy: 0.7164858505980002\n",
      "Std of accuracy : \n",
      "0.04428227593439862\n",
      "\n",
      "[[ 353  345]\n",
      " [ 262 1181]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.51      0.54       698\n",
      "           1       0.77      0.82      0.80      1443\n",
      "\n",
      "    accuracy                           0.72      2141\n",
      "   macro avg       0.67      0.66      0.67      2141\n",
      "weighted avg       0.71      0.72      0.71      2141\n",
      "\n",
      "Sensitivity: 0.505730659025788\n",
      "Specificity: 0.8184338184338185\n",
      "Precision: 0.5739837398373984\n",
      "F1_Score: 0.5376999238385377\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6) #criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 4012.5537627000003 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.662004662004662, 0.6425233644859814, 0.6892523364485982, 0.6565420560747663, 0.633177570093458]\n",
      "Avg accuracy: 0.6566999978214931\n",
      "Std of accuracy : \n",
      "0.019194273608216853\n",
      "\n",
      "[[ 404  294]\n",
      " [ 441 1002]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.58      0.52       698\n",
      "           1       0.77      0.69      0.73      1443\n",
      "\n",
      "    accuracy                           0.66      2141\n",
      "   macro avg       0.63      0.64      0.63      2141\n",
      "weighted avg       0.68      0.66      0.66      2141\n",
      "\n",
      "Sensitivity: 0.5787965616045845\n",
      "Specificity: 0.6943866943866944\n",
      "Precision: 0.47810650887573963\n",
      "F1_Score: 0.5236552171095268\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 4048.4801246 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6899766899766899, 0.6985981308411215, 0.6915887850467289, 0.647196261682243, 0.6495327102803738]\n",
      "Avg accuracy: 0.6753785155654315\n",
      "Std of accuracy : \n",
      "0.02225881915609393\n",
      "\n",
      "[[ 132  566]\n",
      " [ 129 1314]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.19      0.28       698\n",
      "           1       0.70      0.91      0.79      1443\n",
      "\n",
      "    accuracy                           0.68      2141\n",
      "   macro avg       0.60      0.55      0.53      2141\n",
      "weighted avg       0.64      0.68      0.62      2141\n",
      "\n",
      "Sensitivity: 0.18911174785100288\n",
      "Specificity: 0.9106029106029107\n",
      "Precision: 0.5057471264367817\n",
      "F1_Score: 0.27528675703858185\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 4102.1882469 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.627039627039627, 0.633177570093458, 0.6098130841121495, 0.6588785046728972, 0.6682242990654206]\n",
      "Avg accuracy: 0.6394266169967106\n",
      "Std of accuracy : \n",
      "0.02134072099999624\n",
      "\n",
      "[[472 226]\n",
      " [546 897]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.68      0.55       698\n",
      "           1       0.80      0.62      0.70      1443\n",
      "\n",
      "    accuracy                           0.64      2141\n",
      "   macro avg       0.63      0.65      0.62      2141\n",
      "weighted avg       0.69      0.64      0.65      2141\n",
      "\n",
      "Sensitivity: 0.6762177650429799\n",
      "Specificity: 0.6216216216216216\n",
      "Precision: 0.4636542239685658\n",
      "F1_Score: 0.55011655011655\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2021 SRER (Training) to 2021 Jorn (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 5150.1953007 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.4382284382284382, 0.5630841121495327, 0.5537383177570093, 0.5093457943925234, 0.5280373831775701]\n",
      "Avg accuracy: 0.5184868091410147\n",
      "Std of accuracy : \n",
      "0.04437964851581787\n",
      "\n",
      "[[635  63]\n",
      " [968 475]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.91      0.55       698\n",
      "           1       0.88      0.33      0.48      1443\n",
      "\n",
      "    accuracy                           0.52      2141\n",
      "   macro avg       0.64      0.62      0.52      2141\n",
      "weighted avg       0.72      0.52      0.50      2141\n",
      "\n",
      "Sensitivity: 0.9097421203438395\n",
      "Specificity: 0.32917532917532916\n",
      "Precision: 0.39613225202744856\n",
      "F1_Score: 0.5519339417644503\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6) #criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 5158.297985900001 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.5431235431235432, 0.5210280373831776, 0.544392523364486, 0.544392523364486, 0.5163551401869159]\n",
      "Avg accuracy: 0.5338583534845217\n",
      "Std of accuracy : \n",
      "0.012480069680645366\n",
      "\n",
      "[[520 178]\n",
      " [820 623]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.74      0.51       698\n",
      "           1       0.78      0.43      0.56      1443\n",
      "\n",
      "    accuracy                           0.53      2141\n",
      "   macro avg       0.58      0.59      0.53      2141\n",
      "weighted avg       0.65      0.53      0.54      2141\n",
      "\n",
      "Sensitivity: 0.7449856733524355\n",
      "Specificity: 0.43173943173943174\n",
      "Precision: 0.3880597014925373\n",
      "F1_Score: 0.5103042198233562\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 5177.1753029 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.5361305361305362, 0.5584112149532711, 0.5233644859813084, 0.5093457943925234, 0.5397196261682243]\n",
      "Avg accuracy: 0.5333943315251727\n",
      "Std of accuracy : \n",
      "0.016445489133942365\n",
      "\n",
      "[[670  28]\n",
      " [971 472]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.96      0.57       698\n",
      "           1       0.94      0.33      0.49      1443\n",
      "\n",
      "    accuracy                           0.53      2141\n",
      "   macro avg       0.68      0.64      0.53      2141\n",
      "weighted avg       0.77      0.53      0.51      2141\n",
      "\n",
      "Sensitivity: 0.9598853868194842\n",
      "Specificity: 0.3270963270963271\n",
      "Precision: 0.4082876294942108\n",
      "F1_Score: 0.572894399315947\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 5226.7170587 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.5081585081585082, 0.5, 0.5116822429906542, 0.5186915887850467, 0.4976635514018692]\n",
      "Avg accuracy: 0.5072391782672157\n",
      "Std of accuracy : \n",
      "0.007692100234701685\n",
      "\n",
      "[[540 158]\n",
      " [897 546]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.77      0.51       698\n",
      "           1       0.78      0.38      0.51      1443\n",
      "\n",
      "    accuracy                           0.51      2141\n",
      "   macro avg       0.58      0.58      0.51      2141\n",
      "weighted avg       0.65      0.51      0.51      2141\n",
      "\n",
      "Sensitivity: 0.7736389684813754\n",
      "Specificity: 0.3783783783783784\n",
      "Precision: 0.3757828810020877\n",
      "F1_Score: 0.5058548009367682\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2017 SRER TO 2017 JORN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 3416.8259887000004 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.4377358490566038, 0.7396226415094339, 0.690566037735849, 0.6320754716981132, 0.6937618147448015]\n",
      "Avg accuracy: 0.6387523629489603\n",
      "Std of accuracy : \n",
      "0.10614869713084527\n",
      "\n",
      "[[ 312  365]\n",
      " [ 592 1380]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.46      0.39       677\n",
      "           1       0.79      0.70      0.74      1972\n",
      "\n",
      "    accuracy                           0.64      2649\n",
      "   macro avg       0.57      0.58      0.57      2649\n",
      "weighted avg       0.68      0.64      0.65      2649\n",
      "\n",
      "Sensitivity: 0.4608567208271787\n",
      "Specificity: 0.6997971602434077\n",
      "Precision: 0.34513274336283184\n",
      "F1_Score: 0.3946869070208729\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 3525.7383508000003 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6547169811320754, 0.7, 0.6811320754716981, 0.720754716981132, 0.720226843100189]\n",
      "Avg accuracy: 0.695366123337019\n",
      "Std of accuracy : \n",
      "0.02505621586723127\n",
      "\n",
      "[[ 419  258]\n",
      " [ 549 1423]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.62      0.51       677\n",
      "           1       0.85      0.72      0.78      1972\n",
      "\n",
      "    accuracy                           0.70      2649\n",
      "   macro avg       0.64      0.67      0.64      2649\n",
      "weighted avg       0.74      0.70      0.71      2649\n",
      "\n",
      "Sensitivity: 0.6189069423929099\n",
      "Specificity: 0.7216024340770791\n",
      "Precision: 0.43285123966942146\n",
      "F1_Score: 0.5094224924012157\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 3520.0201810000003 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6773584905660377, 0.7169811320754716, 0.7018867924528301, 0.7037735849056603, 0.7391304347826086]\n",
      "Avg accuracy: 0.7078260869565216\n",
      "Std of accuracy : \n",
      "0.020217563977066547\n",
      "\n",
      "[[ 126  551]\n",
      " [ 223 1749]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.19      0.25       677\n",
      "           1       0.76      0.89      0.82      1972\n",
      "\n",
      "    accuracy                           0.71      2649\n",
      "   macro avg       0.56      0.54      0.53      2649\n",
      "weighted avg       0.66      0.71      0.67      2649\n",
      "\n",
      "Sensitivity: 0.1861152141802068\n",
      "Specificity: 0.8869168356997972\n",
      "Precision: 0.36103151862464183\n",
      "F1_Score: 0.2456140350877193\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 3485.9874314000003 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.4377358490566038, 0.44528301886792454, 0.39622641509433965, 0.439622641509434, 0.40831758034026466]\n",
      "Avg accuracy: 0.4254371009737133\n",
      "Std of accuracy : \n",
      "0.019456060125437852\n",
      "\n",
      "[[ 521  156]\n",
      " [1366  606]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.77      0.41       677\n",
      "           1       0.80      0.31      0.44      1972\n",
      "\n",
      "    accuracy                           0.43      2649\n",
      "   macro avg       0.54      0.54      0.42      2649\n",
      "weighted avg       0.66      0.43      0.43      2649\n",
      "\n",
      "Sensitivity: 0.7695716395864106\n",
      "Specificity: 0.30730223123732253\n",
      "Precision: 0.27609962904080554\n",
      "F1_Score: 0.4063962558502341\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 212.57865160000006 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7860125260960334, 0.7640918580375783, 0.7954070981210856, 0.7860125260960334, 0.7682672233820459]\n",
      "Avg accuracy: 0.7799582463465553\n",
      "Std of accuracy : \n",
      "0.011835522157042486\n",
      "\n",
      "[[ 735  640]\n",
      " [ 414 3001]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.64      0.53      0.58      1375\n",
      "       woody       0.82      0.88      0.85      3415\n",
      "\n",
      "    accuracy                           0.78      4790\n",
      "   macro avg       0.73      0.71      0.72      4790\n",
      "weighted avg       0.77      0.78      0.77      4790\n",
      "\n",
      "Sensitivity: 0.5345454545454545\n",
      "Specificity: 0.8787701317715959\n",
      "Precision: 0.639686684073107\n",
      "F1_Score: 0.5824088748019018\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfCombined): \n",
    "    \n",
    "    X_train = dfCombined.iloc[train_index, 5:19]\n",
    "    X_test = dfCombined.iloc[test_index, 5:19]\n",
    "    Y_train = dfCombined.iloc[train_index, -1]\n",
    "    Y_test = dfCombined.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1233.5199241 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.8037578288100209, 0.791231732776618, 0.7901878914405011, 0.778705636743215, 0.7755741127348643]\n",
      "Avg accuracy: 0.7878914405010439\n",
      "Std of accuracy : \n",
      "0.010042599975620623\n",
      "\n",
      "[[ 739  636]\n",
      " [ 380 3035]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.66      0.54      0.59      1375\n",
      "       woody       0.83      0.89      0.86      3415\n",
      "\n",
      "    accuracy                           0.79      4790\n",
      "   macro avg       0.74      0.71      0.72      4790\n",
      "weighted avg       0.78      0.79      0.78      4790\n",
      "\n",
      "Sensitivity: 0.5374545454545454\n",
      "Specificity: 0.8887262079062958\n",
      "Precision: 0.6604110813226095\n",
      "F1_Score: 0.5926222935044106\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfCombined): \n",
    "    \n",
    "    X_train = dfCombined.iloc[train_index, 5:19]\n",
    "    X_test = dfCombined.iloc[test_index, 5:19]\n",
    "    Y_train = dfCombined.iloc[train_index, -1]\n",
    "    Y_test = dfCombined.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1120.7830826999998 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7964509394572025, 0.8037578288100209, 0.8131524008350731, 0.7797494780793319, 0.7818371607515657]\n",
      "Avg accuracy: 0.7949895615866389\n",
      "Std of accuracy : \n",
      "0.01276050671393416\n",
      "\n",
      "[[ 707  668]\n",
      " [ 314 3101]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.69      0.51      0.59      1375\n",
      "       woody       0.82      0.91      0.86      3415\n",
      "\n",
      "    accuracy                           0.79      4790\n",
      "   macro avg       0.76      0.71      0.73      4790\n",
      "weighted avg       0.79      0.79      0.78      4790\n",
      "\n",
      "Sensitivity: 0.5141818181818182\n",
      "Specificity: 0.9080527086383602\n",
      "Precision: 0.692458374142997\n",
      "F1_Score: 0.5901502504173622\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfCombined): \n",
    "    \n",
    "    X_train = dfCombined.iloc[train_index, 5:19]\n",
    "    X_test = dfCombined.iloc[test_index, 5:19]\n",
    "    Y_train = dfCombined.iloc[train_index, -1]\n",
    "    Y_test = dfCombined.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1479.9852984 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.802713987473904, 0.791231732776618, 0.7964509394572025, 0.7933194154488518, 0.7901878914405011]\n",
      "Avg accuracy: 0.7947807933194154\n",
      "Std of accuracy : \n",
      "0.004506687504159273\n",
      "\n",
      "[[ 774  601]\n",
      " [ 382 3033]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.67      0.56      0.61      1375\n",
      "       woody       0.83      0.89      0.86      3415\n",
      "\n",
      "    accuracy                           0.79      4790\n",
      "   macro avg       0.75      0.73      0.74      4790\n",
      "weighted avg       0.79      0.79      0.79      4790\n",
      "\n",
      "Sensitivity: 0.5629090909090909\n",
      "Specificity: 0.8881405563689605\n",
      "Precision: 0.6695501730103807\n",
      "F1_Score: 0.611615962070328\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfCombined): \n",
    "    \n",
    "    X_train = dfCombined.iloc[train_index, 5:19]\n",
    "    X_test = dfCombined.iloc[test_index, 5:19]\n",
    "    Y_train = dfCombined.iloc[train_index, -1]\n",
    "    Y_test = dfCombined.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 12.276497200000023 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6889400921658986, 0.7108294930875576, 0.6785714285714286, 0.7131336405529954, 0.7231833910034602]\n",
      "Avg accuracy: 0.7029316090762681\n",
      "Std of accuracy : \n",
      "0.016527393194427125\n",
      "\n",
      "[[ 312 1147]\n",
      " [ 142 2738]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.21      0.33      1459\n",
      "           1       0.70      0.95      0.81      2880\n",
      "\n",
      "    accuracy                           0.70      4339\n",
      "   macro avg       0.70      0.58      0.57      4339\n",
      "weighted avg       0.70      0.70      0.65      4339\n",
      "\n",
      "0.21384509938313914\n",
      "0.9506944444444444\n",
      "0.6872246696035242\n",
      "0.3261892315734448\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 101.29513110000002 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7684331797235023, 0.7695852534562212, 0.7707373271889401, 0.7557603686635944, 0.7681660899653979]\n",
      "Avg accuracy: 0.7665364437995311\n",
      "Std of accuracy : \n",
      "0.005464792964069163\n",
      "\n",
      "[[ 720  739]\n",
      " [ 274 2606]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.49      0.59      1459\n",
      "           1       0.78      0.90      0.84      2880\n",
      "\n",
      "    accuracy                           0.77      4339\n",
      "   macro avg       0.75      0.70      0.71      4339\n",
      "weighted avg       0.76      0.77      0.75      4339\n",
      "\n",
      "0.49348869088416725\n",
      "0.9048611111111111\n",
      "0.7243460764587525\n",
      "0.5870362821035466\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 141.37268930000002 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.783410138248848, 0.7695852534562212, 0.7730414746543779, 0.738479262672811, 0.7358708189158016]\n",
      "Avg accuracy: 0.7600773895896119\n",
      "Std of accuracy : \n",
      "0.019263028250864164\n",
      "\n",
      "[[ 658  801]\n",
      " [ 240 2640]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.45      0.56      1459\n",
      "           1       0.77      0.92      0.84      2880\n",
      "\n",
      "    accuracy                           0.76      4339\n",
      "   macro avg       0.75      0.68      0.70      4339\n",
      "weighted avg       0.76      0.76      0.74      4339\n",
      "\n",
      "0.45099383139136395\n",
      "0.9166666666666666\n",
      "0.732739420935412\n",
      "0.5583368689011454\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging 2017 -> 2021 SRER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 803.2861290000001 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7684331797235023, 0.7695852534562212, 0.7707373271889401, 0.7557603686635944, 0.7681660899653979]\n",
      "Avg accuracy: 0.7665364437995311\n",
      "Std of accuracy : \n",
      "0.005464792964069163\n",
      "\n",
      "[[ 720  739]\n",
      " [ 274 2606]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.49      0.59      1459\n",
      "           1       0.78      0.90      0.84      2880\n",
      "\n",
      "    accuracy                           0.77      4339\n",
      "   macro avg       0.75      0.70      0.71      4339\n",
      "weighted avg       0.76      0.77      0.75      4339\n",
      "\n",
      "0.49348869088416725\n",
      "0.9048611111111111\n",
      "0.7243460764587525\n",
      "0.5870362821035466\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest 2017 -> 2021 SRER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 874.8300282 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.783410138248848, 0.7695852534562212, 0.7730414746543779, 0.738479262672811, 0.7358708189158016]\n",
      "Avg accuracy: 0.7600773895896119\n",
      "Std of accuracy : \n",
      "0.019263028250864164\n",
      "\n",
      "[[ 658  801]\n",
      " [ 240 2640]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.45      0.56      1459\n",
      "           1       0.77      0.92      0.84      2880\n",
      "\n",
      "    accuracy                           0.76      4339\n",
      "   macro avg       0.75      0.68      0.70      4339\n",
      "weighted avg       0.76      0.76      0.74      4339\n",
      "\n",
      "0.45099383139136395\n",
      "0.9166666666666666\n",
      "0.732739420935412\n",
      "0.5583368689011454\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging 2021 -> 2017 SRER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1032.5774026 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.4619815668202765, 0.4804147465437788, 0.5023041474654378, 0.4066820276497696, 0.44521337946943484]\n",
      "Avg accuracy: 0.45931917358973956\n",
      "Std of accuracy : \n",
      "0.0324637092013152\n",
      "\n",
      "[[ 413 1675]\n",
      " [ 671 1580]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.20      0.26      2088\n",
      "           1       0.49      0.70      0.57      2251\n",
      "\n",
      "    accuracy                           0.46      4339\n",
      "   macro avg       0.43      0.45      0.42      4339\n",
      "weighted avg       0.44      0.46      0.42      4339\n",
      "\n",
      "0.1977969348659004\n",
      "0.7019102621057308\n",
      "0.38099630996309963\n",
      "0.2604035308953342\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest 2021 -> 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1068.7303933 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.4827188940092166, 0.5357142857142857, 0.5506912442396313, 0.49193548387096775, 0.4867358708189158]\n",
      "Avg accuracy: 0.5095591557306035\n",
      "Std of accuracy : \n",
      "0.028027969326465837\n",
      "\n",
      "[[ 474 1614]\n",
      " [ 514 1737]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.23      0.31      2088\n",
      "           1       0.52      0.77      0.62      2251\n",
      "\n",
      "    accuracy                           0.51      4339\n",
      "   macro avg       0.50      0.50      0.46      4339\n",
      "weighted avg       0.50      0.51      0.47      4339\n",
      "\n",
      "0.22701149425287356\n",
      "0.7716570413149711\n",
      "0.4797570850202429\n",
      "0.3081924577373212\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging Combined SRER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1221.5127601 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7546972860125261, 0.7985386221294363, 0.7933194154488518, 0.7860125260960334, 0.7807933194154488]\n",
      "Avg accuracy: 0.7826722338204593\n",
      "Std of accuracy : \n",
      "0.015247231429236091\n",
      "\n",
      "[[ 764  611]\n",
      " [ 430 2985]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.56      0.59      1375\n",
      "           1       0.83      0.87      0.85      3415\n",
      "\n",
      "    accuracy                           0.78      4790\n",
      "   macro avg       0.73      0.71      0.72      4790\n",
      "weighted avg       0.78      0.78      0.78      4790\n",
      "\n",
      "0.5556363636363636\n",
      "0.8740849194729137\n",
      "0.6398659966499163\n",
      "0.5947839626313741\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfCombined): \n",
    "    \n",
    "    X_train = dfCombined.iloc[train_index, 5:18]\n",
    "    X_test = dfCombined.iloc[test_index, 5:18]\n",
    "    Y_train = dfCombined.iloc[train_index, -1]\n",
    "    Y_test = dfCombined.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
