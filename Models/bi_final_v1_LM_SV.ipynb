{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 3: Cross-Validating Classification Models ## \n",
    "\n",
    "I will be running cross-validation on four classification models under two scenearios for this project. \n",
    "\n",
    "The four classifiers are Logistic Regression, Linear Discriminant Analysis, Quadratic Discriminant Analysis, and K-Nearest Neighbor. \n",
    "\n",
    "K-fold cross validation is used, with 5 folds selected as the k-value. \n",
    "\n",
    "Each model will be run using the full dataset for training and testing (100% data scenario) as well as with the dataset split into 50% for training and 50% for testing (50% data scenario). 5-fold cross validation will be run for each model and scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#begin by importing all needed modules and functions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from time import time\n",
    "import timeit  #imports timeit module\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4339 entries, 0 to 4338\n",
      "Data columns (total 41 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   OID_          4339 non-null   int64  \n",
      " 1   Id            4339 non-null   int64  \n",
      " 2   gridcode      4339 non-null   int64  \n",
      " 3   Shape_Length  4339 non-null   float64\n",
      " 4   Shape_Area    4339 non-null   float64\n",
      " 5   CH_min        4339 non-null   float64\n",
      " 6   CH_max        4339 non-null   float64\n",
      " 7   CH_mean       4339 non-null   float64\n",
      " 8   CH_range      4339 non-null   float64\n",
      " 9   CH_med        4339 non-null   float64\n",
      " 10  CC_min        4339 non-null   float64\n",
      " 11  CC_max        4339 non-null   float64\n",
      " 12  CC_mean       4339 non-null   float64\n",
      " 13  CC_range      4339 non-null   float64\n",
      " 14  CC_med        4339 non-null   float64\n",
      " 15  CD_min        4339 non-null   float64\n",
      " 16  CD_max        4339 non-null   float64\n",
      " 17  CD_mean       4339 non-null   float64\n",
      " 18  CD_range      4339 non-null   float64\n",
      " 19  CD_med        4339 non-null   float64\n",
      " 20  ARVI_min      4339 non-null   float64\n",
      " 21  ARVI_max      4339 non-null   float64\n",
      " 22  ARVI_mean     4339 non-null   float64\n",
      " 23  ARVI_range    4339 non-null   float64\n",
      " 24  ARVI_med      4339 non-null   float64\n",
      " 25  EVI_min       4339 non-null   float64\n",
      " 26  EVI_max       4339 non-null   float64\n",
      " 27  EVI_mean      4339 non-null   float64\n",
      " 28  EVI_range     4339 non-null   float64\n",
      " 29  EVI_med       4339 non-null   float64\n",
      " 30  NDVI_min      4339 non-null   float64\n",
      " 31  NDVI_max      4339 non-null   float64\n",
      " 32  NDVI_mean     4339 non-null   float64\n",
      " 33  NDVI_range    4339 non-null   float64\n",
      " 34  NDVI_med      4339 non-null   float64\n",
      " 35  SAVI_min      4339 non-null   float64\n",
      " 36  SAVI_max      4339 non-null   float64\n",
      " 37  SAVI_mean     4339 non-null   float64\n",
      " 38  SAVI_range    4339 non-null   float64\n",
      " 39  SAVI_med      4339 non-null   float64\n",
      " 40  Veg_class     4339 non-null   object \n",
      "dtypes: float64(37), int64(3), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "#Read in dataset, check for empty rows and drop them\n",
    "\n",
    "df = pd.read_csv('SRER_final_bi_v1.csv') \n",
    "df.info()\n",
    "df.isnull().sum()\n",
    "df.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OID_            0\n",
       "Id              0\n",
       "gridcode        0\n",
       "Shape_Length    0\n",
       "Shape_Area      0\n",
       "CH_min          0\n",
       "CH_max          0\n",
       "CH_mean         0\n",
       "CH_range        0\n",
       "CH_med          0\n",
       "CC_min          0\n",
       "CC_max          0\n",
       "CC_mean         0\n",
       "CC_range        0\n",
       "CC_med          0\n",
       "CD_min          0\n",
       "CD_max          0\n",
       "CD_mean         0\n",
       "CD_range        0\n",
       "CD_med          0\n",
       "ARVI_min        0\n",
       "ARVI_max        0\n",
       "ARVI_mean       0\n",
       "ARVI_range      0\n",
       "ARVI_med        0\n",
       "EVI_min         0\n",
       "EVI_max         0\n",
       "EVI_mean        0\n",
       "EVI_range       0\n",
       "EVI_med         0\n",
       "NDVI_min        0\n",
       "NDVI_max        0\n",
       "NDVI_mean       0\n",
       "NDVI_range      0\n",
       "NDVI_med        0\n",
       "SAVI_min        0\n",
       "SAVI_max        0\n",
       "SAVI_mean       0\n",
       "SAVI_range      0\n",
       "SAVI_med        0\n",
       "Veg_Class       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OID_  Id  gridcode  Shape_Length  Shape_Area  CH_min  CH_max   CH_mean  \\\n",
      "0     1   2         2          41.6       17.41    0.00    0.93  0.190714   \n",
      "1     2   3         3          31.4        5.05    0.00    0.21  0.045000   \n",
      "2     3   5         5          33.4        4.84    0.00    0.02  0.010000   \n",
      "3     4   6         6          32.2       14.20    0.00    0.02  0.011667   \n",
      "4     5   7         7          28.8       10.73    0.01    0.02  0.013750   \n",
      "\n",
      "   CH_range  CH_med  ...  NDVI_max  NDVI_mean  NDVI_range  NDVI_med  SAVI_min  \\\n",
      "0      0.93   0.015  ...  0.665698   0.476709    0.382507  0.507514  0.187793   \n",
      "1      0.21   0.015  ...  0.526375   0.406745    0.306644  0.433791  0.176977   \n",
      "2      0.02   0.010  ...  0.208300   0.181575    0.076994  0.205117  0.109678   \n",
      "3      0.02   0.010  ...  0.341480   0.207390    0.234715  0.197327  0.092876   \n",
      "4      0.01   0.010  ...  0.457103   0.247391    0.334749  0.228112  0.094757   \n",
      "\n",
      "   SAVI_max  SAVI_mean  SAVI_range  SAVI_med  Veg_class  \n",
      "0  0.406991   0.286203    0.219198  0.288351      woody  \n",
      "1  0.328585   0.254814    0.151608  0.248126  non-woody  \n",
      "2  0.150184   0.131642    0.040506  0.135063  non-woody  \n",
      "3  0.197330   0.141500    0.104454  0.138725  non-woody  \n",
      "4  0.248598   0.158618    0.153841  0.142817  non-woody  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "(4339, 41)\n"
     ]
    }
   ],
   "source": [
    "#preview dataset and determine shape\n",
    "print(df.head())\n",
    "print(df.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate categorical response from the rest of the data\n",
    "#warnings were encountered when including CH_mean and CH_med variables for log. reg., so they were excluded from the model\n",
    "formula = 'Veg_class ~ CH_max+CH_range+CC_min+CC_max+CC_mean+CC_range+CC_med+CD_min+CD_max+CD_mean+CD_range+CD_med+ARVI_min+ARVI_max+ARVI_mean+ARVI_range+ARVI_med+EVI_min+EVI_max+EVI_mean+EVI_range+EVI_med+NDVI_min+NDVI_max+NDVI_mean+NDVI_range+NDVI_med+SAVI_min+SAVI_max+SAVI_mean+SAVI_range+SAVI_med'\n",
    "formula2 = 'Veg_class ~ CC_max+CC_mean+CC_range+CC_med+CD_min+CD_max+CD_mean+CD_range+CD_med+ARVI_min+ARVI_max+ARVI_mean+ARVI_range+ARVI_med+EVI_min+EVI_max+EVI_mean+EVI_range+EVI_med+NDVI_min+NDVI_max+NDVI_mean+NDVI_range+NDVI_med+SAVI_min+SAVI_max+SAVI_mean+SAVI_range+SAVI_med'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Generalized Linear Model Regression Results                               \n",
      "========================================================================================================\n",
      "Dep. Variable:     ['Veg_class[non-woody]', 'Veg_class[woody]']   No. Observations:                 4339\n",
      "Model:                                                      GLM   Df Residuals:                     4306\n",
      "Model Family:                                          Binomial   Df Model:                           32\n",
      "Link Function:                                            logit   Scale:                          1.0000\n",
      "Method:                                                    IRLS   Log-Likelihood:                -2474.5\n",
      "Date:                                          Sun, 28 Nov 2021   Deviance:                       4949.0\n",
      "Time:                                                  22:18:05   Pearson chi2:                 4.74e+03\n",
      "No. Iterations:                                               9                                         \n",
      "Covariance Type:                                      nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      8.5992      1.779      4.833      0.000       5.112      12.087\n",
      "CH_max        -6.5883      2.848     -2.313      0.021     -12.171      -1.006\n",
      "CH_range       4.8948      2.868      1.707      0.088      -0.726      10.516\n",
      "CC_min     -2.044e+05   4.06e+05     -0.503      0.615      -1e+06    5.91e+05\n",
      "CC_max      2.044e+05   4.06e+05      0.503      0.615   -5.91e+05       1e+06\n",
      "CC_mean        0.0049      0.209      0.023      0.981      -0.405       0.414\n",
      "CC_range   -2.044e+05   4.06e+05     -0.503      0.615      -1e+06    5.91e+05\n",
      "CC_med         0.1027      0.098      1.049      0.294      -0.089       0.294\n",
      "CD_min      7.222e+04   4.06e+05      0.178      0.859   -7.24e+05    8.68e+05\n",
      "CD_max     -7.222e+04   4.06e+05     -0.178      0.859   -8.68e+05    7.24e+05\n",
      "CD_mean       -0.0115      0.210     -0.055      0.956      -0.422       0.399\n",
      "CD_range    7.222e+04   4.06e+05      0.178      0.859   -7.24e+05    8.68e+05\n",
      "CD_med        -0.1018      0.098     -1.038      0.299      -0.294       0.090\n",
      "ARVI_min    3.253e+05    7.2e+05      0.452      0.651   -1.08e+06    1.74e+06\n",
      "ARVI_max   -3.253e+05    7.2e+05     -0.452      0.651   -1.74e+06    1.08e+06\n",
      "ARVI_mean     27.3794     25.153      1.089      0.276     -21.920      76.679\n",
      "ARVI_range  3.253e+05    7.2e+05      0.452      0.651   -1.08e+06    1.74e+06\n",
      "ARVI_med       9.3148     10.531      0.885      0.376     -11.325      29.955\n",
      "EVI_min     1.605e+06   7.38e+05      2.177      0.029     1.6e+05    3.05e+06\n",
      "EVI_max    -1.605e+06   7.38e+05     -2.177      0.030   -3.05e+06    -1.6e+05\n",
      "EVI_mean    -251.7188     69.795     -3.607      0.000    -388.514    -114.924\n",
      "EVI_range   1.605e+06   7.38e+05      2.177      0.029     1.6e+05    3.05e+06\n",
      "EVI_med       55.1052     32.312      1.705      0.088      -8.226     118.436\n",
      "NDVI_min   -1.844e+05   7.17e+05     -0.257      0.797   -1.59e+06    1.22e+06\n",
      "NDVI_max    1.844e+05   7.17e+05      0.257      0.797   -1.22e+06    1.59e+06\n",
      "NDVI_mean    -85.2952     33.226     -2.567      0.010    -150.418     -20.173\n",
      "NDVI_range -1.844e+05   7.17e+05     -0.257      0.797   -1.59e+06    1.22e+06\n",
      "NDVI_med       1.3141     13.643      0.096      0.923     -25.426      28.055\n",
      "SAVI_min    1.255e+06   7.36e+05      1.706      0.088   -1.87e+05     2.7e+06\n",
      "SAVI_max   -1.255e+06   7.36e+05     -1.706      0.088    -2.7e+06    1.87e+05\n",
      "SAVI_mean    252.9934     80.598      3.139      0.002      95.023     410.963\n",
      "SAVI_range  1.255e+06   7.36e+05      1.706      0.088   -1.87e+05     2.7e+06\n",
      "SAVI_med     -47.7491     36.237     -1.318      0.188    -118.773      23.275\n",
      "==============================================================================\n",
      "---Run time is 0.08750090000103228 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer() \n",
    "model_log = smf.glm(formula = formula, data=df, family=sm.families.Binomial())\n",
    "result = model_log.fit()\n",
    "print(result.summary())\n",
    "elapsed = timeit.default_timer() - start_time #gives total computation time\n",
    "print(\"---Run time is %s seconds ---\" % elapsed) #prints computation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next I use the smf.glm() function to run the linear regression model to see if it can handle more variables compared to sklearn. It turns out that the smf.glm() module can handle more variables (can include CH_max, CH_range, CC_min, CC_max, and CC_range), so I will use this moving forward. \n",
    "\n",
    "### The smallest p-value here is found for the CH_max variable, with a p-value of 0.056. This is not below the 0.05 significance threshold, therefore we conclude that none of the independent variables have a significant relationship with the Veg_Class variable based on the logistic regression model. \n",
    "\n",
    "### The p-value of the CH_max variable is very close to having a relationship with Veg_Class, however, and its negative coefficient indicates that non-woody cover (closer to a value of 1.0) may be more likely to have a lower maximum canopy height.  \n",
    "\n",
    "\n",
    "### We also calculate the model runtime as 0.0933 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16866834 0.14658174 0.65134383 0.61338492 0.59084199 0.25340254\n",
      " 0.36703228 0.87421934 0.12872417 0.12385194]\n"
     ]
    }
   ],
   "source": [
    "predictions = result.predict()\n",
    "print(predictions[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we use the predict() function to predict the probability that the ground cover is non-woody, given the values of all predictors. Since there is no dataset provided for the predict() function, probabilities are calculated using the entire dataset. \n",
    "\n",
    "### The first ten probabilities of non-woody cover are printed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['woody' 0.0]\n",
      " ['non-woody' 1.0]\n",
      " ['non-woody' 1.0]\n",
      " ...\n",
      " ['woody' 0.0]\n",
      " ['woody' 0.0]\n",
      " ['woody' 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print(np.column_stack((df.loc[:,\"Veg_class\"], result.model.endog))) # np.vstack or np.hstack\n",
    "# exog means x-values; and endog means Y-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above, we see that 'non-woody' has been mapped to 1.0 and 'woody' has been mapped to 0.0 by Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['woody', 'woody', 'non-woody', 'non-woody', 'non-woody', 'woody', 'woody', 'non-woody', 'woody', 'woody']\n"
     ]
    }
   ],
   "source": [
    "predictions_nominal = [\"woody\" if x < 0.5 else \"non-woody\" for x in predictions]\n",
    "print(predictions_nominal[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above, we set a condition to assign class labels to the predicted probabilities of non-woody cover. If the probability of non-woody cover is less than 0.5, the value is assigned to woody, otherwise it is assigned to non-woody. Cross referencing the predicted probability values with the list above lets us know that the labeling is correct.\n",
    "\n",
    "### Now that we have relabeled our probabilities, we can create a confusion matrix to assess our model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1487  601]\n",
      " [ 736 1515]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(df[\"Veg_class\"], \n",
    "                       predictions_nominal))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                    Predicted\n",
    "                    non-woody          woody\n",
    "Truth  non-woody    1487               601\n",
    "       woody        736                1515 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The values going diagonally from top left to bottom right indicate correctly classified records. The other values are instances where the model either predicted a false positive or false negative response. \n",
    "\n",
    "### Our model correctly classified vegetation cover 2254 + 726 = 2980 times out of a total 3869 records, yielding an overall accuracy of 77.0%.\n",
    "\n",
    "### Our model correctly identified 2254/2510 non-woody records, yielding a sensitivity value of 89.8%.\n",
    "\n",
    "### Our model correctly identified 726/1359 woody records, yielding a specificity value of 53.4%.\n",
    "\n",
    "### Overall accuracy and other performance metrics such as precision, recall, and f1-score can be checked by using the classification_report function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "woody        2251\n",
       "non-woody    2088\n",
       "Name: Veg_class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Veg_class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody      0.669     0.712     0.690      2088\n",
      "       woody      0.716     0.673     0.694      2251\n",
      "\n",
      "    accuracy                          0.692      4339\n",
      "   macro avg      0.692     0.693     0.692      4339\n",
      "weighted avg      0.693     0.692     0.692      4339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df[\"Veg_class\"], \n",
    "                            predictions_nominal, \n",
    "                            digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At first it seems that our model is predicting woody and non-woody cover with decent accuracy (above 70%), but we must remember that this model was trained and tested on the entire dataset, which leads to over-estimation of the training error rate and under-estimation of the test error rate. \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get a better assessment of model accuracy, we must split the dataset into a training set and a test set. We will do this using 50% of the original dataset as training data, and the other 50% as test data. This way we can see how well the model classifies new data that have not been used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#begin by shuffling records so that datasets are randomly sampled\n",
    "#df = shuffle(df)\n",
    "\n",
    "#next, set 50% of records for training and 50% for test (4339/2 = 2169.5, rounded so that training will include one more record compared to test)\n",
    "x_train = df[:2170][:]\n",
    "y_train = df[:2170]['Veg_class']\n",
    "\n",
    "x_test = df[2170:][:]\n",
    "y_test = df[2170:]['Veg_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:              Veg_class   No. Observations:                 2170\n",
      "Model:                            GLM   Df Residuals:                     2140\n",
      "Model Family:                Binomial   Df Model:                           29\n",
      "Link Function:                  logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -1284.3\n",
      "Date:                Sun, 28 Nov 2021   Deviance:                       2568.7\n",
      "Time:                        14:07:43   Pearson chi2:                 2.13e+03\n",
      "No. Iterations:                    10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -7.5873      2.233     -3.398      0.001     -11.963      -3.211\n",
      "CC_max        -0.1055      0.124     -0.848      0.396      -0.349       0.138\n",
      "CC_mean        0.3136      0.200      1.568      0.117      -0.078       0.705\n",
      "CC_range       0.0622      0.083      0.746      0.455      -0.101       0.226\n",
      "CC_med        -0.1488      0.092     -1.613      0.107      -0.330       0.032\n",
      "CD_min     -3.292e+04   7.74e+04     -0.425      0.671   -1.85e+05    1.19e+05\n",
      "CD_max      3.292e+04   7.74e+04      0.425      0.671   -1.19e+05    1.85e+05\n",
      "CD_mean       -0.2889      0.201     -1.435      0.151      -0.683       0.106\n",
      "CD_range   -3.292e+04   7.74e+04     -0.425      0.671   -1.85e+05    1.19e+05\n",
      "CD_med         0.1434      0.093      1.544      0.123      -0.039       0.325\n",
      "ARVI_min   -1.719e+06   9.89e+05     -1.738      0.082   -3.66e+06    2.19e+05\n",
      "ARVI_max    1.719e+06   9.89e+05      1.738      0.082   -2.19e+05    3.66e+06\n",
      "ARVI_mean    -24.9151     32.711     -0.762      0.446     -89.027      39.197\n",
      "ARVI_range -1.719e+06   9.89e+05     -1.738      0.082   -3.66e+06    2.19e+05\n",
      "ARVI_med      -3.7461     13.575     -0.276      0.783     -30.353      22.861\n",
      "EVI_min    -1.959e+06   1.01e+06     -1.937      0.053   -3.94e+06    2.34e+04\n",
      "EVI_max     1.959e+06   1.01e+06      1.937      0.053   -2.34e+04    3.94e+06\n",
      "EVI_mean     106.8907     90.348      1.183      0.237     -70.188     283.969\n",
      "EVI_range  -1.959e+06   1.01e+06     -1.937      0.053   -3.94e+06    2.34e+04\n",
      "EVI_med       14.7351     42.512      0.347      0.729     -68.587      98.057\n",
      "NDVI_min   -5.424e+05   9.99e+05     -0.543      0.587    -2.5e+06    1.42e+06\n",
      "NDVI_max    5.424e+05   9.99e+05      0.543      0.587   -1.42e+06     2.5e+06\n",
      "NDVI_mean     76.4893     43.360      1.764      0.078      -8.494     161.472\n",
      "NDVI_range -5.424e+05   9.99e+05     -0.543      0.587    -2.5e+06    1.42e+06\n",
      "NDVI_med      -6.6975     17.658     -0.379      0.704     -41.307      27.912\n",
      "SAVI_min   -1.782e+06   1.03e+06     -1.729      0.084    -3.8e+06    2.38e+05\n",
      "SAVI_max    1.782e+06   1.03e+06      1.729      0.084   -2.38e+05     3.8e+06\n",
      "SAVI_mean   -115.5234    104.813     -1.102      0.270    -320.952      89.906\n",
      "SAVI_range -1.782e+06   1.03e+06     -1.729      0.084    -3.8e+06    2.38e+05\n",
      "SAVI_med     -19.0594     47.962     -0.397      0.691    -113.063      74.944\n",
      "==============================================================================\n",
      "---Run time is 0.0535019999999804 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer() \n",
    "model = smf.glm(formula = formula2, \n",
    "                data = x_train, \n",
    "                family = sm.families.Binomial())\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "elapsed = timeit.default_timer() - start_time #gives total computation time\n",
    "print(\"---Run time is %s seconds ---\" % elapsed) #prints computation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strangely, after randomizing and splitting the data into 50% training and 50% test, the model begins to produce warnings about data types, division by 0, and more. In reponse I removed more variables until the model ran without errors.\n",
    "\n",
    "### Excluded variables include CH_max and CH_range. Unfortunately I had to remove the CH_max and CC_min variables, which showed to have the closest relationship to Veg_Class.\n",
    "\n",
    "### From this analysis, we see that EVI_range, EVI_min, and EVI_max have the lowest p-values with 0.053. Thus, we conclude that none of the variables have a significant relationship with the response variable according to this logistic regression model.\n",
    "\n",
    "### Finally, we see that this model took 0.0716 seconds to run.\n",
    "\n",
    "### Next, I generate model assessment metrics such as precision, recall, and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody      0.661     0.728     0.693      1017\n",
      "       woody      0.736     0.671     0.702      1152\n",
      "\n",
      "    accuracy                          0.698      2169\n",
      "   macro avg      0.699     0.699     0.697      2169\n",
      "weighted avg      0.701     0.698     0.698      2169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = result.predict(x_test)\n",
    "predictions_nominal = [ \"woody\" if x < 0.5 else \"non-woody\" for x in predictions]\n",
    "print(classification_report(y_test, \n",
    "                            predictions_nominal, \n",
    "                            digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After running the model using half of the data for training and the other half for testing, we see that overall accuracy decreased from 77% to 75.9%. \n",
    "\n",
    "### Sensitivity decreased from 89.8% to 87.8%\n",
    "\n",
    "### Specificity increased from 53.4% to 54.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[740 277]\n",
      " [379 773]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, predictions_nominal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-Fold Cross Validation for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have run the Logistic Regression model using 100% of data for training/test, we will use 5-fold cross validation to futher assess our model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 0.22666449999996985 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6762672811059908, 0.6797235023041475, 0.6831797235023042, 0.6670506912442397, 0.7151095732410612]\n",
      "\n",
      "Avg accuracy : \n",
      "0.6842661542795487\n",
      "\n",
      "Std of accuracy : \n",
      "0.01633087047040362\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=42, shuffle=True) #initiate 5-fold cross, with shuffling and random state = 42\n",
    "start_time = timeit.default_timer() #defines start time\n",
    "\n",
    "\n",
    "model_log = LogisticRegression(solver= 'liblinear')\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "output_pred = []; # set up empty lists to store metrics\n",
    "output_ID = [];\n",
    "index = [];\n",
    "\n",
    "for train_index , test_index in kf.split(df):\n",
    "    #print(train_index); ## to check the training index\n",
    "    #print(test_index); ## to check the testing index\n",
    "    #print(); print();\n",
    "    \n",
    "    X_train , X_test, X_full = df.iloc[train_index, 5:-1], df.iloc[test_index, 5:-1], df.iloc[test_index, :-1] #set training and test data for x\n",
    "    y_train , y_test = df.iloc[train_index,-1], df.iloc[test_index,-1] #set training and test data for y\n",
    "    \n",
    "    output_ID.append(X_full['Id'].tolist())\n",
    "    \n",
    "    #print([X_train.shape, y_train.shape]);\n",
    "    #print([X_test.shape, y_test.shape]);\n",
    "    \n",
    "    model_log.fit(X_train,y_train) #fit model using x training records\n",
    "    pred_values = model_log.predict(X_test) #generate predicted values based on x test records\n",
    "    \n",
    "    output_pred.append(pred_values.tolist())\n",
    "    \n",
    "    acc = accuracy_score(y_test, pred_values) #use training vs test to get accuracy\n",
    "    acc_score.append(acc) #append accuracy value to list\n",
    "    \n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])); ## it is a list \n",
    "\n",
    "elapsed = timeit.default_timer() - start_time #gives total computation time\n",
    "print(\"---Run time is %s seconds ---\" % elapsed) #prints computation time\n",
    "print()\n",
    "\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print()\n",
    "print('Avg accuracy : \\n{}'.format(np.mean(acc_score))); \n",
    "print()\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "\n",
    "output_IDFlat = [item for sublist in output_ID for item in sublist]\n",
    "output_predFlat = [item for sublist in output_pred for item in sublist]\n",
    "\n",
    "predictionDF = pd.DataFrame(list(zip(output_IDFlat, output_predFlat)), columns = ['ID', 'Prediction'])\n",
    "\n",
    "predictionDF.to_csv(\"Log_Reg_pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average accuracy = 69.3%\n",
    "### Standard deviation = 0.015%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we generate a confusion matrix for the 5-fold cross to get a more detailed look at model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.66      0.73      0.69      2088\n",
      "       woody       0.72      0.65      0.68      2251\n",
      "\n",
      "    accuracy                           0.68      4339\n",
      "   macro avg       0.69      0.69      0.68      4339\n",
      "weighted avg       0.69      0.68      0.68      4339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Convert a list to an array\n",
    "Truth = np.asarray(Truth)  ## or np.array(Truth)\n",
    "Output = np.asarray(output_predFlat)\n",
    "\n",
    "np.column_stack((Truth, Output))\n",
    "\n",
    "print(classification_report(Truth, Output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy = 69%\n",
    "### Sensitivity = 70%\n",
    "### Specificity = 69%\n",
    "### Runtime = 0.3639 sec.\n",
    "\n",
    "### 5-fold cross shows very similar accuracy metrics compared to using 100% and 50% of data for testing. 5-fold cross had slightly higher specificity, indicating that the woody class is being predicted with slightly more accuracy. 5-fold cross validation also took more time to run than running the model itself, but when factoring in time to set up the validation set method, the 5-fold cross validation may save time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section I will use the KNN classifier with various K values to perform land cover classification. One model will be run with the entire dataset as training and test and one model will be run with 50% of the dataset used for training and 50% of the dataset used for test. K values 1-10 will be tested for each model to select the ideal K value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k is 1\n",
      "[[2088    0]\n",
      " [   0 2251]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000      2088\n",
      "           1      1.000     1.000     1.000      2251\n",
      "\n",
      "    accuracy                          1.000      4339\n",
      "   macro avg      1.000     1.000     1.000      4339\n",
      "weighted avg      1.000     1.000     1.000      4339\n",
      "\n",
      "k is 2\n",
      "[[2088  928]\n",
      " [   0 1323]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.692     1.000     0.818      2088\n",
      "           1      1.000     0.588     0.740      2251\n",
      "\n",
      "    accuracy                          0.786      4339\n",
      "   macro avg      0.846     0.794     0.779      4339\n",
      "weighted avg      0.852     0.786     0.778      4339\n",
      "\n",
      "k is 3\n",
      "[[1688  490]\n",
      " [ 400 1761]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.775     0.808     0.791      2088\n",
      "           1      0.815     0.782     0.798      2251\n",
      "\n",
      "    accuracy                          0.795      4339\n",
      "   macro avg      0.795     0.795     0.795      4339\n",
      "weighted avg      0.796     0.795     0.795      4339\n",
      "\n",
      "k is 4\n",
      "[[1858  909]\n",
      " [ 230 1342]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.671     0.890     0.765      2088\n",
      "           1      0.854     0.596     0.702      2251\n",
      "\n",
      "    accuracy                          0.737      4339\n",
      "   macro avg      0.763     0.743     0.734      4339\n",
      "weighted avg      0.766     0.737     0.733      4339\n",
      "\n",
      "k is 5\n",
      "[[1570  598]\n",
      " [ 518 1653]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.724     0.752     0.738      2088\n",
      "           1      0.761     0.734     0.748      2251\n",
      "\n",
      "    accuracy                          0.743      4339\n",
      "   macro avg      0.743     0.743     0.743      4339\n",
      "weighted avg      0.743     0.743     0.743      4339\n",
      "\n",
      "k is 6\n",
      "[[1745  893]\n",
      " [ 343 1358]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.661     0.836     0.738      2088\n",
      "           1      0.798     0.603     0.687      2251\n",
      "\n",
      "    accuracy                          0.715      4339\n",
      "   macro avg      0.730     0.720     0.713      4339\n",
      "weighted avg      0.732     0.715     0.712      4339\n",
      "\n",
      "k is 7\n",
      "[[1489  641]\n",
      " [ 599 1610]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.699     0.713     0.706      2088\n",
      "           1      0.729     0.715     0.722      2251\n",
      "\n",
      "    accuracy                          0.714      4339\n",
      "   macro avg      0.714     0.714     0.714      4339\n",
      "weighted avg      0.715     0.714     0.714      4339\n",
      "\n",
      "k is 8\n",
      "[[1681  908]\n",
      " [ 407 1343]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.649     0.805     0.719      2088\n",
      "           1      0.767     0.597     0.671      2251\n",
      "\n",
      "    accuracy                          0.697      4339\n",
      "   macro avg      0.708     0.701     0.695      4339\n",
      "weighted avg      0.711     0.697     0.694      4339\n",
      "\n",
      "k is 9\n",
      "[[1480  695]\n",
      " [ 608 1556]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.680     0.709     0.694      2088\n",
      "           1      0.719     0.691     0.705      2251\n",
      "\n",
      "    accuracy                          0.700      4339\n",
      "   macro avg      0.700     0.700     0.700      4339\n",
      "weighted avg      0.700     0.700     0.700      4339\n",
      "\n",
      "k is 10\n",
      "[[1640  916]\n",
      " [ 448 1335]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.642     0.785     0.706      2088\n",
      "           1      0.749     0.593     0.662      2251\n",
      "\n",
      "    accuracy                          0.686      4339\n",
      "   macro avg      0.695     0.689     0.684      4339\n",
      "weighted avg      0.697     0.686     0.683      4339\n",
      "\n",
      "---Run time is 2.0657850000000053 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#first I remap the woody and non-woody variables to 0 = non-woody and 1 = woody\n",
    "df.Veg_class = pd.Categorical(df.Veg_class)\n",
    "df['Veg_class'] = df.Veg_class.cat.codes\n",
    "df.head()\n",
    "\n",
    "\n",
    "#Next I run the KNN model using the entire dataset for training and test, with k-values 1-10\n",
    "start_time = timeit.default_timer() \n",
    "for k in range(1,11):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "    pred = knn.fit(df[['CH_min', 'CH_max', 'CH_mean', 'CH_range', 'CH_med', 'CC_min', 'CC_max', 'CC_mean', 'CC_range', 'CC_med', 'CD_min', 'CD_max', 'CD_mean', 'CD_range', 'CD_med', 'ARVI_min', 'ARVI_max', 'ARVI_mean', 'ARVI_range', 'ARVI_med', 'EVI_min', 'EVI_max', 'EVI_mean', 'EVI_range', 'EVI_med', 'NDVI_min', 'NDVI_max', 'NDVI_mean', 'NDVI_range', 'NDVI_med', 'SAVI_min', 'SAVI_max', 'SAVI_mean', 'SAVI_range', 'SAVI_med']], df['Veg_class']).predict(df[['CH_min', 'CH_max', 'CH_mean', 'CH_range', 'CH_med', 'CC_min', 'CC_max', 'CC_mean', 'CC_range', 'CC_med', 'CD_min', 'CD_max', 'CD_mean', 'CD_range', 'CD_med', 'ARVI_min', 'ARVI_max', 'ARVI_mean', 'ARVI_range', 'ARVI_med', 'EVI_min', 'EVI_max', 'EVI_mean', 'EVI_range', 'EVI_med', 'NDVI_min', 'NDVI_max', 'NDVI_mean', 'NDVI_range', 'NDVI_med', 'SAVI_min', 'SAVI_max', 'SAVI_mean', 'SAVI_range', 'SAVI_med']])\n",
    "    \n",
    "    \n",
    "    print(\"k is\", k)\n",
    "    print(confusion_matrix(df['Veg_class'], pred).T)\n",
    "    print(classification_report(df['Veg_class'], pred, digits=3))\n",
    "elapsed = timeit.default_timer() - start_time #gives total computation time\n",
    "print(\"---Run time is %s seconds ---\" % elapsed) #prints computation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k = 3 performs best (best tradeoff between metrics)\n",
    "### overall accuracy = 79.5%\n",
    "### sensitivity = 80.8%\n",
    "### specificity = 78.2%\n",
    "### runtime = 2.0658"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k is 1\n",
      "[[551 536]\n",
      " [466 616]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.507     0.542     0.524      1017\n",
      "           1      0.569     0.535     0.551      1152\n",
      "\n",
      "    accuracy                          0.538      2169\n",
      "   macro avg      0.538     0.538     0.538      2169\n",
      "weighted avg      0.540     0.538     0.538      2169\n",
      "\n",
      "k is 2\n",
      "[[789 776]\n",
      " [228 376]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.504     0.776     0.611      1017\n",
      "           1      0.623     0.326     0.428      1152\n",
      "\n",
      "    accuracy                          0.537      2169\n",
      "   macro avg      0.563     0.551     0.520      2169\n",
      "weighted avg      0.567     0.537     0.514      2169\n",
      "\n",
      "k is 3\n",
      "[[539 538]\n",
      " [478 614]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.500     0.530     0.515      1017\n",
      "           1      0.562     0.533     0.547      1152\n",
      "\n",
      "    accuracy                          0.532      2169\n",
      "   macro avg      0.531     0.531     0.531      2169\n",
      "weighted avg      0.533     0.532     0.532      2169\n",
      "\n",
      "k is 4\n",
      "[[727 722]\n",
      " [290 430]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.502     0.715     0.590      1017\n",
      "           1      0.597     0.373     0.459      1152\n",
      "\n",
      "    accuracy                          0.533      2169\n",
      "   macro avg      0.549     0.544     0.525      2169\n",
      "weighted avg      0.552     0.533     0.520      2169\n",
      "\n",
      "k is 5\n",
      "[[545 519]\n",
      " [472 633]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.512     0.536     0.524      1017\n",
      "           1      0.573     0.549     0.561      1152\n",
      "\n",
      "    accuracy                          0.543      2169\n",
      "   macro avg      0.543     0.543     0.542      2169\n",
      "weighted avg      0.544     0.543     0.544      2169\n",
      "\n",
      "k is 6\n",
      "[[699 698]\n",
      " [318 454]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.500     0.687     0.579      1017\n",
      "           1      0.588     0.394     0.472      1152\n",
      "\n",
      "    accuracy                          0.532      2169\n",
      "   macro avg      0.544     0.541     0.526      2169\n",
      "weighted avg      0.547     0.532     0.522      2169\n",
      "\n",
      "k is 7\n",
      "[[562 537]\n",
      " [455 615]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.511     0.553     0.531      1017\n",
      "           1      0.575     0.534     0.554      1152\n",
      "\n",
      "    accuracy                          0.543      2169\n",
      "   macro avg      0.543     0.543     0.542      2169\n",
      "weighted avg      0.545     0.543     0.543      2169\n",
      "\n",
      "k is 8\n",
      "[[683 671]\n",
      " [334 481]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.504     0.672     0.576      1017\n",
      "           1      0.590     0.418     0.489      1152\n",
      "\n",
      "    accuracy                          0.537      2169\n",
      "   macro avg      0.547     0.545     0.533      2169\n",
      "weighted avg      0.550     0.537     0.530      2169\n",
      "\n",
      "k is 9\n",
      "[[552 529]\n",
      " [465 623]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.511     0.543     0.526      1017\n",
      "           1      0.573     0.541     0.556      1152\n",
      "\n",
      "    accuracy                          0.542      2169\n",
      "   macro avg      0.542     0.542     0.541      2169\n",
      "weighted avg      0.544     0.542     0.542      2169\n",
      "\n",
      "k is 10\n",
      "[[649 670]\n",
      " [368 482]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.492     0.638     0.556      1017\n",
      "           1      0.567     0.418     0.482      1152\n",
      "\n",
      "    accuracy                          0.521      2169\n",
      "   macro avg      0.530     0.528     0.519      2169\n",
      "weighted avg      0.532     0.521     0.516      2169\n",
      "\n",
      "---Run time is 0.8780361999999968 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Next I run the KNN model using 50% of the data for training and 50% of the data for testing\n",
    "\n",
    "x_train = df.iloc[:2170, :-1]\n",
    "y_train = df[:2170]['Veg_class']\n",
    "\n",
    "x_test = df.iloc[2170:, :-1]\n",
    "y_test = df[2170:]['Veg_class']\n",
    "\n",
    "start_time = timeit.default_timer() \n",
    "for k in range(1,11):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "    pred = knn.fit(x_train, y_train).predict(x_test)\n",
    "    \n",
    "    \n",
    "    print(\"k is\", k)\n",
    "    print(confusion_matrix(y_test, pred).T)\n",
    "    print(classification_report(y_test, pred, digits=3))\n",
    "elapsed = timeit.default_timer() - start_time #gives total computation time\n",
    "print(\"---Run time is %s seconds ---\" % elapsed) #prints computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3472,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k = 5 performs best (best tradeoff between three metrics)\n",
    "### overall accuracy = 54.3%\n",
    "### sensitivity = 53.6%\n",
    "### specificity = 54.9%\n",
    "### runtime = 0.8780 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold Cross Validation\n",
    "### 5-fold cross validation will be run with k values 1-10 as in the validation set method to assess each k value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k is 1\n",
      "Accuracy of each fold: \n",
      " [0.597926267281106, 0.586405529953917, 0.565668202764977, 0.5737327188940092, 0.5940023068050749]\n",
      "\n",
      "Avg accuracy : \n",
      "0.5835470051398168\n",
      "\n",
      "Std of accuracy : \n",
      "0.012167328923190284\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.57      0.58      0.57      2088\n",
      "       woody       0.60      0.58      0.59      2251\n",
      "\n",
      "    accuracy                           0.58      4339\n",
      "   macro avg       0.58      0.58      0.58      4339\n",
      "weighted avg       0.58      0.58      0.58      4339\n",
      "\n",
      "k is 2\n",
      "Accuracy of each fold: \n",
      " [0.5806451612903226, 0.5806451612903226, 0.5817972350230415, 0.5576036866359447, 0.5836216839677048]\n",
      "\n",
      "Avg accuracy : \n",
      "0.5768625856414673\n",
      "\n",
      "Std of accuracy : \n",
      "0.009690790707536317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.54      0.81      0.65      2088\n",
      "       woody       0.67      0.36      0.47      2251\n",
      "\n",
      "    accuracy                           0.58      4339\n",
      "   macro avg       0.61      0.59      0.56      4339\n",
      "weighted avg       0.61      0.58      0.56      4339\n",
      "\n",
      "k is 3\n",
      "Accuracy of each fold: \n",
      " [0.618663594470046, 0.5829493087557603, 0.6163594470046083, 0.5817972350230415, 0.6251441753171857]\n",
      "\n",
      "Avg accuracy : \n",
      "0.6049827521141284\n",
      "\n",
      "Std of accuracy : \n",
      "0.018687527652307655\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.59      0.61      0.60      2088\n",
      "       woody       0.62      0.60      0.61      2251\n",
      "\n",
      "    accuracy                           0.60      4339\n",
      "   macro avg       0.60      0.61      0.60      4339\n",
      "weighted avg       0.61      0.60      0.61      4339\n",
      "\n",
      "k is 4\n",
      "Accuracy of each fold: \n",
      " [0.5748847926267281, 0.5817972350230415, 0.5806451612903226, 0.5311059907834101, 0.6124567474048442]\n",
      "\n",
      "Avg accuracy : \n",
      "0.5761779854256692\n",
      "\n",
      "Std of accuracy : \n",
      "0.02608002011361191\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.54      0.75      0.63      2088\n",
      "       woody       0.64      0.42      0.51      2251\n",
      "\n",
      "    accuracy                           0.58      4339\n",
      "   macro avg       0.59      0.58      0.57      4339\n",
      "weighted avg       0.59      0.58      0.57      4339\n",
      "\n",
      "k is 5\n",
      "Accuracy of each fold: \n",
      " [0.5829493087557603, 0.5852534562211982, 0.6094470046082949, 0.5829493087557603, 0.6286043829296425]\n",
      "\n",
      "Avg accuracy : \n",
      "0.5978406922541313\n",
      "\n",
      "Std of accuracy : \n",
      "0.018346968503711043\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.58      0.60      0.59      2088\n",
      "       woody       0.62      0.60      0.61      2251\n",
      "\n",
      "    accuracy                           0.60      4339\n",
      "   macro avg       0.60      0.60      0.60      4339\n",
      "weighted avg       0.60      0.60      0.60      4339\n",
      "\n",
      "k is 6\n",
      "Accuracy of each fold: \n",
      " [0.5737327188940092, 0.5679723502304147, 0.5933179723502304, 0.5380184331797235, 0.615916955017301]\n",
      "\n",
      "Avg accuracy : \n",
      "0.5777916859343358\n",
      "\n",
      "Std of accuracy : \n",
      "0.026036159141854733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.55      0.72      0.62      2088\n",
      "       woody       0.63      0.45      0.52      2251\n",
      "\n",
      "    accuracy                           0.58      4339\n",
      "   macro avg       0.59      0.58      0.57      4339\n",
      "weighted avg       0.59      0.58      0.57      4339\n",
      "\n",
      "k is 7\n",
      "Accuracy of each fold: \n",
      " [0.5944700460829493, 0.576036866359447, 0.6152073732718893, 0.5645161290322581, 0.6089965397923875]\n",
      "\n",
      "Avg accuracy : \n",
      "0.5918453909077862\n",
      "\n",
      "Std of accuracy : \n",
      "0.01920224437865312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.57      0.60      0.59      2088\n",
      "       woody       0.61      0.58      0.60      2251\n",
      "\n",
      "    accuracy                           0.59      4339\n",
      "   macro avg       0.59      0.59      0.59      4339\n",
      "weighted avg       0.59      0.59      0.59      4339\n",
      "\n",
      "k is 8\n",
      "Accuracy of each fold: \n",
      " [0.576036866359447, 0.5794930875576036, 0.5933179723502304, 0.5472350230414746, 0.6020761245674741]\n",
      "\n",
      "Avg accuracy : \n",
      "0.579631814775246\n",
      "\n",
      "Std of accuracy : \n",
      "0.01872726870214306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.55      0.70      0.62      2088\n",
      "       woody       0.63      0.47      0.54      2251\n",
      "\n",
      "    accuracy                           0.58      4339\n",
      "   macro avg       0.59      0.58      0.58      4339\n",
      "weighted avg       0.59      0.58      0.57      4339\n",
      "\n",
      "k is 9\n",
      "Accuracy of each fold: \n",
      " [0.5875576036866359, 0.576036866359447, 0.597926267281106, 0.5702764976958525, 0.6055363321799307]\n",
      "\n",
      "Avg accuracy : \n",
      "0.5874667134405943\n",
      "\n",
      "Std of accuracy : \n",
      "0.013130625046916445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.57      0.60      0.58      2088\n",
      "       woody       0.61      0.58      0.59      2251\n",
      "\n",
      "    accuracy                           0.59      4339\n",
      "   macro avg       0.59      0.59      0.59      4339\n",
      "weighted avg       0.59      0.59      0.59      4339\n",
      "\n",
      "k is 10\n",
      "Accuracy of each fold: \n",
      " [0.586405529953917, 0.5702764976958525, 0.6071428571428571, 0.5645161290322581, 0.6089965397923875]\n",
      "\n",
      "Avg accuracy : \n",
      "0.5874675107234545\n",
      "\n",
      "Std of accuracy : \n",
      "0.018297775837404363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.56      0.70      0.62      2088\n",
      "       woody       0.63      0.48      0.55      2251\n",
      "\n",
      "    accuracy                           0.59      4339\n",
      "   macro avg       0.60      0.59      0.58      4339\n",
      "weighted avg       0.60      0.59      0.58      4339\n",
      "\n",
      "---Run time is 3.2216678999998294 seconds ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "result=[]\n",
    "start_time = timeit.default_timer() \n",
    "for k in range(1,11):\n",
    "    \n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors = k)\n",
    "    \n",
    "    acc_score = [];\n",
    "    Truth = [];\n",
    "    output_pred = [];\n",
    "    output_ID = [];\n",
    "\n",
    "    for train_index , test_index in kf.split(df):\n",
    "        #print(train_index); ## to check the training index\n",
    "        #print(test_index); ## to check the testing index\n",
    "        #print(); print();\n",
    "    \n",
    "        X_train , X_test, X_full = df.iloc[train_index, 5:-1], df.iloc[test_index, 5:-1], df.iloc[test_index, :-1]\n",
    "        y_train , y_test = df.iloc[train_index,-1], df.iloc[test_index,-1]\n",
    "    \n",
    "        #print([X_train.shape, y_train.shape]);\n",
    "        #print([X_test.shape, y_test.shape]);\n",
    "        \n",
    "        output_ID.append(X_full['Id'].tolist())\n",
    "        \n",
    "        knn.fit(X_train,y_train)\n",
    "        pred_values = knn.predict(X_test)\n",
    "        \n",
    "        output_pred.append(pred_values.tolist())\n",
    "     \n",
    "        acc = accuracy_score(y_test, pred_values)\n",
    "        acc_score.append(acc)\n",
    "        \n",
    "        \n",
    "        Truth.extend(y_test.values.reshape(y_test.shape[0])); ## it is a list \n",
    "    \n",
    "    print(\"k is\", k)\n",
    "    print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "    print()\n",
    "    print('Avg accuracy : \\n{}'.format(np.mean(acc_score))); \n",
    "    print()\n",
    "    print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "\n",
    "    output_IDFlat = [item for sublist in output_ID for item in sublist]\n",
    "    output_predFlat = [item for sublist in output_pred for item in sublist]\n",
    "    \n",
    "    predictionDF = pd.DataFrame(list(zip(output_IDFlat, output_predFlat)), columns = ['ID', 'Prediction'])\n",
    "    \n",
    "    predictionDF.to_csv('knn'+str(k)+'_pred.csv')\n",
    "    \n",
    "    Truth = np.asarray(Truth)  ## or np.array(Truth)\n",
    "    Output = np.asarray(output_predFlat)\n",
    "\n",
    "    np.column_stack((Truth, Output))\n",
    "\n",
    "    print(classification_report(Truth, Output))\n",
    "    \n",
    "elapsed = timeit.default_timer() - start_time #gives total computation time\n",
    "print(\"---Run time is %s seconds ---\" % elapsed) #prints computation time\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k = 10 has the highest overall accuracy with 55%. This is well below the accuracy reported for the validation set method.\n",
    "\n",
    "### Runtime = 3.0314 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Linear Discriminant Analysis (LDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section I will be using Linear Discriminant Analysis for land cover classification\n",
    "\n",
    "### As with the other models, one model will be trained and tested with the entire dataset and one model will be trained with 50% of the dataset and tested with 50% of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48121687 0.51878313]\n",
      "[[5.58429119e-03 4.30028734e-02 1.49203417e-02 3.74185822e-02\n",
      "  1.30531609e-02 1.54677202e+01 6.45607274e+01 3.82010689e+01\n",
      "  4.90930073e+01 3.72941329e+01 1.54716953e+01 6.45329975e+01\n",
      "  3.81849045e+01 4.90613023e+01 3.72730840e+01 3.81274211e-02\n",
      "  1.85078672e-01 9.94291988e-02 1.46951251e-01 9.30927518e-02\n",
      "  1.43241749e-01 2.11871499e-01 1.71893421e-01 6.86297506e-02\n",
      "  1.68902770e-01 2.10016125e-01 3.34135376e-01 2.62371235e-01\n",
      "  1.24119250e-01 2.57300461e-01 1.42705280e-01 2.07888906e-01\n",
      "  1.70312378e-01 6.51836286e-02 1.67678809e-01]\n",
      " [5.17236783e-02 2.73025321e-01 1.43306035e-01 2.21301644e-01\n",
      "  1.29386939e-01 1.95057306e+01 7.03702349e+01 4.42549972e+01\n",
      "  5.08645044e+01 4.39452463e+01 1.93281651e+01 6.99373607e+01\n",
      "  4.38089342e+01 5.06091957e+01 4.34224342e+01 1.10782208e-01\n",
      "  3.03764405e-01 2.06690971e-01 1.92982196e-01 2.05608390e-01\n",
      "  1.77020225e-01 2.65746094e-01 2.20985702e-01 8.87258674e-02\n",
      "  2.20464308e-01 2.74197700e-01 4.31125843e-01 3.52945620e-01\n",
      "  1.56928142e-01 3.52713639e-01 1.75744227e-01 2.56999071e-01\n",
      "  2.16439246e-01 8.12548440e-02 2.16362615e-01]]\n",
      "[[-4.42893205e-01  3.77239183e-02  2.03839658e+00  1.43054321e-01\n",
      "  -1.32243176e+00  2.37728946e-02  2.19892092e-02 -3.95310141e-02\n",
      "   3.74462746e-03 -2.86053009e-02 -2.23704577e-02 -2.07791720e-02\n",
      "   4.39185940e-02 -3.71555328e-03  2.81719664e-02  7.28817880e+00\n",
      "   4.02757583e+00 -3.68529565e+01 -6.80754758e-01 -5.13894547e+00\n",
      "  -2.99409093e+00 -5.36156987e+00  9.71804172e+01 -6.32646431e+00\n",
      "  -2.46838276e+01 -1.97655680e+01 -1.12455164e+01  8.62317024e+01\n",
      "   3.23720706e+00 -3.12810328e+00 -1.93668178e+00  1.70276699e-01\n",
      "  -1.07753422e+02  2.15673118e+00  2.33059757e+01]]\n",
      "(array([0, 1], dtype=int8), array([2205, 2134], dtype=int64))\n",
      "[[1470  618]\n",
      " [ 735 1516]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.667     0.704     0.685      2088\n",
      "           1      0.710     0.673     0.691      2251\n",
      "\n",
      "    accuracy                          0.688      4339\n",
      "   macro avg      0.689     0.689     0.688      4339\n",
      "weighted avg      0.689     0.688     0.688      4339\n",
      "\n",
      "---Run time is 0.029903299999887167 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#start with using the entire dataset for training and test\n",
    "start_time = timeit.default_timer() \n",
    "lda = LinearDiscriminantAnalysis()\n",
    "model_lda = lda.fit(df[['CH_min', 'CH_max', 'CH_mean', 'CH_range', 'CH_med', 'CC_min', 'CC_max', 'CC_mean', 'CC_range', 'CC_med', 'CD_min', 'CD_max', 'CD_mean', 'CD_range', 'CD_med', 'ARVI_min', 'ARVI_max', 'ARVI_mean', 'ARVI_range', 'ARVI_med', 'EVI_min', 'EVI_max', 'EVI_mean', 'EVI_range', 'EVI_med', 'NDVI_min', 'NDVI_max', 'NDVI_mean', 'NDVI_range', 'NDVI_med', 'SAVI_min', 'SAVI_max', 'SAVI_mean', 'SAVI_range', 'SAVI_med']], df['Veg_class'])\n",
    "\n",
    "print(model_lda.priors_)\n",
    "print(model_lda.means_)\n",
    "print(model_lda.coef_)\n",
    "\n",
    "pred=model_lda.predict(df[['CH_min', 'CH_max', 'CH_mean', 'CH_range', 'CH_med', 'CC_min', 'CC_max', 'CC_mean', 'CC_range', 'CC_med', 'CD_min', 'CD_max', 'CD_mean', 'CD_range', 'CD_med', 'ARVI_min', 'ARVI_max', 'ARVI_mean', 'ARVI_range', 'ARVI_med', 'EVI_min', 'EVI_max', 'EVI_mean', 'EVI_range', 'EVI_med', 'NDVI_min', 'NDVI_max', 'NDVI_mean', 'NDVI_range', 'NDVI_med', 'SAVI_min', 'SAVI_max', 'SAVI_mean', 'SAVI_range', 'SAVI_med']])\n",
    "print(np.unique(pred, return_counts=True))\n",
    "print(confusion_matrix(df['Veg_class'], pred))\n",
    "print()\n",
    "print(classification_report(df['Veg_class'], pred, digits=3))\n",
    "elapsed = timeit.default_timer() - start_time #gives total computation time\n",
    "print(\"---Run time is %s seconds ---\" % elapsed) #prints computation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy = 68.8%\n",
    "### Sensitivity = 70.4%\n",
    "### Specificity = 67.3%\n",
    "### runtime = 0.02990 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2170, 3472]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-bac96ea7210b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearDiscriminantAnalysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel_lda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_lda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpriors_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[0mTarget\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \"\"\"\n\u001b[1;32m--> 424\u001b[1;33m         X, y = self._validate_data(X, y, ensure_min_samples=2, estimator=self,\n\u001b[0m\u001b[0;32m    425\u001b[0m                                    dtype=[np.float64, np.float32])\n\u001b[0;32m    426\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    810\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 812\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2170, 3472]"
     ]
    }
   ],
   "source": [
    "#Next, use 50% of data for training and 50% for test\n",
    "start_time = timeit.default_timer() \n",
    "lda = LinearDiscriminantAnalysis()\n",
    "model_lda = lda.fit(x_train, y_train)\n",
    "\n",
    "print(model_lda.priors_)\n",
    "print(model_lda.means_)\n",
    "print(model_lda.coef_)\n",
    "\n",
    "pred=model_lda.predict(x_test)\n",
    "print(np.unique(pred, return_counts=True))\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print()\n",
    "print(classification_report(y_test, pred, digits=3))\n",
    "elapsed = timeit.default_timer() - start_time #gives total computation time\n",
    "print(\"---Run time is %s seconds ---\" % elapsed) #prints computation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy = 77.6%\n",
    "### Sensitivity = 90.5%\n",
    "### Specificity = 54.1%\n",
    "### runtime = 0.0489 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold Cross Validation\n",
    "### Next, we use the 5-fold cross validation method to assess LDA model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 0.10089260000040667 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6670506912442397, 0.6797235023041475, 0.6762672811059908, 0.6751152073732719, 0.7104959630911188]\n",
      "\n",
      "Avg accuracy : \n",
      "0.6817305290237536\n",
      "\n",
      "Std of accuracy : \n",
      "0.014970541399163\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.66      0.70      0.68      2088\n",
      "       woody       0.70      0.66      0.68      2251\n",
      "\n",
      "    accuracy                           0.68      4339\n",
      "   macro avg       0.68      0.68      0.68      4339\n",
      "weighted avg       0.68      0.68      0.68      4339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "output_pred = [];\n",
    "output_ID = [];\n",
    "\n",
    "for train_index , test_index in kf.split(df):\n",
    "    #print(train_index); ## to check the training index\n",
    "    #print(test_index); ## to check the testing index\n",
    "    #print(); print();\n",
    "    \n",
    "    X_train , X_test, X_full = df.iloc[train_index, 5:-1], df.iloc[test_index, 5:-1], df.iloc[test_index, :-1]\n",
    "    y_train , y_test = df.iloc[train_index,-1], df.iloc[test_index,-1]\n",
    "    \n",
    "    output_ID.append(X_full['Id'].tolist())\n",
    "    \n",
    "    #print([X_train.shape, y_train.shape]);\n",
    "    #print([X_test.shape, y_test.shape]);\n",
    "    \n",
    "    lda.fit(X_train,y_train)\n",
    "    pred_values = lda.predict(X_test)\n",
    "    \n",
    "    output_pred.append(pred_values.tolist())\n",
    "     \n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "    \n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])); ## it is a list \n",
    "\n",
    "elapsed = timeit.default_timer() - start_time #gives total computation time\n",
    "print(\"---Run time is %s seconds ---\" % elapsed) #prints computation time\n",
    "print()\n",
    "\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print()\n",
    "print('Avg accuracy : \\n{}'.format(np.mean(acc_score))); \n",
    "print()\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "\n",
    "output_IDFlat = [item for sublist in output_ID for item in sublist]\n",
    "output_predFlat = [item for sublist in output_pred for item in sublist]\n",
    "\n",
    "predictionDF = pd.DataFrame(list(zip(output_IDFlat, output_predFlat)), columns = ['ID', 'Prediction'])\n",
    "predictionDF.to_csv('LDA_pred.csv')\n",
    "\n",
    "Truth = np.asarray(Truth)  ## or np.array(Truth)\n",
    "Output = np.asarray(output_predFlat)\n",
    "\n",
    "np.column_stack((Truth, Output))\n",
    "\n",
    "print(classification_report(Truth, Output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Quadratic Discriminant Analysis (QDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section I will be using Quadratice Discriminant Analysis for land cover classification\n",
    "\n",
    "### As with the other models, one model will be trained and tested with the entire dataset and one model will be trained with 50% of the dataset and tested with 50% of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64874645 0.35125355]\n",
      "\n",
      "[[4.05976096e-03 2.75019920e-02 1.25140943e-02 2.34422310e-02\n",
      "  1.16035857e-02 1.18697210e+01 6.90014336e+01 3.82073677e+01\n",
      "  5.71317127e+01 3.69702786e+01 1.18697210e+01 6.89881667e+01\n",
      "  3.82050500e+01 5.71184458e+01 3.69702786e+01 1.88597495e-02\n",
      "  1.89581977e-01 9.19690975e-02 1.70722228e-01 8.40892780e-02\n",
      "  1.31612837e-01 2.08867981e-01 1.65098788e-01 7.72551447e-02\n",
      "  1.62000046e-01 1.91405354e-01 3.36960050e-01 2.54430154e-01\n",
      "  1.45554698e-01 2.48124194e-01 1.30994836e-01 2.05366027e-01\n",
      "  1.63560773e-01 7.43711922e-02 1.60757318e-01]\n",
      " [1.46725534e-02 1.98565122e-01 8.27510361e-02 1.83892569e-01\n",
      "  6.32008829e-02 1.31161147e+01 7.45027956e+01 4.27195312e+01\n",
      "  6.13866811e+01 4.22453272e+01 1.31247239e+01 7.44329648e+01\n",
      "  4.26608627e+01 6.13082411e+01 4.21465045e+01 7.93598447e-02\n",
      "  3.22179671e-01 2.01130450e-01 2.42819828e-01 2.01333353e-01\n",
      "  1.56951563e-01 2.65500199e-01 2.10556075e-01 1.08548637e-01\n",
      "  2.09937252e-01 2.43662453e-01 4.44557441e-01 3.45535444e-01\n",
      "  2.00894988e-01 3.46517453e-01 1.55728055e-01 2.57127667e-01\n",
      "  2.06464923e-01 1.01399614e-01 2.06332057e-01]]\n",
      "(array(['non-woody', 'woody'], dtype=object), array([3426,  443], dtype=int64))\n",
      "\n",
      "[[2379  131]\n",
      " [1047  312]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody      0.694     0.948     0.802      2510\n",
      "       woody      0.704     0.230     0.346      1359\n",
      "\n",
      "    accuracy                          0.696      3869\n",
      "   macro avg      0.699     0.589     0.574      3869\n",
      "weighted avg      0.698     0.696     0.642      3869\n",
      "\n",
      "---Run time is 0.08178769999994984 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsh3146\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:715: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "#start with using the entire dataset for training and test\n",
    "start_time = timeit.default_timer() \n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "model_qda = qda.fit(df[['CH_min', 'CH_max', 'CH_mean', 'CH_range', 'CH_med', 'CC_min', 'CC_max', 'CC_mean', 'CC_range', 'CC_med', 'CD_min', 'CD_max', 'CD_mean', 'CD_range', 'CD_med', 'ARVI_min', 'ARVI_max', 'ARVI_mean', 'ARVI_range', 'ARVI_med', 'EVI_min', 'EVI_max', 'EVI_mean', 'EVI_range', 'EVI_med', 'NDVI_min', 'NDVI_max', 'NDVI_mean', 'NDVI_range', 'NDVI_med', 'SAVI_min', 'SAVI_max', 'SAVI_mean', 'SAVI_range', 'SAVI_med']], df['Veg_Class'])\n",
    "print(model_qda.priors_)\n",
    "print()\n",
    "print(model_qda.means_)\n",
    "\n",
    "pred2=model_qda.predict(df[['CH_min', 'CH_max', 'CH_mean', 'CH_range', 'CH_med', 'CC_min', 'CC_max', 'CC_mean', 'CC_range', 'CC_med', 'CD_min', 'CD_max', 'CD_mean', 'CD_range', 'CD_med', 'ARVI_min', 'ARVI_max', 'ARVI_mean', 'ARVI_range', 'ARVI_med', 'EVI_min', 'EVI_max', 'EVI_mean', 'EVI_range', 'EVI_med', 'NDVI_min', 'NDVI_max', 'NDVI_mean', 'NDVI_range', 'NDVI_med', 'SAVI_min', 'SAVI_max', 'SAVI_mean', 'SAVI_range', 'SAVI_med']])\n",
    "print(np.unique(pred2, return_counts=True))\n",
    "print()\n",
    "print(confusion_matrix(df['Veg_Class'], pred2))\n",
    "print()\n",
    "print(classification_report(df['Veg_Class'], pred2, digits=3))\n",
    "elapsed = timeit.default_timer() - start_time #gives total computation time\n",
    "print(\"---Run time is %s seconds ---\" % elapsed) #prints computation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy = 69.6%\n",
    "### Sensitivity = 94.8%\n",
    "### Specificity = 23.0%\n",
    "### runtime = 0.0818 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65356774 0.34643226]\n",
      "\n",
      "[[2.04970728e+03 2.04970728e+03 2.04970728e+03 2.81977848e+01\n",
      "  8.32041139e+00 4.12183544e-03 2.90031645e-02 1.25997495e-02\n",
      "  2.48813290e-02 1.16178797e-02 1.19321993e+01 6.91114709e+01\n",
      "  3.83908637e+01 5.71792717e+01 3.74227450e+01 1.19321993e+01\n",
      "  6.91114709e+01 3.83910220e+01 5.71792717e+01 3.74227450e+01\n",
      "  1.67604952e-02 1.87984325e-01 8.93671421e-02 1.71223830e-01\n",
      "  8.12018562e-02 1.30882665e-01 2.08438983e-01 1.64271732e-01\n",
      "  7.75563174e-02 1.60925972e-01 1.89702169e-01 3.35690157e-01\n",
      "  2.52361068e-01 1.45987989e-01 2.45843203e-01 1.30306447e-01\n",
      "  2.05016497e-01 1.62776865e-01 7.47100510e-02 1.59718646e-01]\n",
      " [1.84577313e+03 1.84577313e+03 1.84577313e+03 2.10797015e+01\n",
      "  5.98007463e+00 1.63283582e-02 2.20477612e-01 9.12068627e-02\n",
      "  2.04149254e-01 6.80298507e-02 1.30697014e+01 7.41992532e+01\n",
      "  4.27603303e+01 6.11295519e+01 4.24770147e+01 1.30573133e+01\n",
      "  7.42241786e+01 4.27086651e+01 6.11668654e+01 4.23286565e+01\n",
      "  7.61676004e-02 3.23095016e-01 2.00441605e-01 2.46927417e-01\n",
      "  2.01089567e-01 1.55646307e-01 2.65287317e-01 2.10020249e-01\n",
      "  1.09641011e-01 2.09588186e-01 2.40557580e-01 4.44921817e-01\n",
      "  3.44409465e-01 2.04364237e-01 3.45683665e-01 1.54268274e-01\n",
      "  2.56879642e-01 2.05735859e-01 1.02611372e-01 2.05667871e-01]]\n",
      "(array(['non-woody', 'woody'], dtype=object), array([1840,   95], dtype=int64))\n",
      "\n",
      "[[1241    5]\n",
      " [ 599   90]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody      0.674     0.996     0.804      1246\n",
      "       woody      0.947     0.131     0.230       689\n",
      "\n",
      "    accuracy                          0.688      1935\n",
      "   macro avg      0.811     0.563     0.517      1935\n",
      "weighted avg      0.772     0.688     0.600      1935\n",
      "\n",
      "---Run time is 0.05037129999982426 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsh3146\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:715: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "#Next, use 50% of data for training and 50% for test\n",
    "\n",
    "start_time = timeit.default_timer() \n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "model_qda = qda.fit(x_train, y_train)\n",
    "print(model_qda.priors_)\n",
    "print()\n",
    "print(model_qda.means_)\n",
    "\n",
    "pred2=model_qda.predict(x_test)\n",
    "print(np.unique(pred2, return_counts=True))\n",
    "print()\n",
    "print(confusion_matrix(y_test, pred2))\n",
    "print()\n",
    "print(classification_report(y_test, pred2, digits=3))\n",
    "elapsed = timeit.default_timer() - start_time #gives total computation time\n",
    "print(\"---Run time is %s seconds ---\" % elapsed) #prints computation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy = 68.8%\n",
    "### Sensitivity = 99.6%\n",
    "### Specificity = 13.1%\n",
    "### runtime = 0.0503 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 0.05059780000010505 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.5679723502304147, 0.5645161290322581, 0.5956221198156681, 0.5253456221198156, 0.5732410611303345]\n",
      "\n",
      "Avg accuracy : \n",
      "0.5653394564656982\n",
      "\n",
      "Std of accuracy : \n",
      "0.022744633825008704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-woody       0.53      0.95      0.68      2088\n",
      "       woody       0.83      0.20      0.33      2251\n",
      "\n",
      "    accuracy                           0.57      4339\n",
      "   macro avg       0.68      0.58      0.50      4339\n",
      "weighted avg       0.68      0.57      0.50      4339\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsh3146\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:715: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\tsh3146\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:715: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\tsh3146\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:715: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\tsh3146\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:715: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\tsh3146\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:715: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "output_pred = [];\n",
    "output_ID = [];\n",
    "\n",
    "for train_index , test_index in kf.split(df):\n",
    "    #print(train_index); ## to check the training index\n",
    "    #print(test_index); ## to check the testing index\n",
    "    #print(); print();\n",
    "    \n",
    "    X_train , X_test, X_full = df.iloc[train_index, 5:-1], df.iloc[test_index, 5:-1], df.iloc[test_index, :-1]\n",
    "    y_train , y_test = df.iloc[train_index,-1], df.iloc[test_index,-1]\n",
    "    \n",
    "    output_ID.append(X_full['Id'].tolist())\n",
    "    \n",
    "    #print([X_train.shape, y_train.shape]);\n",
    "    #print([X_test.shape, y_test.shape]);\n",
    "    \n",
    "    qda.fit(X_train,y_train)\n",
    "    pred_values = qda.predict(X_test)\n",
    "    \n",
    "    output_pred.append(pred_values.tolist())\n",
    "     \n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "    \n",
    "    Truth.extend(y_test.values.reshape(y_test.shape[0])); ## it is a list \n",
    "    \n",
    "elapsed = timeit.default_timer() - start_time #gives total computation time\n",
    "print(\"---Run time is %s seconds ---\" % elapsed) #prints computation time\n",
    "print()\n",
    "\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print()\n",
    "print('Avg accuracy : \\n{}'.format(np.mean(acc_score))); \n",
    "print()\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "\n",
    "output_IDFlat = [item for sublist in output_ID for item in sublist]\n",
    "output_predFlat = [item for sublist in output_pred for item in sublist]\n",
    "\n",
    "predictionDF = pd.DataFrame(list(zip(output_IDFlat, output_predFlat)), columns = ['ID', 'Prediction'])\n",
    "predictionDF.to_csv('QDA_pred.csv')\n",
    "\n",
    "Truth = np.asarray(Truth)  ## or np.array(Truth)\n",
    "Output = np.asarray(output_predFlat)\n",
    "\n",
    "np.column_stack((Truth, Output))\n",
    "\n",
    "print(classification_report(Truth, Output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
