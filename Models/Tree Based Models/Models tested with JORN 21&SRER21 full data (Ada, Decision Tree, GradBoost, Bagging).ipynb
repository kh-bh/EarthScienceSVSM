{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61fab92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LMS-Khatrib\\AppData\\Local\\Temp\\ipykernel_8340\\641145125.py:13: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('retina')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 70 #display 70 dpi in Jupyter Notebook, may consider100 dpi \n",
    "plt.rcParams['savefig.dpi'] = 300 #define 300 dpi for saving figures\n",
    "\n",
    "import seaborn as sns\n",
    "## here are some settings \n",
    "sns.set_style('whitegrid')\n",
    "sns.set(rc={\"figure.dpi\":70, 'savefig.dpi':300}) #defining dpi setting\n",
    "sns.set_context('notebook')\n",
    "sns.set_style(\"ticks\")\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "# Tells matplotlib to display images inline instead of a new window\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "random.seed(1000)\n",
    "\n",
    "from time import time\n",
    "import timeit #imports timeit module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60565229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import neighbors\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4e77a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2017 = pd.read_csv('JORN21_dataset_v1.csv', na_values = '?').dropna()\n",
    "df2021 = pd.read_csv(\"SRER21_dataset_v1.csv\", na_values = '?').dropna()\n",
    "\n",
    "df2017 = df2017.reindex(columns=[\"OID_\", \"Id\", \"gridcode\",\"Shape_Length\", \"Shape_Area\",\"CH_mean\", \"ARVI_mean\",\"ARVI_max\",\"ARVI_med\",\"EVI_mean\",\"EVI_max\",\"EVI_med\",\"NDVI_mean\",\"NDVI_max\",\"NDVI_med\",\"SAVI_mean\",\"SAVI_max\",\"SAVI_med\", \"Veg_class\"])\n",
    "df2021 = df2021.reindex(columns=[\"OID_\", \"Id\", \"gridcode\",\"Shape_Length\", \"Shape_Area\",\"CH_mean\", \"ARVI_mean\",\"ARVI_max\",\"ARVI_med\",\"EVI_mean\",\"EVI_max\",\"EVI_med\",\"NDVI_mean\",\"NDVI_max\",\"NDVI_med\",\"SAVI_mean\",\"SAVI_max\",\"SAVI_med\", \"Veg_class\"])\n",
    "\n",
    "bigtest1_df = pd.read_csv('SRER21_pred.csv', na_values='?')\n",
    "bg1 = bigtest1_df.drop(columns=[\"Veg_class\"])\n",
    "\n",
    "dffull2021 = bg1.dropna()\n",
    "dffull2021 = dffull2021.reindex(columns = [\"OID_\", \"Id\", \"gridcode\",\"Shape_Length\", \"Shape_Area\",\"CH_mean\", \"ARVI_mean\",\"ARVI_max\",\"ARVI_med\",\"EVI_mean\",\"EVI_max\",\"EVI_med\",\"NDVI_mean\",\"NDVI_max\",\"NDVI_med\",\"SAVI_mean\",\"SAVI_max\",\"SAVI_med\", \"Veg_class\"]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a4aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2017.Veg_class = df2017.Veg_class.map({'non-woody':0, 'woody':1})\n",
    "df2021.Veg_class = df2021.Veg_class.map({'non-woody':0, 'woody':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1d0ff3",
   "metadata": {},
   "source": [
    "# Predicting models (JORN 2021 -> SRER 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d114f3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 7.8061879 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.8298368298368298, 0.8317757009345794, 0.794392523364486, 0.8294392523364486, 0.794392523364486]\n",
      "Avg accuracy: 0.815967365967366\n",
      "Std of accuracy : \n",
      "0.01763351848884137\n",
      "\n",
      "[[ 498  200]\n",
      " [ 194 1249]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.71      0.72       698\n",
      "           1       0.86      0.87      0.86      1443\n",
      "\n",
      "    accuracy                           0.82      2141\n",
      "   macro avg       0.79      0.79      0.79      2141\n",
      "weighted avg       0.82      0.82      0.82      2141\n",
      "\n",
      "Sensitivity: 0.8655578655578655\n",
      "Specificity: 0.7134670487106017\n",
      "precision: 0.7196531791907514\n",
      "f1_score: 0.7165467625899281\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "start_time = 0\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47fe89a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.953224043715847\n",
      "Specificity: 0.22331838565022422\n",
      "precision: 0.699438202247191\n",
      "f1_score: 0.33854520734194427\n",
      "[0.7140337986774431]\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8476ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a852b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "woodyarea = 0\n",
    "nonwoodyarea = 0 \n",
    "totalarea = 0\n",
    "index = -1\n",
    "woody = 0\n",
    "nw = 0\n",
    "#print(bigtest_df.iat[index,4])\n",
    "area = bigtest_df[\"Shape_Area\"]\n",
    "#print(area)\n",
    "\n",
    "for i in bigtest_df[\"Veg_class\"]:\n",
    "    index += 1\n",
    "    if i == 1:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        woodyarea += bigtest_df.iat[index,4]\n",
    "        #print(woodyarea)\n",
    "        woody += 1\n",
    "    if i == 0:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        nonwoodyarea += bigtest_df.iat[index,4]\n",
    "        #print(nonwoodyarea)\n",
    "        nw += 1\n",
    "\n",
    "FWCdt = woodyarea / totalarea * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "316335f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 52.17706 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.8344988344988346, 0.8387850467289719, 0.8411214953271028, 0.8247663551401869, 0.8341121495327103]\n",
      "Avg accuracy: 0.8346567762455612\n",
      "Std of accuracy : \n",
      "0.005603163454349531\n",
      "\n",
      "[[ 496  202]\n",
      " [ 152 1291]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.71      0.74       698\n",
      "           1       0.86      0.89      0.88      1443\n",
      "\n",
      "    accuracy                           0.83      2141\n",
      "   macro avg       0.82      0.80      0.81      2141\n",
      "weighted avg       0.83      0.83      0.83      2141\n",
      "\n",
      "Sensitivity: 0.8946638946638946\n",
      "Specificity: 0.7106017191977078\n",
      "precision: 0.7654320987654321\n",
      "f1_score: 0.736998514115899\n"
     ]
    }
   ],
   "source": [
    "#Bagging Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf689da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9628415300546448\n",
      "Specificity: 0.1681614349775785\n",
      "precision: 0.6880733944954128\n",
      "f1_score: 0.2702702702702703\n",
      "[0.7024246877296105]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "108049c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99693572",
   "metadata": {},
   "outputs": [],
   "source": [
    "woodyarea = 0\n",
    "nonwoodyarea = 0 \n",
    "totalarea = 0\n",
    "index = -1\n",
    "woody = 0\n",
    "nw = 0\n",
    "#print(bigtest_df.iat[index,4])\n",
    "area = bigtest_df[\"Shape_Area\"]\n",
    "#print(area)\n",
    "\n",
    "for i in bigtest_df[\"Veg_class\"]:\n",
    "    index += 1\n",
    "    if i == 1:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        woodyarea += bigtest_df.iat[index,4]\n",
    "        #print(woodyarea)\n",
    "        woody += 1\n",
    "    if i == 0:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        nonwoodyarea += bigtest_df.iat[index,4]\n",
    "        #print(nonwoodyarea)\n",
    "        nw += 1\n",
    "\n",
    "FWCbag = woodyarea / totalarea * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6aea812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 110.7476753 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.717948717948718, 0.7102803738317757, 0.7079439252336449, 0.7079439252336449, 0.6915887850467289]\n",
      "Avg accuracy: 0.7071411454589025\n",
      "Std of accuracy : \n",
      "0.008600269798364259\n",
      "\n",
      "[[ 173  569]\n",
      " [  58 1341]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.23      0.36       742\n",
      "           1       0.70      0.96      0.81      1399\n",
      "\n",
      "    accuracy                           0.71      2141\n",
      "   macro avg       0.73      0.60      0.58      2141\n",
      "weighted avg       0.72      0.71      0.65      2141\n",
      "\n",
      "Sensitivity: 0.958541815582559\n",
      "Specificity: 0.23315363881401618\n",
      "precision: 0.7489177489177489\n",
      "f1_score: 0.355601233299075\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a5b2aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9753005464480874\n",
      "Specificity: 0.1780269058295964\n",
      "precision: 0.7784313725490196\n",
      "f1_score: 0.28978102189781024\n",
      "[0.7140337986774431]\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1228e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98162533",
   "metadata": {},
   "outputs": [],
   "source": [
    "woodyarea = 0\n",
    "nonwoodyarea = 0 \n",
    "totalarea = 0\n",
    "index = -1\n",
    "woody = 0\n",
    "nw = 0\n",
    "#print(bigtest_df.iat[index,4])\n",
    "area = bigtest_df[\"Shape_Area\"]\n",
    "#print(area)\n",
    "\n",
    "for i in bigtest_df[\"Veg_class\"]:\n",
    "    index += 1\n",
    "    if i == 1:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        woodyarea += bigtest_df.iat[index,4]\n",
    "        #print(woodyarea)\n",
    "        woody += 1\n",
    "    if i == 0:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        nonwoodyarea += bigtest_df.iat[index,4]\n",
    "        #print(nonwoodyarea)\n",
    "        nw += 1\n",
    "\n",
    "FWCada = woodyarea / totalarea * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6b5bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 289.9464748 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.8298368298368298, 0.8387850467289719, 0.8411214953271028, 0.8341121495327103, 0.8200934579439252]\n",
      "Avg accuracy: 0.8327897958739079\n",
      "Std of accuracy : \n",
      "0.007443465228519795\n",
      "\n",
      "[[ 499  199]\n",
      " [ 159 1284]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.71      0.74       698\n",
      "           1       0.87      0.89      0.88      1443\n",
      "\n",
      "    accuracy                           0.83      2141\n",
      "   macro avg       0.81      0.80      0.81      2141\n",
      "weighted avg       0.83      0.83      0.83      2141\n",
      "\n",
      "Sensitivity: 0.8898128898128899\n",
      "Specificity: 0.7148997134670487\n",
      "precision: 0.7583586626139818\n",
      "f1_score: 0.7359882005899705\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58a5ebc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9315846994535519\n",
      "Specificity: 0.257847533632287\n",
      "precision: 0.6475225225225225\n",
      "f1_score: 0.36882617062219375\n",
      "Accuracy: [0.710800881704629]\n"
     ]
    }
   ],
   "source": [
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7008074",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f217fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "woodyarea = 0\n",
    "nonwoodyarea = 0 \n",
    "totalarea = 0\n",
    "index = -1\n",
    "woody = 0\n",
    "nw = 0\n",
    "#print(bigtest_df.iat[index,4])\n",
    "area = bigtest_df[\"Shape_Area\"]\n",
    "#print(area)\n",
    "\n",
    "for i in bigtest_df[\"Veg_class\"]:\n",
    "    index += 1\n",
    "    if i == 1:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        woodyarea += bigtest_df.iat[index,4]\n",
    "        #print(woodyarea)\n",
    "        woody += 1\n",
    "    if i == 0:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        nonwoodyarea += bigtest_df.iat[index,4]\n",
    "        #print(nonwoodyarea)\n",
    "        nw += 1\n",
    "\n",
    "FWCgrad = woodyarea / totalarea * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9daab4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.87153289926091\n",
      "87.38374729095261\n",
      "86.7726733842285\n",
      "80.42383647059538\n"
     ]
    }
   ],
   "source": [
    "print(FWCada)\n",
    "print(FWCdt)\n",
    "print(FWCbag)\n",
    "print(FWCgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b6e1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
