{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61fab92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LMS-Khatrib\\AppData\\Local\\Temp\\ipykernel_11832\\641145125.py:13: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('retina')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 70 #display 70 dpi in Jupyter Notebook, may consider100 dpi \n",
    "plt.rcParams['savefig.dpi'] = 300 #define 300 dpi for saving figures\n",
    "\n",
    "import seaborn as sns\n",
    "## here are some settings \n",
    "sns.set_style('whitegrid')\n",
    "sns.set(rc={\"figure.dpi\":70, 'savefig.dpi':300}) #defining dpi setting\n",
    "sns.set_context('notebook')\n",
    "sns.set_style(\"ticks\")\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "# Tells matplotlib to display images inline instead of a new window\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "random.seed(1000)\n",
    "\n",
    "from time import time\n",
    "import timeit #imports timeit module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60565229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import neighbors\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4e77a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2017 = pd.read_csv('JORN17_dataset_v2.csv', na_values = '?').dropna()\n",
    "df2021 = pd.read_csv(\"SRER_2017_training_bi.csv\", na_values = '?').dropna()\n",
    "\n",
    "df2017 = df2017.reindex(columns=[\"OID_\", \"Id\", \"gridcode\",\"Shape_Length\", \"Shape_Area\",\"CH_mean\", \"ARVI_mean\",\"ARVI_max\",\"ARVI_med\",\"EVI_mean\",\"EVI_max\",\"EVI_med\",\"NDVI_mean\",\"NDVI_max\",\"NDVI_med\",\"SAVI_mean\",\"SAVI_max\",\"SAVI_med\", \"Veg_class\"])\n",
    "df2021 = df2021.reindex(columns=[\"OID_\", \"Id\", \"gridcode\",\"Shape_Length\", \"Shape_Area\",\"CH_mean\", \"ARVI_mean\",\"ARVI_max\",\"ARVI_med\",\"EVI_mean\",\"EVI_max\",\"EVI_med\",\"NDVI_mean\",\"NDVI_max\",\"NDVI_med\",\"SAVI_mean\",\"SAVI_max\",\"SAVI_med\", \"Veg_class\"])\n",
    "\n",
    "bigtest1_df = pd.read_csv('SRER17_pred.csv', na_values='?')\n",
    "bg1 = bigtest1_df.drop(columns=[\"Veg_class\"])\n",
    "\n",
    "dffull2021 = bg1.dropna()\n",
    "dffull2021 = dffull2021.reindex(columns = [\"OID_\", \"Id\", \"gridcode\",\"Shape_Length\", \"Shape_Area\",\"CH_mean\", \"ARVI_mean\",\"ARVI_max\",\"ARVI_med\",\"EVI_mean\",\"EVI_max\",\"EVI_med\",\"NDVI_mean\",\"NDVI_max\",\"NDVI_med\",\"SAVI_mean\",\"SAVI_max\",\"SAVI_med\", \"Veg_class\"]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a4aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2017.Veg_class = df2017.Veg_class.map({'non-woody':0, 'woody':1})\n",
    "df2021.Veg_class = df2021.Veg_class.map({'non-woody':0, 'woody':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179effde",
   "metadata": {},
   "source": [
    "# Predicting models (JORN 2017 -> SRER 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d114f3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 7.463667 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7660377358490567, 0.7811320754716982, 0.7509433962264151, 0.7377358490566037, 0.7504725897920604]\n",
      "Avg accuracy: 0.7572643292791669\n",
      "Std of accuracy : \n",
      "0.014927058858816143\n",
      "\n",
      "[[ 615  362]\n",
      " [ 281 1391]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.63      0.66       977\n",
      "           1       0.79      0.83      0.81      1672\n",
      "\n",
      "    accuracy                           0.76      2649\n",
      "   macro avg       0.74      0.73      0.73      2649\n",
      "weighted avg       0.75      0.76      0.75      2649\n",
      "\n",
      "Sensitivity: 0.8319377990430622\n",
      "Specificity: 0.6294779938587513\n",
      "precision: 0.6863839285714286\n",
      "f1_score: 0.6567004805125468\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "start_time = 0\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47fe89a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.44247001332741004\n",
      "Specificity: 0.5842911877394636\n",
      "precision: 0.49292929292929294\n",
      "f1_score: 0.5347359193513039\n",
      "[0.5107167550126758]\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8476ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a852b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "woodyarea = 0\n",
    "nonwoodyarea = 0 \n",
    "totalarea = 0\n",
    "index = -1\n",
    "woody = 0\n",
    "nw = 0\n",
    "#print(bigtest_df.iat[index,4])\n",
    "area = bigtest_df[\"Shape_Area\"]\n",
    "#print(area)\n",
    "\n",
    "for i in bigtest_df[\"Veg_class\"]:\n",
    "    index += 1\n",
    "    if i == 1:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        woodyarea += bigtest_df.iat[index,4]\n",
    "        #print(woodyarea)\n",
    "        woody += 1\n",
    "    if i == 0:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        nonwoodyarea += bigtest_df.iat[index,4]\n",
    "        #print(nonwoodyarea)\n",
    "        nw += 1\n",
    "\n",
    "FWCdt = woodyarea / totalarea * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "316335f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 49.2327722 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7622641509433963, 0.7943396226415095, 0.7716981132075472, 0.7396226415094339, 0.7844990548204159]\n",
      "Avg accuracy: 0.7704847166244606\n",
      "Std of accuracy : \n",
      "0.018904898454860507\n",
      "\n",
      "[[ 621  356]\n",
      " [ 252 1420]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.64      0.67       977\n",
      "           1       0.80      0.85      0.82      1672\n",
      "\n",
      "    accuracy                           0.77      2649\n",
      "   macro avg       0.76      0.74      0.75      2649\n",
      "weighted avg       0.77      0.77      0.77      2649\n",
      "\n",
      "Sensitivity: 0.8492822966507177\n",
      "Specificity: 0.6356192425793245\n",
      "precision: 0.711340206185567\n",
      "f1_score: 0.6713513513513514\n"
     ]
    }
   ],
   "source": [
    "#Bagging Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf689da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.5117725455353176\n",
      "Specificity: 0.5828544061302682\n",
      "precision: 0.5254749568221071\n",
      "f1_score: 0.552679382379655\n",
      "[0.5459783360221249]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "108049c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99693572",
   "metadata": {},
   "outputs": [],
   "source": [
    "woodyarea = 0\n",
    "nonwoodyarea = 0 \n",
    "totalarea = 0\n",
    "index = -1\n",
    "woody = 0\n",
    "nw = 0\n",
    "#print(bigtest_df.iat[index,4])\n",
    "area = bigtest_df[\"Shape_Area\"]\n",
    "#print(area)\n",
    "\n",
    "for i in bigtest_df[\"Veg_class\"]:\n",
    "    index += 1\n",
    "    if i == 1:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        woodyarea += bigtest_df.iat[index,4]\n",
    "        #print(woodyarea)\n",
    "        woody += 1\n",
    "    if i == 0:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        nonwoodyarea += bigtest_df.iat[index,4]\n",
    "        #print(nonwoodyarea)\n",
    "        nw += 1\n",
    "\n",
    "FWCbag = woodyarea / totalarea * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6aea812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 107.2099883 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.5433962264150943, 0.5, 0.5245283018867924, 0.5811320754716981, 0.5916824196597353]\n",
      "Avg accuracy: 0.548147804686664\n",
      "Std of accuracy : \n",
      "0.034298379917433876\n",
      "\n",
      "[[1034  319]\n",
      " [ 878  418]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.76      0.63      1353\n",
      "           1       0.57      0.32      0.41      1296\n",
      "\n",
      "    accuracy                           0.55      2649\n",
      "   macro avg       0.55      0.54      0.52      2649\n",
      "weighted avg       0.55      0.55      0.52      2649\n",
      "\n",
      "Sensitivity: 0.32253086419753085\n",
      "Specificity: 0.7642276422764228\n",
      "precision: 0.5407949790794979\n",
      "f1_score: 0.6333843797856049\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a5b2aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.42425588627276767\n",
      "Specificity: 0.7576628352490421\n",
      "precision: 0.5496872828353023\n",
      "f1_score: 0.6371325010068465\n",
      "[0.5846969347775985]\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1228e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98162533",
   "metadata": {},
   "outputs": [],
   "source": [
    "woodyarea = 0\n",
    "nonwoodyarea = 0 \n",
    "totalarea = 0\n",
    "index = -1\n",
    "woody = 0\n",
    "nw = 0\n",
    "#print(bigtest_df.iat[index,4])\n",
    "area = bigtest_df[\"Shape_Area\"]\n",
    "#print(area)\n",
    "\n",
    "for i in bigtest_df[\"Veg_class\"]:\n",
    "    index += 1\n",
    "    if i == 1:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        woodyarea += bigtest_df.iat[index,4]\n",
    "        #print(woodyarea)\n",
    "        woody += 1\n",
    "    if i == 0:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        nonwoodyarea += bigtest_df.iat[index,4]\n",
    "        #print(nonwoodyarea)\n",
    "        nw += 1\n",
    "\n",
    "FWCada = woodyarea / totalarea * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6b5bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 276.6527788 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7566037735849057, 0.7905660377358491, 0.7754716981132076, 0.7641509433962265, 0.7939508506616257]\n",
      "Avg accuracy: 0.776148660698363\n",
      "Std of accuracy : \n",
      "0.014499550608281621\n",
      "\n",
      "[[ 635  342]\n",
      " [ 251 1421]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.65      0.68       977\n",
      "           1       0.81      0.85      0.83      1672\n",
      "\n",
      "    accuracy                           0.78      2649\n",
      "   macro avg       0.76      0.75      0.75      2649\n",
      "weighted avg       0.77      0.78      0.77      2649\n",
      "\n",
      "Sensitivity: 0.8498803827751196\n",
      "Specificity: 0.6499488229273286\n",
      "precision: 0.7167042889390519\n",
      "f1_score: 0.6816961889425658\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58a5ebc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.5579742336739227\n",
      "Specificity: 0.5617816091954023\n",
      "precision: 0.5410516605166051\n",
      "f1_score: 0.5512218045112782\n",
      "Accuracy: [0.5598064070062226]\n"
     ]
    }
   ],
   "source": [
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7008074",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f217fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "woodyarea = 0\n",
    "nonwoodyarea = 0 \n",
    "totalarea = 0\n",
    "index = -1\n",
    "woody = 0\n",
    "nw = 0\n",
    "#print(bigtest_df.iat[index,4])\n",
    "area = bigtest_df[\"Shape_Area\"]\n",
    "#print(area)\n",
    "\n",
    "for i in bigtest_df[\"Veg_class\"]:\n",
    "    index += 1\n",
    "    if i == 1:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        woodyarea += bigtest_df.iat[index,4]\n",
    "        #print(woodyarea)\n",
    "        woody += 1\n",
    "    if i == 0:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        nonwoodyarea += bigtest_df.iat[index,4]\n",
    "        #print(nonwoodyarea)\n",
    "        nw += 1\n",
    "\n",
    "FWCgrad = woodyarea / totalarea * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9daab4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.2199732755323\n",
      "40.84432868999988\n",
      "45.62229219966271\n",
      "43.53936885781361\n"
     ]
    }
   ],
   "source": [
    "print(FWCada)\n",
    "print(FWCdt)\n",
    "print(FWCbag)\n",
    "print(FWCgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b6e1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
