{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61fab92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LMS-Khatrib\\AppData\\Local\\Temp\\ipykernel_5540\\641145125.py:13: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('retina')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 70 #display 70 dpi in Jupyter Notebook, may consider100 dpi \n",
    "plt.rcParams['savefig.dpi'] = 300 #define 300 dpi for saving figures\n",
    "\n",
    "import seaborn as sns\n",
    "## here are some settings \n",
    "sns.set_style('whitegrid')\n",
    "sns.set(rc={\"figure.dpi\":70, 'savefig.dpi':300}) #defining dpi setting\n",
    "sns.set_context('notebook')\n",
    "sns.set_style(\"ticks\")\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "# Tells matplotlib to display images inline instead of a new window\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "random.seed(1000)\n",
    "\n",
    "from time import time\n",
    "import timeit #imports timeit module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60565229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import neighbors\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4e77a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2017 = pd.read_csv('SRER_2017_training_bi.csv', na_values = '?').dropna()\n",
    "df2021 = pd.read_csv(\"SRER21_dataset_v1.csv\", na_values = '?').dropna()\n",
    "\n",
    "df2017 = df2017.reindex(columns=[\"OID_\", \"Id\", \"gridcode\",\"Shape_Length\", \"Shape_Area\",\"CH_mean\", \"ARVI_mean\",\"ARVI_max\",\"ARVI_med\",\"EVI_mean\",\"EVI_max\",\"EVI_med\",\"NDVI_mean\",\"NDVI_max\",\"NDVI_med\",\"SAVI_mean\",\"SAVI_max\",\"SAVI_med\", \"Veg_class\"])\n",
    "df2021 = df2021.reindex(columns=[\"OID_\", \"Id\", \"gridcode\",\"Shape_Length\", \"Shape_Area\",\"CH_mean\", \"ARVI_mean\",\"ARVI_max\",\"ARVI_med\",\"EVI_mean\",\"EVI_max\",\"EVI_med\",\"NDVI_mean\",\"NDVI_max\",\"NDVI_med\",\"SAVI_mean\",\"SAVI_max\",\"SAVI_med\", \"Veg_class\"])\n",
    "\n",
    "bigtest1_df = pd.read_csv('SRER21_pred.csv', na_values='?')\n",
    "bg1 = bigtest1_df.drop(columns=[\"Veg_class\"])\n",
    "\n",
    "dffull2021 = bg1.dropna()\n",
    "dffull2021 = dffull2021.reindex(columns = [\"OID_\", \"Id\", \"gridcode\",\"Shape_Length\", \"Shape_Area\",\"CH_mean\", \"ARVI_mean\",\"ARVI_max\",\"ARVI_med\",\"EVI_mean\",\"EVI_max\",\"EVI_med\",\"NDVI_mean\",\"NDVI_max\",\"NDVI_med\",\"SAVI_mean\",\"SAVI_max\",\"SAVI_med\", \"Veg_class\"]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b31ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2017.Veg_class = df2017.Veg_class.map({'non-woody':0, 'woody':1})\n",
    "df2021.Veg_class = df2021.Veg_class.map({'non-woody':0, 'woody':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eb4b92",
   "metadata": {},
   "source": [
    "# Predicting models (SRER 2017 -> SRER 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f02f2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 6.825481 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6682027649769585, 0.6831797235023042, 0.6612903225806451, 0.6900921658986175, 0.6758938869665513]\n",
      "Avg accuracy: 0.6757317727850153\n",
      "Std of accuracy : \n",
      "0.010266066275368545\n",
      "\n",
      "[[1428  660]\n",
      " [ 747 1504]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67      2088\n",
      "           1       0.70      0.67      0.68      2251\n",
      "\n",
      "    accuracy                           0.68      4339\n",
      "   macro avg       0.68      0.68      0.68      4339\n",
      "weighted avg       0.68      0.68      0.68      4339\n",
      "\n",
      "Sensitivity: 0.6681474900044425\n",
      "Specificity: 0.6839080459770115\n",
      "precision: 0.6565517241379311\n",
      "f1_score: 0.6699507389162561\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "start_time = 0\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64599ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9766120218579235\n",
      "Specificity: 0.11748878923766816\n",
      "precision: 0.7100271002710027\n",
      "f1_score: 0.20161600615621394\n",
      "[0.695077149155033]\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0313d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cdf56d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "woodyarea = 0\n",
    "nonwoodyarea = 0 \n",
    "totalarea = 0\n",
    "index = -1\n",
    "woody = 0\n",
    "nw = 0\n",
    "#print(bigtest_df.iat[index,4])\n",
    "area = bigtest_df[\"Shape_Area\"]\n",
    "#print(area)\n",
    "\n",
    "for i in bigtest_df[\"Veg_class\"]:\n",
    "    index += 1\n",
    "    if i == 1:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        woodyarea += bigtest_df.iat[index,4]\n",
    "        #print(woodyarea)\n",
    "        woody += 1\n",
    "    if i == 0:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        nonwoodyarea += bigtest_df.iat[index,4]\n",
    "        #print(nonwoodyarea)\n",
    "        nw += 1\n",
    "\n",
    "FWCdt = woodyarea / totalarea * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a7300e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 55.2374446 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6670506912442397, 0.6728110599078341, 0.6670506912442397, 0.6970046082949308, 0.6597462514417531]\n",
      "Avg accuracy: 0.6727326604265995\n",
      "Std of accuracy : \n",
      "0.012824585495850723\n",
      "\n",
      "[[1422  666]\n",
      " [ 754 1497]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.68      0.67      2088\n",
      "           1       0.69      0.67      0.68      2251\n",
      "\n",
      "    accuracy                           0.67      4339\n",
      "   macro avg       0.67      0.67      0.67      4339\n",
      "weighted avg       0.67      0.67      0.67      4339\n",
      "\n",
      "Sensitivity: 0.6650377609951132\n",
      "Specificity: 0.6810344827586207\n",
      "precision: 0.6534926470588235\n",
      "f1_score: 0.6669793621013133\n"
     ]
    }
   ],
   "source": [
    "#Bagging Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc15c480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9283060109289617\n",
      "Specificity: 0.4094170403587444\n",
      "precision: 0.7356970185334408\n",
      "f1_score: 0.526073177758571\n",
      "[0.7582659808963997]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6489e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bce92ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "woodyarea = 0\n",
    "nonwoodyarea = 0 \n",
    "totalarea = 0\n",
    "index = -1\n",
    "woody = 0\n",
    "nw = 0\n",
    "#print(bigtest_df.iat[index,4])\n",
    "area = bigtest_df[\"Shape_Area\"]\n",
    "#print(area)\n",
    "\n",
    "for i in bigtest_df[\"Veg_class\"]:\n",
    "    index += 1\n",
    "    if i == 1:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        woodyarea += bigtest_df.iat[index,4]\n",
    "        #print(woodyarea)\n",
    "        woody += 1\n",
    "    if i == 0:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        nonwoodyarea += bigtest_df.iat[index,4]\n",
    "        #print(nonwoodyarea)\n",
    "        nw += 1\n",
    "\n",
    "FWCbag = woodyarea / totalarea * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8a8e1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 119.5633313 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7373271889400922, 0.706221198156682, 0.7142857142857143, 0.7016129032258065, 0.7093425605536332]\n",
      "Avg accuracy: 0.7137579130323857\n",
      "Std of accuracy : \n",
      "0.012486697023695197\n",
      "\n",
      "[[ 296 1163]\n",
      " [  79 2801]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.20      0.32      1459\n",
      "           1       0.71      0.97      0.82      2880\n",
      "\n",
      "    accuracy                           0.71      4339\n",
      "   macro avg       0.75      0.59      0.57      4339\n",
      "weighted avg       0.73      0.71      0.65      4339\n",
      "\n",
      "Sensitivity: 0.9725694444444445\n",
      "Specificity: 0.20287868403015763\n",
      "precision: 0.7893333333333333\n",
      "f1_score: 0.32279171210468915\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a63ddaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9626229508196721\n",
      "Specificity: 0.21255605381165918\n",
      "precision: 0.7348837209302326\n",
      "f1_score: 0.3297391304347826\n",
      "[0.7168258633357825]\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85b51eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e256239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "woodyarea = 0\n",
    "nonwoodyarea = 0 \n",
    "totalarea = 0\n",
    "index = -1\n",
    "woody = 0\n",
    "nw = 0\n",
    "#print(bigtest_df.iat[index,4])\n",
    "area = bigtest_df[\"Shape_Area\"]\n",
    "#print(area)\n",
    "\n",
    "for i in bigtest_df[\"Veg_class\"]:\n",
    "    index += 1\n",
    "    if i == 1:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        woodyarea += bigtest_df.iat[index,4]\n",
    "        #print(woodyarea)\n",
    "        woody += 1\n",
    "    if i == 0:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        nonwoodyarea += bigtest_df.iat[index,4]\n",
    "        #print(nonwoodyarea)\n",
    "        nw += 1\n",
    "\n",
    "FWCada = woodyarea / totalarea * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b4220f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 307.5700605 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6532258064516129, 0.6797235023041475, 0.6716589861751152, 0.684331797235023, 0.6597462514417531]\n",
      "Avg accuracy: 0.6697372687215303\n",
      "Std of accuracy : \n",
      "0.011737690865284093\n",
      "\n",
      "[[1374  714]\n",
      " [ 719 1532]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66      2088\n",
      "           1       0.68      0.68      0.68      2251\n",
      "\n",
      "    accuracy                           0.67      4339\n",
      "   macro avg       0.67      0.67      0.67      4339\n",
      "weighted avg       0.67      0.67      0.67      4339\n",
      "\n",
      "Sensitivity: 0.6805864060417592\n",
      "Specificity: 0.6580459770114943\n",
      "precision: 0.6564739608217869\n",
      "f1_score: 0.657259028940445\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7814bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9191256830601093\n",
      "Specificity: 0.4748878923766816\n",
      "precision: 0.7410776766969909\n",
      "f1_score: 0.5788466794206067\n",
      "Accuracy: [0.773548861131521]\n"
     ]
    }
   ],
   "source": [
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fb888fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e739323",
   "metadata": {},
   "outputs": [],
   "source": [
    "woodyarea = 0\n",
    "nonwoodyarea = 0 \n",
    "totalarea = 0\n",
    "index = -1\n",
    "woody = 0\n",
    "nw = 0\n",
    "#print(bigtest_df.iat[index,4])\n",
    "area = bigtest_df[\"Shape_Area\"]\n",
    "#print(area)\n",
    "\n",
    "for i in bigtest_df[\"Veg_class\"]:\n",
    "    index += 1\n",
    "    if i == 1:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        woodyarea += bigtest_df.iat[index,4]\n",
    "        #print(woodyarea)\n",
    "        woody += 1\n",
    "    if i == 0:\n",
    "        totalarea += bigtest_df.iat[index,4]\n",
    "        nonwoodyarea += bigtest_df.iat[index,4]\n",
    "        #print(nonwoodyarea)\n",
    "        nw += 1\n",
    "\n",
    "FWCgrad = woodyarea / totalarea * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb066049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.2572034159111\n",
      "81.06374687393047\n",
      "64.67877324216532\n",
      "60.67088099818383\n"
     ]
    }
   ],
   "source": [
    "print(FWCada)\n",
    "print(FWCdt)\n",
    "print(FWCbag)\n",
    "print(FWCgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aaf9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
