{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "---Run time is 0.0002511999999992298 seconds ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhoom\\AppData\\Local\\Temp/ipykernel_65524/2171782275.py:13: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('retina')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 70 #display 70 dpi in Jupyter Notebook, may consider100 dpi \n",
    "plt.rcParams['savefig.dpi'] = 300 #define 300 dpi for saving figures\n",
    "\n",
    "import seaborn as sns\n",
    "## here are some settings \n",
    "sns.set_style('whitegrid')\n",
    "sns.set(rc={\"figure.dpi\":70, 'savefig.dpi':300}) #defining dpi setting\n",
    "sns.set_context('notebook')\n",
    "sns.set_style(\"ticks\")\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "# Tells matplotlib to display images inline instead of a new window\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "random.seed(1000)\n",
    "\n",
    "from time import time\n",
    "import timeit #imports timeit module\n",
    "start_time = timeit.default_timer() #defines start time so computational time can be calculated\n",
    "print(\"Hello World\")\n",
    "elapsed = timeit.default_timer() - start_time #gives total computation time\n",
    "print(\"---Run time is %s seconds ---\" % elapsed) #prints computation time\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import neighbors\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhoom\\AppData\\Local\\Temp/ipykernel_65524/404817301.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dffull2021[\"Veg_class\"] = \"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OID__x</th>\n",
       "      <th>Id</th>\n",
       "      <th>gridcode</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>CH_mean</th>\n",
       "      <th>ARVI_mean</th>\n",
       "      <th>ARVI_max</th>\n",
       "      <th>ARVI_med</th>\n",
       "      <th>EVI_mean</th>\n",
       "      <th>EVI_max</th>\n",
       "      <th>EVI_med</th>\n",
       "      <th>NDVI_mean</th>\n",
       "      <th>NDVI_max</th>\n",
       "      <th>NDVI_med</th>\n",
       "      <th>SAVI_mean</th>\n",
       "      <th>SAVI_max</th>\n",
       "      <th>SAVI_med</th>\n",
       "      <th>Veg_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.2</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.053054</td>\n",
       "      <td>0.252378</td>\n",
       "      <td>-0.007871</td>\n",
       "      <td>0.145961</td>\n",
       "      <td>0.234673</td>\n",
       "      <td>0.126273</td>\n",
       "      <td>0.221155</td>\n",
       "      <td>0.400501</td>\n",
       "      <td>0.170450</td>\n",
       "      <td>0.145235</td>\n",
       "      <td>0.234261</td>\n",
       "      <td>0.125838</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>5.73</td>\n",
       "      <td>0.016248</td>\n",
       "      <td>0.040314</td>\n",
       "      <td>0.252378</td>\n",
       "      <td>0.043301</td>\n",
       "      <td>0.141544</td>\n",
       "      <td>0.234673</td>\n",
       "      <td>0.135954</td>\n",
       "      <td>0.206992</td>\n",
       "      <td>0.400501</td>\n",
       "      <td>0.207240</td>\n",
       "      <td>0.139659</td>\n",
       "      <td>0.234261</td>\n",
       "      <td>0.134476</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.014410</td>\n",
       "      <td>0.111847</td>\n",
       "      <td>0.281741</td>\n",
       "      <td>0.085010</td>\n",
       "      <td>0.167371</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.152276</td>\n",
       "      <td>0.274226</td>\n",
       "      <td>0.432882</td>\n",
       "      <td>0.249348</td>\n",
       "      <td>0.167777</td>\n",
       "      <td>0.255698</td>\n",
       "      <td>0.152786</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.012281</td>\n",
       "      <td>0.204984</td>\n",
       "      <td>0.310943</td>\n",
       "      <td>0.281741</td>\n",
       "      <td>0.211967</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.361578</td>\n",
       "      <td>0.438509</td>\n",
       "      <td>0.432882</td>\n",
       "      <td>0.213450</td>\n",
       "      <td>0.255698</td>\n",
       "      <td>0.255698</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>18.2</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.260015</td>\n",
       "      <td>0.365155</td>\n",
       "      <td>0.256816</td>\n",
       "      <td>0.203891</td>\n",
       "      <td>0.232604</td>\n",
       "      <td>0.209083</td>\n",
       "      <td>0.403924</td>\n",
       "      <td>0.488291</td>\n",
       "      <td>0.410121</td>\n",
       "      <td>0.207537</td>\n",
       "      <td>0.236474</td>\n",
       "      <td>0.212997</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657503</th>\n",
       "      <td>1657605</td>\n",
       "      <td>1657605</td>\n",
       "      <td>1657605</td>\n",
       "      <td>9.4</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.014932</td>\n",
       "      <td>0.509040</td>\n",
       "      <td>0.536940</td>\n",
       "      <td>0.506748</td>\n",
       "      <td>0.382288</td>\n",
       "      <td>0.407742</td>\n",
       "      <td>0.407359</td>\n",
       "      <td>0.584744</td>\n",
       "      <td>0.606661</td>\n",
       "      <td>0.589247</td>\n",
       "      <td>0.353369</td>\n",
       "      <td>0.377064</td>\n",
       "      <td>0.373630</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657504</th>\n",
       "      <td>1657606</td>\n",
       "      <td>1657606</td>\n",
       "      <td>1657606</td>\n",
       "      <td>7.2</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.194732</td>\n",
       "      <td>0.493816</td>\n",
       "      <td>0.545701</td>\n",
       "      <td>0.531150</td>\n",
       "      <td>0.355797</td>\n",
       "      <td>0.390464</td>\n",
       "      <td>0.351225</td>\n",
       "      <td>0.571676</td>\n",
       "      <td>0.602668</td>\n",
       "      <td>0.582336</td>\n",
       "      <td>0.332901</td>\n",
       "      <td>0.363158</td>\n",
       "      <td>0.328131</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657505</th>\n",
       "      <td>1657607</td>\n",
       "      <td>1657607</td>\n",
       "      <td>1657607</td>\n",
       "      <td>10.2</td>\n",
       "      <td>2.59</td>\n",
       "      <td>0.035261</td>\n",
       "      <td>0.525481</td>\n",
       "      <td>0.641269</td>\n",
       "      <td>0.543615</td>\n",
       "      <td>0.453275</td>\n",
       "      <td>0.507084</td>\n",
       "      <td>0.464525</td>\n",
       "      <td>0.607242</td>\n",
       "      <td>0.687645</td>\n",
       "      <td>0.619722</td>\n",
       "      <td>0.410453</td>\n",
       "      <td>0.451535</td>\n",
       "      <td>0.418655</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657506</th>\n",
       "      <td>1657608</td>\n",
       "      <td>1657608</td>\n",
       "      <td>1657608</td>\n",
       "      <td>11.8</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.847143</td>\n",
       "      <td>0.529820</td>\n",
       "      <td>0.610390</td>\n",
       "      <td>0.529278</td>\n",
       "      <td>0.417614</td>\n",
       "      <td>0.479161</td>\n",
       "      <td>0.392868</td>\n",
       "      <td>0.604037</td>\n",
       "      <td>0.665525</td>\n",
       "      <td>0.598336</td>\n",
       "      <td>0.381633</td>\n",
       "      <td>0.430885</td>\n",
       "      <td>0.361104</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657507</th>\n",
       "      <td>1657609</td>\n",
       "      <td>1657609</td>\n",
       "      <td>1657609</td>\n",
       "      <td>12.8</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.013516</td>\n",
       "      <td>0.440997</td>\n",
       "      <td>0.530145</td>\n",
       "      <td>0.452717</td>\n",
       "      <td>0.368150</td>\n",
       "      <td>0.433651</td>\n",
       "      <td>0.362634</td>\n",
       "      <td>0.529237</td>\n",
       "      <td>0.605752</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.336447</td>\n",
       "      <td>0.393824</td>\n",
       "      <td>0.332581</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1657508 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          OID__x       Id  gridcode  Shape_Length  Shape_Area   CH_mean  \\\n",
       "0              1        1         1          14.2        5.04  0.010000   \n",
       "1              2        2         2          26.2        5.73  0.016248   \n",
       "2              3        3         3          10.6        2.29  0.014410   \n",
       "3              4        4         4          11.0        2.85  0.012281   \n",
       "4              5        5         5          18.2        3.78  0.012011   \n",
       "...          ...      ...       ...           ...         ...       ...   \n",
       "1657503  1657605  1657605   1657605           9.4        2.21  0.014932   \n",
       "1657504  1657606  1657606   1657606           7.2        2.07  1.194732   \n",
       "1657505  1657607  1657607   1657607          10.2        2.59  0.035261   \n",
       "1657506  1657608  1657608   1657608          11.8        2.90  0.847143   \n",
       "1657507  1657609  1657609   1657609          12.8        2.19  0.013516   \n",
       "\n",
       "         ARVI_mean  ARVI_max  ARVI_med  EVI_mean   EVI_max   EVI_med  \\\n",
       "0         0.053054  0.252378 -0.007871  0.145961  0.234673  0.126273   \n",
       "1         0.040314  0.252378  0.043301  0.141544  0.234673  0.135954   \n",
       "2         0.111847  0.281741  0.085010  0.167371  0.254378  0.152276   \n",
       "3         0.204984  0.310943  0.281741  0.211967  0.254378  0.254378   \n",
       "4         0.260015  0.365155  0.256816  0.203891  0.232604  0.209083   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "1657503   0.509040  0.536940  0.506748  0.382288  0.407742  0.407359   \n",
       "1657504   0.493816  0.545701  0.531150  0.355797  0.390464  0.351225   \n",
       "1657505   0.525481  0.641269  0.543615  0.453275  0.507084  0.464525   \n",
       "1657506   0.529820  0.610390  0.529278  0.417614  0.479161  0.392868   \n",
       "1657507   0.440997  0.530145  0.452717  0.368150  0.433651  0.362634   \n",
       "\n",
       "         NDVI_mean  NDVI_max  NDVI_med  SAVI_mean  SAVI_max  SAVI_med  \\\n",
       "0         0.221155  0.400501  0.170450   0.145235  0.234261  0.125838   \n",
       "1         0.206992  0.400501  0.207240   0.139659  0.234261  0.134476   \n",
       "2         0.274226  0.432882  0.249348   0.167777  0.255698  0.152786   \n",
       "3         0.361578  0.438509  0.432882   0.213450  0.255698  0.255698   \n",
       "4         0.403924  0.488291  0.410121   0.207537  0.236474  0.212997   \n",
       "...            ...       ...       ...        ...       ...       ...   \n",
       "1657503   0.584744  0.606661  0.589247   0.353369  0.377064  0.373630   \n",
       "1657504   0.571676  0.602668  0.582336   0.332901  0.363158  0.328131   \n",
       "1657505   0.607242  0.687645  0.619722   0.410453  0.451535  0.418655   \n",
       "1657506   0.604037  0.665525  0.598336   0.381633  0.430885  0.361104   \n",
       "1657507   0.529237  0.605752  0.528302   0.336447  0.393824  0.332581   \n",
       "\n",
       "        Veg_class  \n",
       "0                  \n",
       "1                  \n",
       "2                  \n",
       "3                  \n",
       "4                  \n",
       "...           ...  \n",
       "1657503            \n",
       "1657504            \n",
       "1657505            \n",
       "1657506            \n",
       "1657507            \n",
       "\n",
       "[1657508 rows x 19 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2017 = pd.read_csv('SRER_2017_training_bi.csv', na_values = '?').dropna()\n",
    "df2021 = pd.read_csv(\"SRER21_dataset_v1.csv\", na_values = '?').dropna()\n",
    "dfJornada2017 = pd.read_csv(\"JORN17_dataset_v2.csv\", na_values = '?').dropna()\n",
    "dfJornada2021 = pd.read_csv(\"JORN21_dataset_v1.csv\", na_values = '?').dropna()\n",
    "dfJornada2017[\"Year\"] = '2017'\n",
    "dfJornada2021[\"Year\"] = '2021'\n",
    "dfJornada2017 = dfJornada2017.reindex(columns=['OID_','Id', 'gridcode','Shape_Length','Shape_Area', 'CH_mean', 'ARVI_max', 'ARVI_mean', 'ARVI_med', 'EVI_max', 'EVI_mean', 'EVI_med', 'NDVI_max', 'NDVI_mean', 'NDVI_med', 'SAVI_max', 'SAVI_mean', 'SAVI_med', 'Year', 'Veg_class'])\n",
    "dfJornada2021 = dfJornada2021.reindex(columns=['OID_','Id', 'gridcode','Shape_Length','Shape_Area', 'CH_mean', 'ARVI_max', 'ARVI_mean', 'ARVI_med', 'EVI_max', 'EVI_mean', 'EVI_med', 'NDVI_max', 'NDVI_mean', 'NDVI_med', 'SAVI_max', 'SAVI_mean', 'SAVI_med', 'Year', 'Veg_class'])\n",
    "dffull2017_ = pd.read_csv(\"SRER17_pred.csv\", na_values = '?')\n",
    "del dffull2017_[\"Veg_class\"]\n",
    "dffull2017 = dffull2017_.dropna()\n",
    "dffull2017[\"Veg_class\"] = \"\"\n",
    "\n",
    "dffull2021_ = pd.read_csv(\"SRER21_pred.csv\", na_values = '?')\n",
    "del dffull2021_[\"Veg_class\"]\n",
    "dffull2021 = dffull2021_.dropna()\n",
    "dffull2021[\"Veg_class\"] = \"\"\n",
    "\n",
    "\n",
    "#dfCombined = dfCombined.reindex(columns=['OID_','Id', 'gridcode','Shape_Length','Shape_Area', 'CH_mean', 'ARVI_max', 'ARVI_mean', 'ARVI_med', 'EVI_max', 'EVI_mean', 'EVI_med', 'NDVI_max', 'NDVI_mean', 'NDVI_med', 'SAVI_max', 'SAVI_mean', 'SAVI_med', 'Year', 'Veg_class'])\n",
    "\n",
    "\n",
    "frames = [dfJornada2017,dfJornada2021]\n",
    "\n",
    "#dfCombined = pd.concat(frames)\n",
    "#dfCombined = dfCombined.reindex(columns=['OID_','Id', 'gridcode','Shape_Length','Shape_Area', 'CH_mean', 'ARVI_max', 'ARVI_mean', 'ARVI_med', 'EVI_max', 'EVI_mean', 'EVI_med', 'NDVI_max', 'NDVI_mean', 'NDVI_med', 'SAVI_max', 'SAVI_mean', 'SAVI_med', 'Year', 'Veg_class'])\n",
    "\n",
    "dffull2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OID_</th>\n",
       "      <th>Id</th>\n",
       "      <th>gridcode</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>CH_mean</th>\n",
       "      <th>ARVI_max</th>\n",
       "      <th>ARVI_mean</th>\n",
       "      <th>ARVI_med</th>\n",
       "      <th>EVI_max</th>\n",
       "      <th>EVI_mean</th>\n",
       "      <th>EVI_med</th>\n",
       "      <th>NDVI_max</th>\n",
       "      <th>NDVI_mean</th>\n",
       "      <th>NDVI_med</th>\n",
       "      <th>SAVI_max</th>\n",
       "      <th>SAVI_mean</th>\n",
       "      <th>SAVI_med</th>\n",
       "      <th>Veg_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>41.6</td>\n",
       "      <td>17.41</td>\n",
       "      <td>0.190714</td>\n",
       "      <td>0.609099</td>\n",
       "      <td>0.344183</td>\n",
       "      <td>0.379428</td>\n",
       "      <td>0.443455</td>\n",
       "      <td>0.292047</td>\n",
       "      <td>0.290985</td>\n",
       "      <td>0.665698</td>\n",
       "      <td>0.476709</td>\n",
       "      <td>0.507514</td>\n",
       "      <td>0.406991</td>\n",
       "      <td>0.286203</td>\n",
       "      <td>0.288351</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>31.4</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.245301</td>\n",
       "      <td>0.274936</td>\n",
       "      <td>0.332059</td>\n",
       "      <td>0.252919</td>\n",
       "      <td>0.244477</td>\n",
       "      <td>0.526375</td>\n",
       "      <td>0.406745</td>\n",
       "      <td>0.433791</td>\n",
       "      <td>0.328585</td>\n",
       "      <td>0.254814</td>\n",
       "      <td>0.248126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>33.4</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>0.026805</td>\n",
       "      <td>0.150550</td>\n",
       "      <td>0.131985</td>\n",
       "      <td>0.134891</td>\n",
       "      <td>0.208300</td>\n",
       "      <td>0.181575</td>\n",
       "      <td>0.205117</td>\n",
       "      <td>0.150184</td>\n",
       "      <td>0.131642</td>\n",
       "      <td>0.135063</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>32.2</td>\n",
       "      <td>14.20</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.189842</td>\n",
       "      <td>0.033418</td>\n",
       "      <td>0.018961</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>0.141579</td>\n",
       "      <td>0.138338</td>\n",
       "      <td>0.341480</td>\n",
       "      <td>0.207390</td>\n",
       "      <td>0.197327</td>\n",
       "      <td>0.197330</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>0.138725</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>28.8</td>\n",
       "      <td>10.73</td>\n",
       "      <td>0.013750</td>\n",
       "      <td>0.322694</td>\n",
       "      <td>0.080315</td>\n",
       "      <td>0.061453</td>\n",
       "      <td>0.248442</td>\n",
       "      <td>0.158710</td>\n",
       "      <td>0.142326</td>\n",
       "      <td>0.457103</td>\n",
       "      <td>0.247391</td>\n",
       "      <td>0.228112</td>\n",
       "      <td>0.248598</td>\n",
       "      <td>0.158618</td>\n",
       "      <td>0.142817</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4334</th>\n",
       "      <td>4335</td>\n",
       "      <td>4487</td>\n",
       "      <td>4487</td>\n",
       "      <td>8.8</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.290182</td>\n",
       "      <td>0.290182</td>\n",
       "      <td>0.290182</td>\n",
       "      <td>0.283959</td>\n",
       "      <td>0.283959</td>\n",
       "      <td>0.283959</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.269486</td>\n",
       "      <td>0.269486</td>\n",
       "      <td>0.269486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4335</th>\n",
       "      <td>4336</td>\n",
       "      <td>4488</td>\n",
       "      <td>4488</td>\n",
       "      <td>10.8</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.408320</td>\n",
       "      <td>0.408320</td>\n",
       "      <td>0.408320</td>\n",
       "      <td>0.372648</td>\n",
       "      <td>0.372648</td>\n",
       "      <td>0.372648</td>\n",
       "      <td>0.510469</td>\n",
       "      <td>0.510469</td>\n",
       "      <td>0.510469</td>\n",
       "      <td>0.340879</td>\n",
       "      <td>0.340879</td>\n",
       "      <td>0.340879</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4336</th>\n",
       "      <td>4337</td>\n",
       "      <td>4489</td>\n",
       "      <td>4489</td>\n",
       "      <td>11.8</td>\n",
       "      <td>2.62</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.437195</td>\n",
       "      <td>0.413297</td>\n",
       "      <td>0.413297</td>\n",
       "      <td>0.235606</td>\n",
       "      <td>0.234926</td>\n",
       "      <td>0.234926</td>\n",
       "      <td>0.499074</td>\n",
       "      <td>0.485601</td>\n",
       "      <td>0.485601</td>\n",
       "      <td>0.227606</td>\n",
       "      <td>0.226722</td>\n",
       "      <td>0.226722</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4337</th>\n",
       "      <td>4338</td>\n",
       "      <td>4490</td>\n",
       "      <td>4490</td>\n",
       "      <td>11.2</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.570094</td>\n",
       "      <td>0.550652</td>\n",
       "      <td>0.550652</td>\n",
       "      <td>0.416298</td>\n",
       "      <td>0.388085</td>\n",
       "      <td>0.388085</td>\n",
       "      <td>0.640722</td>\n",
       "      <td>0.621694</td>\n",
       "      <td>0.621694</td>\n",
       "      <td>0.387576</td>\n",
       "      <td>0.363288</td>\n",
       "      <td>0.363288</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338</th>\n",
       "      <td>4339</td>\n",
       "      <td>4491</td>\n",
       "      <td>4491</td>\n",
       "      <td>14.8</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.487398</td>\n",
       "      <td>0.484593</td>\n",
       "      <td>0.484593</td>\n",
       "      <td>0.286898</td>\n",
       "      <td>0.277861</td>\n",
       "      <td>0.277861</td>\n",
       "      <td>0.543662</td>\n",
       "      <td>0.540401</td>\n",
       "      <td>0.540401</td>\n",
       "      <td>0.270742</td>\n",
       "      <td>0.263390</td>\n",
       "      <td>0.263390</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4339 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      OID_    Id  gridcode  Shape_Length  Shape_Area   CH_mean  ARVI_max  \\\n",
       "0        1     2         2          41.6       17.41  0.190714  0.609099   \n",
       "1        2     3         3          31.4        5.05  0.045000  0.387755   \n",
       "2        3     5         5          33.4        4.84  0.010000  0.038844   \n",
       "3        4     6         6          32.2       14.20  0.011667  0.189842   \n",
       "4        5     7         7          28.8       10.73  0.013750  0.322694   \n",
       "...    ...   ...       ...           ...         ...       ...       ...   \n",
       "4334  4335  4487      4487           8.8        2.30  0.020000  0.290182   \n",
       "4335  4336  4488      4488          10.8        2.03  0.020000  0.408320   \n",
       "4336  4337  4489      4489          11.8        2.62  0.015000  0.437195   \n",
       "4337  4338  4490      4490          11.2        3.09  0.015000  0.570094   \n",
       "4338  4339  4491      4491          14.8        3.33  0.865000  0.487398   \n",
       "\n",
       "      ARVI_mean  ARVI_med   EVI_max  EVI_mean   EVI_med  NDVI_max  NDVI_mean  \\\n",
       "0      0.344183  0.379428  0.443455  0.292047  0.290985  0.665698   0.476709   \n",
       "1      0.245301  0.274936  0.332059  0.252919  0.244477  0.526375   0.406745   \n",
       "2      0.004040  0.026805  0.150550  0.131985  0.134891  0.208300   0.181575   \n",
       "3      0.033418  0.018961  0.198972  0.141579  0.138338  0.341480   0.207390   \n",
       "4      0.080315  0.061453  0.248442  0.158710  0.142326  0.457103   0.247391   \n",
       "...         ...       ...       ...       ...       ...       ...        ...   \n",
       "4334   0.290182  0.290182  0.283959  0.283959  0.283959  0.419200   0.419200   \n",
       "4335   0.408320  0.408320  0.372648  0.372648  0.372648  0.510469   0.510469   \n",
       "4336   0.413297  0.413297  0.235606  0.234926  0.234926  0.499074   0.485601   \n",
       "4337   0.550652  0.550652  0.416298  0.388085  0.388085  0.640722   0.621694   \n",
       "4338   0.484593  0.484593  0.286898  0.277861  0.277861  0.543662   0.540401   \n",
       "\n",
       "      NDVI_med  SAVI_max  SAVI_mean  SAVI_med  Veg_class  \n",
       "0     0.507514  0.406991   0.286203  0.288351          1  \n",
       "1     0.433791  0.328585   0.254814  0.248126          0  \n",
       "2     0.205117  0.150184   0.131642  0.135063          0  \n",
       "3     0.197327  0.197330   0.141500  0.138725          0  \n",
       "4     0.228112  0.248598   0.158618  0.142817          0  \n",
       "...        ...       ...        ...       ...        ...  \n",
       "4334  0.419200  0.269486   0.269486  0.269486          1  \n",
       "4335  0.510469  0.340879   0.340879  0.340879          1  \n",
       "4336  0.485601  0.227606   0.226722  0.226722          1  \n",
       "4337  0.621694  0.387576   0.363288  0.363288          1  \n",
       "4338  0.540401  0.270742   0.263390  0.263390          1  \n",
       "\n",
       "[4339 rows x 19 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2017.Veg_class = df2017.Veg_class.map({'non-woody':0, 'woody':1})\n",
    "df2021.Veg_class = df2021.Veg_class.map({'non-woody':0, 'woody':1})\n",
    "dfJornada2017.Veg_class = dfJornada2017.Veg_class.map({'non-woody':0, 'woody':1})\n",
    "dfJornada2021.Veg_class = dfJornada2021.Veg_class.map({'non-woody':0, 'woody':1})\n",
    "#dfCombined.Veg_class = dfCombined.Veg_class.map({'non-woody':0, 'woody':1})\n",
    "df2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2649, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfJornada2017.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2017 Jorn (Training) to 2021 Jorn (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 22.9375967 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7482517482517482, 0.6658878504672897, 0.7686915887850467, 0.7429906542056075, 0.764018691588785]\n",
      "Avg accuracy: 0.7379681066596955\n",
      "Std of accuracy : \n",
      "0.03728031096989959\n",
      "\n",
      "[[ 364  334]\n",
      " [ 227 1216]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.52      0.56       698\n",
      "           1       0.78      0.84      0.81      1443\n",
      "\n",
      "    accuracy                           0.74      2141\n",
      "   macro avg       0.70      0.68      0.69      2141\n",
      "weighted avg       0.73      0.74      0.73      2141\n",
      "\n",
      "Sensitivity: 0.5214899713467048\n",
      "Specificity: 0.8426888426888427\n",
      "Precision: 0.6159052453468697\n",
      "F1_Score: 0.5647788983708301\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 34.365592299999996 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7855477855477856, 0.735981308411215, 0.7780373831775701, 0.7336448598130841, 0.7920560747663551]\n",
      "Avg accuracy: 0.765053482343202\n",
      "Std of accuracy : \n",
      "0.0250975337778841\n",
      "\n",
      "[[ 334  364]\n",
      " [ 139 1304]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.48      0.57       698\n",
      "           1       0.78      0.90      0.84      1443\n",
      "\n",
      "    accuracy                           0.77      2141\n",
      "   macro avg       0.74      0.69      0.70      2141\n",
      "weighted avg       0.76      0.77      0.75      2141\n",
      "\n",
      "Sensitivity: 0.4785100286532951\n",
      "Specificity: 0.9036729036729036\n",
      "Precision: 0.7061310782241015\n",
      "F1_Score: 0.5704526046114433\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 57.25102699999999 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.8111888111888111, 0.7593457943925234, 0.8037383177570093, 0.7780373831775701, 0.7990654205607477]\n",
      "Avg accuracy: 0.7902751454153323\n",
      "Std of accuracy : \n",
      "0.018985757514509215\n",
      "\n",
      "[[ 341  357]\n",
      " [  92 1351]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.49      0.60       698\n",
      "           1       0.79      0.94      0.86      1443\n",
      "\n",
      "    accuracy                           0.79      2141\n",
      "   macro avg       0.79      0.71      0.73      2141\n",
      "weighted avg       0.79      0.79      0.77      2141\n",
      "\n",
      "Sensitivity: 0.4885386819484241\n",
      "Specificity: 0.9362439362439362\n",
      "Precision: 0.7875288683602771\n",
      "F1_Score: 0.6030061892130858\n"
     ]
    }
   ],
   "source": [
    "#Ada boost\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 105.3876154 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.8041958041958042, 0.7593457943925234, 0.7873831775700935, 0.764018691588785, 0.780373831775701]\n",
      "Avg accuracy: 0.7790634599045815\n",
      "Std of accuracy : \n",
      "0.016233920616834412\n",
      "\n",
      "[[ 359  339]\n",
      " [ 134 1309]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.51      0.60       698\n",
      "           1       0.79      0.91      0.85      1443\n",
      "\n",
      "    accuracy                           0.78      2141\n",
      "   macro avg       0.76      0.71      0.72      2141\n",
      "weighted avg       0.77      0.78      0.77      2141\n",
      "\n",
      "Sensitivity: 0.5143266475644699\n",
      "Specificity: 0.9071379071379071\n",
      "Precision: 0.7281947261663286\n",
      "F1_Score: 0.6028547439126785\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2021 Jorn (Training) to 2017 Jorn (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 105.64114409999999 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6806526806526807, 0.75, 0.7242990654205608, 0.7242990654205608, 0.6915887850467289]\n",
      "Avg accuracy: 0.7141679193081062\n",
      "Std of accuracy : \n",
      "0.024989224781643882\n",
      "\n",
      "[[ 440  215]\n",
      " [ 397 1089]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.67      0.59       655\n",
      "           1       0.84      0.73      0.78      1486\n",
      "\n",
      "    accuracy                           0.71      2141\n",
      "   macro avg       0.68      0.70      0.69      2141\n",
      "weighted avg       0.74      0.71      0.72      2141\n",
      "\n",
      "Sensitivity: 0.6717557251908397\n",
      "Specificity: 0.7328398384925976\n",
      "Precision: 0.5256869772998806\n",
      "F1_Score: 0.5898123324396783\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 116.3169541 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6853146853146853, 0.7336448598130841, 0.7523364485981309, 0.6845794392523364, 0.7172897196261683]\n",
      "Avg accuracy: 0.714633030520881\n",
      "Std of accuracy : \n",
      "0.026656478006339523\n",
      "\n",
      "[[ 429  226]\n",
      " [ 385 1101]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.65      0.58       655\n",
      "           1       0.83      0.74      0.78      1486\n",
      "\n",
      "    accuracy                           0.71      2141\n",
      "   macro avg       0.68      0.70      0.68      2141\n",
      "weighted avg       0.74      0.71      0.72      2141\n",
      "\n",
      "Sensitivity: 0.6549618320610687\n",
      "Specificity: 0.7409152086137282\n",
      "Precision: 0.527027027027027\n",
      "F1_Score: 0.5840707964601769\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 138.5860517 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6853146853146853, 0.6985981308411215, 0.7429906542056075, 0.719626168224299, 0.7079439252336449]\n",
      "Avg accuracy: 0.7108947127638717\n",
      "Std of accuracy : \n",
      "0.019599176767514173\n",
      "\n",
      "[[ 422  233]\n",
      " [ 386 1100]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.64      0.58       655\n",
      "           1       0.83      0.74      0.78      1486\n",
      "\n",
      "    accuracy                           0.71      2141\n",
      "   macro avg       0.67      0.69      0.68      2141\n",
      "weighted avg       0.73      0.71      0.72      2141\n",
      "\n",
      "Sensitivity: 0.6442748091603053\n",
      "Specificity: 0.7402422611036339\n",
      "Precision: 0.5222772277227723\n",
      "F1_Score: 0.5768967874231032\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 186.88258050000002 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.8041958041958042, 0.7593457943925234, 0.7873831775700935, 0.764018691588785, 0.780373831775701]\n",
      "Avg accuracy: 0.7790634599045815\n",
      "Std of accuracy : \n",
      "0.016233920616834412\n",
      "\n",
      "[[ 359  339]\n",
      " [ 134 1309]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.51      0.60       698\n",
      "           1       0.79      0.91      0.85      1443\n",
      "\n",
      "    accuracy                           0.78      2141\n",
      "   macro avg       0.76      0.71      0.72      2141\n",
      "weighted avg       0.77      0.78      0.77      2141\n",
      "\n",
      "Sensitivity: 0.5143266475644699\n",
      "Specificity: 0.9071379071379071\n",
      "Precision: 0.7281947261663286\n",
      "F1_Score: 0.6028547439126785\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2017 SRER (Training) to 2021 Jorn (Testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 187.17310500000002 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7365967365967366, 0.7336448598130841, 0.780373831775701, 0.6448598130841121, 0.7219626168224299]\n",
      "Avg accuracy: 0.7234875716184128\n",
      "Std of accuracy : \n",
      "0.04403570642975829\n",
      "\n",
      "[[ 281  417]\n",
      " [ 175 1268]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.40      0.49       698\n",
      "           1       0.75      0.88      0.81      1443\n",
      "\n",
      "    accuracy                           0.72      2141\n",
      "   macro avg       0.68      0.64      0.65      2141\n",
      "weighted avg       0.71      0.72      0.71      2141\n",
      "\n",
      "Sensitivity: 0.40257879656160456\n",
      "Specificity: 0.8787248787248787\n",
      "Precision: 0.6162280701754386\n",
      "F1_Score: 0.487001733102253\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6) #criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 199.6698207 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.7342657342657343, 0.7570093457943925, 0.7266355140186916, 0.7616822429906542, 0.7873831775700935]\n",
      "Avg accuracy: 0.7533952029279132\n",
      "Std of accuracy : \n",
      "0.021535723826952683\n",
      "\n",
      "[[ 370  328]\n",
      " [ 200 1243]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.53      0.58       698\n",
      "           1       0.79      0.86      0.82      1443\n",
      "\n",
      "    accuracy                           0.75      2141\n",
      "   macro avg       0.72      0.70      0.70      2141\n",
      "weighted avg       0.74      0.75      0.75      2141\n",
      "\n",
      "Sensitivity: 0.5300859598853869\n",
      "Specificity: 0.8613998613998614\n",
      "Precision: 0.6491228070175439\n",
      "F1_Score: 0.583596214511041\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 222.3140997 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.682983682983683, 0.6915887850467289, 0.6845794392523364, 0.6658878504672897, 0.6845794392523364]\n",
      "Avg accuracy: 0.681923839400475\n",
      "Std of accuracy : \n",
      "0.008553266101058287\n",
      "\n",
      "[[ 187  511]\n",
      " [ 170 1273]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.27      0.35       698\n",
      "           1       0.71      0.88      0.79      1443\n",
      "\n",
      "    accuracy                           0.68      2141\n",
      "   macro avg       0.62      0.58      0.57      2141\n",
      "weighted avg       0.65      0.68      0.65      2141\n",
      "\n",
      "Sensitivity: 0.2679083094555874\n",
      "Specificity: 0.8821898821898821\n",
      "Precision: 0.5238095238095238\n",
      "F1_Score: 0.35450236966824644\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 270.14525799999996 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.703962703962704, 0.7686915887850467, 0.6915887850467289, 0.7383177570093458, 0.7850467289719626]\n",
      "Avg accuracy: 0.7375215127551576\n",
      "Std of accuracy : \n",
      "0.035963489662101965\n",
      "\n",
      "[[ 355  343]\n",
      " [ 219 1224]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.51      0.56       698\n",
      "           1       0.78      0.85      0.81      1443\n",
      "\n",
      "    accuracy                           0.74      2141\n",
      "   macro avg       0.70      0.68      0.69      2141\n",
      "weighted avg       0.73      0.74      0.73      2141\n",
      "\n",
      "Sensitivity: 0.5085959885386819\n",
      "Specificity: 0.8482328482328483\n",
      "Precision: 0.6184668989547039\n",
      "F1_Score: 0.5581761006289307\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2021 SRER (Training) to 2021 Jorn (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 270.4010278 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.627039627039627, 0.633177570093458, 0.40654205607476634, 0.4532710280373832, 0.43457943925233644]\n",
      "Avg accuracy: 0.5109219440995142\n",
      "Std of accuracy : \n",
      "0.09846493951834304\n",
      "\n",
      "[[514 184]\n",
      " [863 580]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.74      0.50       698\n",
      "           1       0.76      0.40      0.53      1443\n",
      "\n",
      "    accuracy                           0.51      2141\n",
      "   macro avg       0.57      0.57      0.51      2141\n",
      "weighted avg       0.63      0.51      0.52      2141\n",
      "\n",
      "Sensitivity: 0.7363896848137536\n",
      "Specificity: 0.4019404019404019\n",
      "Precision: 0.3732752360203341\n",
      "F1_Score: 0.495421686746988\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6) #criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 279.49725689999997 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.44988344988344986, 0.35046728971962615, 0.4322429906542056, 0.3621495327102804, 0.3808411214953271]\n",
      "Avg accuracy: 0.3951168768925778\n",
      "Std of accuracy : \n",
      "0.03914576464834235\n",
      "\n",
      "[[317 381]\n",
      " [914 529]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.45      0.33       698\n",
      "           1       0.58      0.37      0.45      1443\n",
      "\n",
      "    accuracy                           0.40      2141\n",
      "   macro avg       0.42      0.41      0.39      2141\n",
      "weighted avg       0.48      0.40      0.41      2141\n",
      "\n",
      "Sensitivity: 0.45415472779369626\n",
      "Specificity: 0.3665973665973666\n",
      "Precision: 0.25751421608448416\n",
      "F1_Score: 0.3286677034733022\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 301.8356234 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.46853146853146854, 0.43457943925233644, 0.4462616822429907, 0.441588785046729, 0.39953271028037385]\n",
      "Avg accuracy: 0.4380988170707797\n",
      "Std of accuracy : \n",
      "0.022381640388579885\n",
      "\n",
      "[[ 503  195]\n",
      " [1008  435]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.72      0.46       698\n",
      "           1       0.69      0.30      0.42      1443\n",
      "\n",
      "    accuracy                           0.44      2141\n",
      "   macro avg       0.51      0.51      0.44      2141\n",
      "weighted avg       0.57      0.44      0.43      2141\n",
      "\n",
      "Sensitivity: 0.7206303724928367\n",
      "Specificity: 0.30145530145530147\n",
      "Precision: 0.3328921244209133\n",
      "F1_Score: 0.4554096876414667\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 347.1202823 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.47086247086247085, 0.3691588785046729, 0.37383177570093457, 0.4322429906542056, 0.5186915887850467]\n",
      "Avg accuracy: 0.43295754090146615\n",
      "Std of accuracy : \n",
      "0.057190465460733954\n",
      "\n",
      "[[333 365]\n",
      " [849 594]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.48      0.35       698\n",
      "           1       0.62      0.41      0.49      1443\n",
      "\n",
      "    accuracy                           0.43      2141\n",
      "   macro avg       0.45      0.44      0.42      2141\n",
      "weighted avg       0.51      0.43      0.45      2141\n",
      "\n",
      "Sensitivity: 0.47707736389684813\n",
      "Specificity: 0.41164241164241167\n",
      "Precision: 0.2817258883248731\n",
      "F1_Score: 0.35425531914893615\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2017 SRER TO 2017 JORN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 347.4286442 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.5169811320754717, 0.6169811320754717, 0.6660377358490566, 0.6018867924528302, 0.6691871455576559]\n",
      "Avg accuracy: 0.6142147876020971\n",
      "Std of accuracy : \n",
      "0.055356289610878545\n",
      "\n",
      "[[ 280  697]\n",
      " [ 325 1347]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.29      0.35       977\n",
      "           1       0.66      0.81      0.72      1672\n",
      "\n",
      "    accuracy                           0.61      2649\n",
      "   macro avg       0.56      0.55      0.54      2649\n",
      "weighted avg       0.59      0.61      0.59      2649\n",
      "\n",
      "Sensitivity: 0.2865916069600819\n",
      "Specificity: 0.805622009569378\n",
      "Precision: 0.4628099173553719\n",
      "F1_Score: 0.3539823008849558\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 351.71699509999996 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6679245283018868, 0.6566037735849056, 0.6679245283018868, 0.6849056603773584, 0.7107750472589792]\n",
      "Avg accuracy: 0.6776267075650033\n",
      "Std of accuracy : \n",
      "0.01887868584701041\n",
      "\n",
      "[[ 317  660]\n",
      " [ 194 1478]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.32      0.43       977\n",
      "           1       0.69      0.88      0.78      1672\n",
      "\n",
      "    accuracy                           0.68      2649\n",
      "   macro avg       0.66      0.60      0.60      2649\n",
      "weighted avg       0.67      0.68      0.65      2649\n",
      "\n",
      "Sensitivity: 0.32446264073694986\n",
      "Specificity: 0.8839712918660287\n",
      "Precision: 0.6203522504892368\n",
      "F1_Score: 0.4260752688172043\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 378.9236186 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.5207547169811321, 0.5773584905660377, 0.6150943396226415, 0.6452830188679245, 0.5803402646502835]\n",
      "Avg accuracy: 0.5877661661376039\n",
      "Std of accuracy : \n",
      "0.04173491453395459\n",
      "\n",
      "[[ 238  739]\n",
      " [ 353 1319]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.24      0.30       977\n",
      "           1       0.64      0.79      0.71      1672\n",
      "\n",
      "    accuracy                           0.59      2649\n",
      "   macro avg       0.52      0.52      0.51      2649\n",
      "weighted avg       0.55      0.59      0.56      2649\n",
      "\n",
      "Sensitivity: 0.2436028659160696\n",
      "Specificity: 0.7888755980861244\n",
      "Precision: 0.4027072758037225\n",
      "F1_Score: 0.3035714285714286\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 440.6084022 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.5716981132075472, 0.5584905660377358, 0.6, 0.6396226415094339, 0.6446124763705104]\n",
      "Avg accuracy: 0.6028847594250456\n",
      "Std of accuracy : \n",
      "0.03476391084809911\n",
      "\n",
      "[[ 440  537]\n",
      " [ 515 1157]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.45      0.46       977\n",
      "           1       0.68      0.69      0.69      1672\n",
      "\n",
      "    accuracy                           0.60      2649\n",
      "   macro avg       0.57      0.57      0.57      2649\n",
      "weighted avg       0.60      0.60      0.60      2649\n",
      "\n",
      "Sensitivity: 0.4503582395087001\n",
      "Specificity: 0.6919856459330144\n",
      "Precision: 0.4607329842931937\n",
      "F1_Score: 0.4554865424430642\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfCombined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_65524/1233342704.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mOutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfCombined\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfCombined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dfCombined' is not defined"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfCombined): \n",
    "    \n",
    "    X_train = dfCombined.iloc[train_index, 5:19]\n",
    "    X_test = dfCombined.iloc[test_index, 5:19]\n",
    "    Y_train = dfCombined.iloc[train_index, -1]\n",
    "    Y_test = dfCombined.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfCombined): \n",
    "    \n",
    "    X_train = dfCombined.iloc[train_index, 5:19]\n",
    "    X_test = dfCombined.iloc[test_index, 5:19]\n",
    "    Y_train = dfCombined.iloc[train_index, -1]\n",
    "    Y_test = dfCombined.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_depth=6)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfCombined): \n",
    "    \n",
    "    X_train = dfCombined.iloc[train_index, 5:19]\n",
    "    X_test = dfCombined.iloc[test_index, 5:19]\n",
    "    Y_train = dfCombined.iloc[train_index, -1]\n",
    "    Y_test = dfCombined.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfCombined): \n",
    "    \n",
    "    X_train = dfCombined.iloc[train_index, 5:19]\n",
    "    X_test = dfCombined.iloc[test_index, 5:19]\n",
    "    Y_train = dfCombined.iloc[train_index, -1]\n",
    "    Y_test = dfCombined.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging 2017 -> 2021 SRER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest 2017 -> 2021 SRER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging 2021 -> 2017 SRER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 715.3412887000001 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.4619815668202765, 0.4804147465437788, 0.5023041474654378, 0.4066820276497696, 0.44521337946943484]\n",
      "Avg accuracy: 0.45931917358973956\n",
      "Std of accuracy : \n",
      "0.0324637092013152\n",
      "\n",
      "[[ 413 1675]\n",
      " [ 671 1580]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.20      0.26      2088\n",
      "           1       0.49      0.70      0.57      2251\n",
      "\n",
      "    accuracy                           0.46      4339\n",
      "   macro avg       0.43      0.45      0.42      4339\n",
      "weighted avg       0.44      0.46      0.42      4339\n",
      "\n",
      "0.1977969348659004\n",
      "0.7019102621057308\n",
      "0.38099630996309963\n",
      "0.2604035308953342\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest 2021 -> 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 723.5696395 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.4827188940092166, 0.5357142857142857, 0.5506912442396313, 0.49193548387096775, 0.4867358708189158]\n",
      "Avg accuracy: 0.5095591557306035\n",
      "Std of accuracy : \n",
      "0.028027969326465837\n",
      "\n",
      "[[ 474 1614]\n",
      " [ 514 1737]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.23      0.31      2088\n",
      "           1       0.52      0.77      0.62      2251\n",
      "\n",
      "    accuracy                           0.51      4339\n",
      "   macro avg       0.50      0.50      0.46      4339\n",
      "weighted avg       0.50      0.51      0.47      4339\n",
      "\n",
      "0.22701149425287356\n",
      "0.7716570413149711\n",
      "0.4797570850202429\n",
      "0.3081924577373212\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2021.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2021.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging Combined SRER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfCombined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_65524/2867011076.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mOutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcrossvalidation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfCombined\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfCombined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dfCombined' is not defined"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfCombined): \n",
    "    \n",
    "    X_train = dfCombined.iloc[train_index, 5:18]\n",
    "    X_test = dfCombined.iloc[test_index, 5:18]\n",
    "    Y_train = dfCombined.iloc[train_index, -1]\n",
    "    Y_test = dfCombined.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Models With Same Testing and Validation Sets (SRER 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Models With Same Testing and Validation Sets (Jornada 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    #Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Truth.extend(Y_test.values)\n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting models (SRER 2017 -> SRER 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 796.1033558 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6682027649769585, 0.684331797235023, 0.663594470046083, 0.6912442396313364, 0.671280276816609]\n",
      "Avg accuracy: 0.675730709741202\n",
      "Std of accuracy : \n",
      "0.010377022070475169\n",
      "\n",
      "[[1428  660]\n",
      " [ 747 1504]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67      2088\n",
      "           1       0.70      0.67      0.68      2251\n",
      "\n",
      "    accuracy                           0.68      4339\n",
      "   macro avg       0.68      0.68      0.68      4339\n",
      "weighted avg       0.68      0.68      0.68      4339\n",
      "\n",
      "Sensitivity: 0.6681474900044425\n",
      "Specificity: 0.6839080459770115\n",
      "precision: 0.6565517241379311\n",
      "f1_score: 0.6699507389162561\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9674857284686026\n",
      "Specificity: 0.1092479674796748\n",
      "precision: 0.6213872832369942\n",
      "f1_score: 0.18582541054451168\n",
      "[0.6858429214607303]\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhoom\\AppData\\Local\\Temp/ipykernel_65524/2883311971.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bigtest_df[\"Veg_class\"] = finalPredicted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OID_</th>\n",
       "      <th>Id</th>\n",
       "      <th>gridcode</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>CH_mean</th>\n",
       "      <th>ARVI_mean</th>\n",
       "      <th>ARVI_med</th>\n",
       "      <th>ARVI_max</th>\n",
       "      <th>EVI_mean</th>\n",
       "      <th>EVI_med</th>\n",
       "      <th>EVI_max</th>\n",
       "      <th>NDVI_mean</th>\n",
       "      <th>NDVI_med</th>\n",
       "      <th>NDVI_max</th>\n",
       "      <th>SAVI_mean</th>\n",
       "      <th>SAVI_med</th>\n",
       "      <th>SAVI_max</th>\n",
       "      <th>Veg_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.8</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207123</td>\n",
       "      <td>0.167813</td>\n",
       "      <td>0.335815</td>\n",
       "      <td>0.212394</td>\n",
       "      <td>0.186196</td>\n",
       "      <td>0.291992</td>\n",
       "      <td>0.350170</td>\n",
       "      <td>0.315030</td>\n",
       "      <td>0.463047</td>\n",
       "      <td>0.208236</td>\n",
       "      <td>0.183683</td>\n",
       "      <td>0.282265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.319239</td>\n",
       "      <td>0.334353</td>\n",
       "      <td>0.347863</td>\n",
       "      <td>0.275198</td>\n",
       "      <td>0.279712</td>\n",
       "      <td>0.299364</td>\n",
       "      <td>0.440041</td>\n",
       "      <td>0.453643</td>\n",
       "      <td>0.467307</td>\n",
       "      <td>0.263501</td>\n",
       "      <td>0.269123</td>\n",
       "      <td>0.285889</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3486.2</td>\n",
       "      <td>1942.77</td>\n",
       "      <td>0.120091</td>\n",
       "      <td>0.014902</td>\n",
       "      <td>-0.002058</td>\n",
       "      <td>0.388165</td>\n",
       "      <td>0.155599</td>\n",
       "      <td>0.143094</td>\n",
       "      <td>0.441733</td>\n",
       "      <td>0.199209</td>\n",
       "      <td>0.184422</td>\n",
       "      <td>0.503913</td>\n",
       "      <td>0.155182</td>\n",
       "      <td>0.144957</td>\n",
       "      <td>0.397375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>31.6</td>\n",
       "      <td>9.23</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.419381</td>\n",
       "      <td>0.443782</td>\n",
       "      <td>0.507713</td>\n",
       "      <td>0.344481</td>\n",
       "      <td>0.337613</td>\n",
       "      <td>0.405390</td>\n",
       "      <td>0.527309</td>\n",
       "      <td>0.540501</td>\n",
       "      <td>0.596006</td>\n",
       "      <td>0.326518</td>\n",
       "      <td>0.321309</td>\n",
       "      <td>0.381685</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>31.8</td>\n",
       "      <td>11.82</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.465333</td>\n",
       "      <td>0.440134</td>\n",
       "      <td>0.565654</td>\n",
       "      <td>0.335128</td>\n",
       "      <td>0.329047</td>\n",
       "      <td>0.380148</td>\n",
       "      <td>0.562001</td>\n",
       "      <td>0.551508</td>\n",
       "      <td>0.639035</td>\n",
       "      <td>0.321567</td>\n",
       "      <td>0.314126</td>\n",
       "      <td>0.359091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>10.4</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.480752</td>\n",
       "      <td>0.480752</td>\n",
       "      <td>0.487603</td>\n",
       "      <td>0.471716</td>\n",
       "      <td>0.471716</td>\n",
       "      <td>0.497532</td>\n",
       "      <td>0.587427</td>\n",
       "      <td>0.587427</td>\n",
       "      <td>0.591646</td>\n",
       "      <td>0.430824</td>\n",
       "      <td>0.430824</td>\n",
       "      <td>0.449164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>25.4</td>\n",
       "      <td>5.98</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189495</td>\n",
       "      <td>0.156688</td>\n",
       "      <td>0.420349</td>\n",
       "      <td>0.240643</td>\n",
       "      <td>0.246809</td>\n",
       "      <td>0.319283</td>\n",
       "      <td>0.350102</td>\n",
       "      <td>0.332174</td>\n",
       "      <td>0.515654</td>\n",
       "      <td>0.237516</td>\n",
       "      <td>0.245360</td>\n",
       "      <td>0.305830</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>9.8</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341788</td>\n",
       "      <td>0.341788</td>\n",
       "      <td>0.341788</td>\n",
       "      <td>0.291975</td>\n",
       "      <td>0.291975</td>\n",
       "      <td>0.291975</td>\n",
       "      <td>0.463293</td>\n",
       "      <td>0.463293</td>\n",
       "      <td>0.463293</td>\n",
       "      <td>0.280296</td>\n",
       "      <td>0.280296</td>\n",
       "      <td>0.280296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>19.2</td>\n",
       "      <td>4.90</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.472085</td>\n",
       "      <td>0.420540</td>\n",
       "      <td>0.650954</td>\n",
       "      <td>0.368233</td>\n",
       "      <td>0.320483</td>\n",
       "      <td>0.493067</td>\n",
       "      <td>0.562986</td>\n",
       "      <td>0.515776</td>\n",
       "      <td>0.699278</td>\n",
       "      <td>0.344232</td>\n",
       "      <td>0.306789</td>\n",
       "      <td>0.446165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>95.8</td>\n",
       "      <td>37.74</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150098</td>\n",
       "      <td>0.141256</td>\n",
       "      <td>0.265815</td>\n",
       "      <td>0.253935</td>\n",
       "      <td>0.256579</td>\n",
       "      <td>0.340908</td>\n",
       "      <td>0.310233</td>\n",
       "      <td>0.307031</td>\n",
       "      <td>0.411046</td>\n",
       "      <td>0.240402</td>\n",
       "      <td>0.241444</td>\n",
       "      <td>0.316999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    OID_   Id  gridcode  Shape_Length  Shape_Area   CH_mean  ARVI_mean  \\\n",
       "0      1    1         1          15.8        3.36  0.000000   0.207123   \n",
       "1      2    2         2          24.0        6.93  1.000000   0.319239   \n",
       "2      3    3         3        3486.2     1942.77  0.120091   0.014902   \n",
       "3      4    4         4          31.6        9.23  1.000000   0.419381   \n",
       "4      5    5         5          31.8       11.82  1.000000   0.465333   \n",
       "..   ...  ...       ...           ...         ...       ...        ...   \n",
       "95    96   96        96          10.4        2.31  0.500000   0.480752   \n",
       "96    97   97        97          25.4        5.98  0.000000   0.189495   \n",
       "97    98   98        98           9.8        2.05  0.000000   0.341788   \n",
       "98    99   99        99          19.2        4.90  1.000000   0.472085   \n",
       "99   100  100       100          95.8       37.74  0.000000   0.150098   \n",
       "\n",
       "    ARVI_med  ARVI_max  EVI_mean   EVI_med   EVI_max  NDVI_mean  NDVI_med  \\\n",
       "0   0.167813  0.335815  0.212394  0.186196  0.291992   0.350170  0.315030   \n",
       "1   0.334353  0.347863  0.275198  0.279712  0.299364   0.440041  0.453643   \n",
       "2  -0.002058  0.388165  0.155599  0.143094  0.441733   0.199209  0.184422   \n",
       "3   0.443782  0.507713  0.344481  0.337613  0.405390   0.527309  0.540501   \n",
       "4   0.440134  0.565654  0.335128  0.329047  0.380148   0.562001  0.551508   \n",
       "..       ...       ...       ...       ...       ...        ...       ...   \n",
       "95  0.480752  0.487603  0.471716  0.471716  0.497532   0.587427  0.587427   \n",
       "96  0.156688  0.420349  0.240643  0.246809  0.319283   0.350102  0.332174   \n",
       "97  0.341788  0.341788  0.291975  0.291975  0.291975   0.463293  0.463293   \n",
       "98  0.420540  0.650954  0.368233  0.320483  0.493067   0.562986  0.515776   \n",
       "99  0.141256  0.265815  0.253935  0.256579  0.340908   0.310233  0.307031   \n",
       "\n",
       "    NDVI_max  SAVI_mean  SAVI_med  SAVI_max  Veg_class  \n",
       "0   0.463047   0.208236  0.183683  0.282265          0  \n",
       "1   0.467307   0.263501  0.269123  0.285889          1  \n",
       "2   0.503913   0.155182  0.144957  0.397375          1  \n",
       "3   0.596006   0.326518  0.321309  0.381685          1  \n",
       "4   0.639035   0.321567  0.314126  0.359091          1  \n",
       "..       ...        ...       ...       ...        ...  \n",
       "95  0.591646   0.430824  0.430824  0.449164          1  \n",
       "96  0.515654   0.237516  0.245360  0.305830          1  \n",
       "97  0.463293   0.280296  0.280296  0.280296          1  \n",
       "98  0.699278   0.344232  0.306789  0.446165          1  \n",
       "99  0.411046   0.240402  0.241444  0.316999          1  \n",
       "\n",
       "[100 rows x 19 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 861.0689895 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6682027649769585, 0.6612903225806451, 0.6658986175115207, 0.7027649769585254, 0.6678200692041523]\n",
      "Avg accuracy: 0.6731953502463603\n",
      "Std of accuracy : \n",
      "0.01498769075059182\n",
      "\n",
      "[[1429  659]\n",
      " [ 759 1492]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.68      0.67      2088\n",
      "           1       0.69      0.66      0.68      2251\n",
      "\n",
      "    accuracy                           0.67      4339\n",
      "   macro avg       0.67      0.67      0.67      4339\n",
      "weighted avg       0.67      0.67      0.67      4339\n",
      "\n",
      "Sensitivity: 0.6628165259884495\n",
      "Specificity: 0.6843869731800766\n",
      "precision: 0.653107861060329\n",
      "f1_score: 0.6683816651075771\n"
     ]
    }
   ],
   "source": [
    "#Bagging Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9106478034251675\n",
      "Specificity: 0.47764227642276424\n",
      "precision: 0.7230769230769231\n",
      "f1_score: 0.5752753977968177\n",
      "[0.7685509421377356]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(acc_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhoom\\AppData\\Local\\Temp/ipykernel_65524/2883311971.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bigtest_df[\"Veg_class\"] = finalPredicted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OID_</th>\n",
       "      <th>Id</th>\n",
       "      <th>gridcode</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>CH_mean</th>\n",
       "      <th>ARVI_mean</th>\n",
       "      <th>ARVI_med</th>\n",
       "      <th>ARVI_max</th>\n",
       "      <th>EVI_mean</th>\n",
       "      <th>EVI_med</th>\n",
       "      <th>EVI_max</th>\n",
       "      <th>NDVI_mean</th>\n",
       "      <th>NDVI_med</th>\n",
       "      <th>NDVI_max</th>\n",
       "      <th>SAVI_mean</th>\n",
       "      <th>SAVI_med</th>\n",
       "      <th>SAVI_max</th>\n",
       "      <th>Veg_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.8</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207123</td>\n",
       "      <td>0.167813</td>\n",
       "      <td>0.335815</td>\n",
       "      <td>0.212394</td>\n",
       "      <td>0.186196</td>\n",
       "      <td>0.291992</td>\n",
       "      <td>0.350170</td>\n",
       "      <td>0.315030</td>\n",
       "      <td>0.463047</td>\n",
       "      <td>0.208236</td>\n",
       "      <td>0.183683</td>\n",
       "      <td>0.282265</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.319239</td>\n",
       "      <td>0.334353</td>\n",
       "      <td>0.347863</td>\n",
       "      <td>0.275198</td>\n",
       "      <td>0.279712</td>\n",
       "      <td>0.299364</td>\n",
       "      <td>0.440041</td>\n",
       "      <td>0.453643</td>\n",
       "      <td>0.467307</td>\n",
       "      <td>0.263501</td>\n",
       "      <td>0.269123</td>\n",
       "      <td>0.285889</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3486.2</td>\n",
       "      <td>1942.77</td>\n",
       "      <td>0.120091</td>\n",
       "      <td>0.014902</td>\n",
       "      <td>-0.002058</td>\n",
       "      <td>0.388165</td>\n",
       "      <td>0.155599</td>\n",
       "      <td>0.143094</td>\n",
       "      <td>0.441733</td>\n",
       "      <td>0.199209</td>\n",
       "      <td>0.184422</td>\n",
       "      <td>0.503913</td>\n",
       "      <td>0.155182</td>\n",
       "      <td>0.144957</td>\n",
       "      <td>0.397375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>31.6</td>\n",
       "      <td>9.23</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.419381</td>\n",
       "      <td>0.443782</td>\n",
       "      <td>0.507713</td>\n",
       "      <td>0.344481</td>\n",
       "      <td>0.337613</td>\n",
       "      <td>0.405390</td>\n",
       "      <td>0.527309</td>\n",
       "      <td>0.540501</td>\n",
       "      <td>0.596006</td>\n",
       "      <td>0.326518</td>\n",
       "      <td>0.321309</td>\n",
       "      <td>0.381685</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>31.8</td>\n",
       "      <td>11.82</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.465333</td>\n",
       "      <td>0.440134</td>\n",
       "      <td>0.565654</td>\n",
       "      <td>0.335128</td>\n",
       "      <td>0.329047</td>\n",
       "      <td>0.380148</td>\n",
       "      <td>0.562001</td>\n",
       "      <td>0.551508</td>\n",
       "      <td>0.639035</td>\n",
       "      <td>0.321567</td>\n",
       "      <td>0.314126</td>\n",
       "      <td>0.359091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>10.4</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.480752</td>\n",
       "      <td>0.480752</td>\n",
       "      <td>0.487603</td>\n",
       "      <td>0.471716</td>\n",
       "      <td>0.471716</td>\n",
       "      <td>0.497532</td>\n",
       "      <td>0.587427</td>\n",
       "      <td>0.587427</td>\n",
       "      <td>0.591646</td>\n",
       "      <td>0.430824</td>\n",
       "      <td>0.430824</td>\n",
       "      <td>0.449164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>25.4</td>\n",
       "      <td>5.98</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189495</td>\n",
       "      <td>0.156688</td>\n",
       "      <td>0.420349</td>\n",
       "      <td>0.240643</td>\n",
       "      <td>0.246809</td>\n",
       "      <td>0.319283</td>\n",
       "      <td>0.350102</td>\n",
       "      <td>0.332174</td>\n",
       "      <td>0.515654</td>\n",
       "      <td>0.237516</td>\n",
       "      <td>0.245360</td>\n",
       "      <td>0.305830</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>9.8</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341788</td>\n",
       "      <td>0.341788</td>\n",
       "      <td>0.341788</td>\n",
       "      <td>0.291975</td>\n",
       "      <td>0.291975</td>\n",
       "      <td>0.291975</td>\n",
       "      <td>0.463293</td>\n",
       "      <td>0.463293</td>\n",
       "      <td>0.463293</td>\n",
       "      <td>0.280296</td>\n",
       "      <td>0.280296</td>\n",
       "      <td>0.280296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>19.2</td>\n",
       "      <td>4.90</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.472085</td>\n",
       "      <td>0.420540</td>\n",
       "      <td>0.650954</td>\n",
       "      <td>0.368233</td>\n",
       "      <td>0.320483</td>\n",
       "      <td>0.493067</td>\n",
       "      <td>0.562986</td>\n",
       "      <td>0.515776</td>\n",
       "      <td>0.699278</td>\n",
       "      <td>0.344232</td>\n",
       "      <td>0.306789</td>\n",
       "      <td>0.446165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>95.8</td>\n",
       "      <td>37.74</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150098</td>\n",
       "      <td>0.141256</td>\n",
       "      <td>0.265815</td>\n",
       "      <td>0.253935</td>\n",
       "      <td>0.256579</td>\n",
       "      <td>0.340908</td>\n",
       "      <td>0.310233</td>\n",
       "      <td>0.307031</td>\n",
       "      <td>0.411046</td>\n",
       "      <td>0.240402</td>\n",
       "      <td>0.241444</td>\n",
       "      <td>0.316999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    OID_   Id  gridcode  Shape_Length  Shape_Area   CH_mean  ARVI_mean  \\\n",
       "0      1    1         1          15.8        3.36  0.000000   0.207123   \n",
       "1      2    2         2          24.0        6.93  1.000000   0.319239   \n",
       "2      3    3         3        3486.2     1942.77  0.120091   0.014902   \n",
       "3      4    4         4          31.6        9.23  1.000000   0.419381   \n",
       "4      5    5         5          31.8       11.82  1.000000   0.465333   \n",
       "..   ...  ...       ...           ...         ...       ...        ...   \n",
       "95    96   96        96          10.4        2.31  0.500000   0.480752   \n",
       "96    97   97        97          25.4        5.98  0.000000   0.189495   \n",
       "97    98   98        98           9.8        2.05  0.000000   0.341788   \n",
       "98    99   99        99          19.2        4.90  1.000000   0.472085   \n",
       "99   100  100       100          95.8       37.74  0.000000   0.150098   \n",
       "\n",
       "    ARVI_med  ARVI_max  EVI_mean   EVI_med   EVI_max  NDVI_mean  NDVI_med  \\\n",
       "0   0.167813  0.335815  0.212394  0.186196  0.291992   0.350170  0.315030   \n",
       "1   0.334353  0.347863  0.275198  0.279712  0.299364   0.440041  0.453643   \n",
       "2  -0.002058  0.388165  0.155599  0.143094  0.441733   0.199209  0.184422   \n",
       "3   0.443782  0.507713  0.344481  0.337613  0.405390   0.527309  0.540501   \n",
       "4   0.440134  0.565654  0.335128  0.329047  0.380148   0.562001  0.551508   \n",
       "..       ...       ...       ...       ...       ...        ...       ...   \n",
       "95  0.480752  0.487603  0.471716  0.471716  0.497532   0.587427  0.587427   \n",
       "96  0.156688  0.420349  0.240643  0.246809  0.319283   0.350102  0.332174   \n",
       "97  0.341788  0.341788  0.291975  0.291975  0.291975   0.463293  0.463293   \n",
       "98  0.420540  0.650954  0.368233  0.320483  0.493067   0.562986  0.515776   \n",
       "99  0.141256  0.265815  0.253935  0.256579  0.340908   0.310233  0.307031   \n",
       "\n",
       "    NDVI_max  SAVI_mean  SAVI_med  SAVI_max  Veg_class  \n",
       "0   0.463047   0.208236  0.183683  0.282265          1  \n",
       "1   0.467307   0.263501  0.269123  0.285889          1  \n",
       "2   0.503913   0.155182  0.144957  0.397375          1  \n",
       "3   0.596006   0.326518  0.321309  0.381685          1  \n",
       "4   0.639035   0.321567  0.314126  0.359091          1  \n",
       "..       ...        ...       ...       ...        ...  \n",
       "95  0.591646   0.430824  0.430824  0.449164          1  \n",
       "96  0.515654   0.237516  0.245360  0.305830          0  \n",
       "97  0.463293   0.280296  0.280296  0.280296          1  \n",
       "98  0.699278   0.344232  0.306789  0.446165          1  \n",
       "99  0.411046   0.240402  0.241444  0.316999          0  \n",
       "\n",
       "[100 rows x 19 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 939.9561746 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.728110599078341, 0.7223502304147466, 0.684331797235023, 0.6981566820276498, 0.6782006920415224]\n",
      "Avg accuracy: 0.7022300001594566\n",
      "Std of accuracy : \n",
      "0.019944795326499436\n",
      "\n",
      "[[ 229 1230]\n",
      " [  62 2818]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.16      0.26      1459\n",
      "           1       0.70      0.98      0.81      2880\n",
      "\n",
      "    accuracy                           0.70      4339\n",
      "   macro avg       0.74      0.57      0.54      4339\n",
      "weighted avg       0.73      0.70      0.63      4339\n",
      "\n",
      "Sensitivity: 0.9784722222222222\n",
      "Specificity: 0.15695681973954764\n",
      "precision: 0.7869415807560137\n",
      "f1_score: 0.26171428571428573\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2021.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9731943410275503\n",
      "Specificity: 0.15853658536585366\n",
      "precision: 0.7428571428571429\n",
      "f1_score: 0.2613065326633166\n",
      "[0.7058529264632316]\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhoom\\AppData\\Local\\Temp/ipykernel_65524/2883311971.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bigtest_df[\"Veg_class\"] = finalPredicted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OID_</th>\n",
       "      <th>Id</th>\n",
       "      <th>gridcode</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>CH_mean</th>\n",
       "      <th>ARVI_mean</th>\n",
       "      <th>ARVI_med</th>\n",
       "      <th>ARVI_max</th>\n",
       "      <th>EVI_mean</th>\n",
       "      <th>EVI_med</th>\n",
       "      <th>EVI_max</th>\n",
       "      <th>NDVI_mean</th>\n",
       "      <th>NDVI_med</th>\n",
       "      <th>NDVI_max</th>\n",
       "      <th>SAVI_mean</th>\n",
       "      <th>SAVI_med</th>\n",
       "      <th>SAVI_max</th>\n",
       "      <th>Veg_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.8</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207123</td>\n",
       "      <td>0.167813</td>\n",
       "      <td>0.335815</td>\n",
       "      <td>0.212394</td>\n",
       "      <td>0.186196</td>\n",
       "      <td>0.291992</td>\n",
       "      <td>0.350170</td>\n",
       "      <td>0.315030</td>\n",
       "      <td>0.463047</td>\n",
       "      <td>0.208236</td>\n",
       "      <td>0.183683</td>\n",
       "      <td>0.282265</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.319239</td>\n",
       "      <td>0.334353</td>\n",
       "      <td>0.347863</td>\n",
       "      <td>0.275198</td>\n",
       "      <td>0.279712</td>\n",
       "      <td>0.299364</td>\n",
       "      <td>0.440041</td>\n",
       "      <td>0.453643</td>\n",
       "      <td>0.467307</td>\n",
       "      <td>0.263501</td>\n",
       "      <td>0.269123</td>\n",
       "      <td>0.285889</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3486.2</td>\n",
       "      <td>1942.77</td>\n",
       "      <td>0.120091</td>\n",
       "      <td>0.014902</td>\n",
       "      <td>-0.002058</td>\n",
       "      <td>0.388165</td>\n",
       "      <td>0.155599</td>\n",
       "      <td>0.143094</td>\n",
       "      <td>0.441733</td>\n",
       "      <td>0.199209</td>\n",
       "      <td>0.184422</td>\n",
       "      <td>0.503913</td>\n",
       "      <td>0.155182</td>\n",
       "      <td>0.144957</td>\n",
       "      <td>0.397375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>31.6</td>\n",
       "      <td>9.23</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.419381</td>\n",
       "      <td>0.443782</td>\n",
       "      <td>0.507713</td>\n",
       "      <td>0.344481</td>\n",
       "      <td>0.337613</td>\n",
       "      <td>0.405390</td>\n",
       "      <td>0.527309</td>\n",
       "      <td>0.540501</td>\n",
       "      <td>0.596006</td>\n",
       "      <td>0.326518</td>\n",
       "      <td>0.321309</td>\n",
       "      <td>0.381685</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>31.8</td>\n",
       "      <td>11.82</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.465333</td>\n",
       "      <td>0.440134</td>\n",
       "      <td>0.565654</td>\n",
       "      <td>0.335128</td>\n",
       "      <td>0.329047</td>\n",
       "      <td>0.380148</td>\n",
       "      <td>0.562001</td>\n",
       "      <td>0.551508</td>\n",
       "      <td>0.639035</td>\n",
       "      <td>0.321567</td>\n",
       "      <td>0.314126</td>\n",
       "      <td>0.359091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>10.4</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.480752</td>\n",
       "      <td>0.480752</td>\n",
       "      <td>0.487603</td>\n",
       "      <td>0.471716</td>\n",
       "      <td>0.471716</td>\n",
       "      <td>0.497532</td>\n",
       "      <td>0.587427</td>\n",
       "      <td>0.587427</td>\n",
       "      <td>0.591646</td>\n",
       "      <td>0.430824</td>\n",
       "      <td>0.430824</td>\n",
       "      <td>0.449164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>25.4</td>\n",
       "      <td>5.98</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189495</td>\n",
       "      <td>0.156688</td>\n",
       "      <td>0.420349</td>\n",
       "      <td>0.240643</td>\n",
       "      <td>0.246809</td>\n",
       "      <td>0.319283</td>\n",
       "      <td>0.350102</td>\n",
       "      <td>0.332174</td>\n",
       "      <td>0.515654</td>\n",
       "      <td>0.237516</td>\n",
       "      <td>0.245360</td>\n",
       "      <td>0.305830</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>9.8</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341788</td>\n",
       "      <td>0.341788</td>\n",
       "      <td>0.341788</td>\n",
       "      <td>0.291975</td>\n",
       "      <td>0.291975</td>\n",
       "      <td>0.291975</td>\n",
       "      <td>0.463293</td>\n",
       "      <td>0.463293</td>\n",
       "      <td>0.463293</td>\n",
       "      <td>0.280296</td>\n",
       "      <td>0.280296</td>\n",
       "      <td>0.280296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>19.2</td>\n",
       "      <td>4.90</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.472085</td>\n",
       "      <td>0.420540</td>\n",
       "      <td>0.650954</td>\n",
       "      <td>0.368233</td>\n",
       "      <td>0.320483</td>\n",
       "      <td>0.493067</td>\n",
       "      <td>0.562986</td>\n",
       "      <td>0.515776</td>\n",
       "      <td>0.699278</td>\n",
       "      <td>0.344232</td>\n",
       "      <td>0.306789</td>\n",
       "      <td>0.446165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>95.8</td>\n",
       "      <td>37.74</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150098</td>\n",
       "      <td>0.141256</td>\n",
       "      <td>0.265815</td>\n",
       "      <td>0.253935</td>\n",
       "      <td>0.256579</td>\n",
       "      <td>0.340908</td>\n",
       "      <td>0.310233</td>\n",
       "      <td>0.307031</td>\n",
       "      <td>0.411046</td>\n",
       "      <td>0.240402</td>\n",
       "      <td>0.241444</td>\n",
       "      <td>0.316999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    OID_   Id  gridcode  Shape_Length  Shape_Area   CH_mean  ARVI_mean  \\\n",
       "0      1    1         1          15.8        3.36  0.000000   0.207123   \n",
       "1      2    2         2          24.0        6.93  1.000000   0.319239   \n",
       "2      3    3         3        3486.2     1942.77  0.120091   0.014902   \n",
       "3      4    4         4          31.6        9.23  1.000000   0.419381   \n",
       "4      5    5         5          31.8       11.82  1.000000   0.465333   \n",
       "..   ...  ...       ...           ...         ...       ...        ...   \n",
       "95    96   96        96          10.4        2.31  0.500000   0.480752   \n",
       "96    97   97        97          25.4        5.98  0.000000   0.189495   \n",
       "97    98   98        98           9.8        2.05  0.000000   0.341788   \n",
       "98    99   99        99          19.2        4.90  1.000000   0.472085   \n",
       "99   100  100       100          95.8       37.74  0.000000   0.150098   \n",
       "\n",
       "    ARVI_med  ARVI_max  EVI_mean   EVI_med   EVI_max  NDVI_mean  NDVI_med  \\\n",
       "0   0.167813  0.335815  0.212394  0.186196  0.291992   0.350170  0.315030   \n",
       "1   0.334353  0.347863  0.275198  0.279712  0.299364   0.440041  0.453643   \n",
       "2  -0.002058  0.388165  0.155599  0.143094  0.441733   0.199209  0.184422   \n",
       "3   0.443782  0.507713  0.344481  0.337613  0.405390   0.527309  0.540501   \n",
       "4   0.440134  0.565654  0.335128  0.329047  0.380148   0.562001  0.551508   \n",
       "..       ...       ...       ...       ...       ...        ...       ...   \n",
       "95  0.480752  0.487603  0.471716  0.471716  0.497532   0.587427  0.587427   \n",
       "96  0.156688  0.420349  0.240643  0.246809  0.319283   0.350102  0.332174   \n",
       "97  0.341788  0.341788  0.291975  0.291975  0.291975   0.463293  0.463293   \n",
       "98  0.420540  0.650954  0.368233  0.320483  0.493067   0.562986  0.515776   \n",
       "99  0.141256  0.265815  0.253935  0.256579  0.340908   0.310233  0.307031   \n",
       "\n",
       "    NDVI_max  SAVI_mean  SAVI_med  SAVI_max  Veg_class  \n",
       "0   0.463047   0.208236  0.183683  0.282265          1  \n",
       "1   0.467307   0.263501  0.269123  0.285889          1  \n",
       "2   0.503913   0.155182  0.144957  0.397375          1  \n",
       "3   0.596006   0.326518  0.321309  0.381685          1  \n",
       "4   0.639035   0.321567  0.314126  0.359091          1  \n",
       "..       ...        ...       ...       ...        ...  \n",
       "95  0.591646   0.430824  0.430824  0.449164          1  \n",
       "96  0.515654   0.237516  0.245360  0.305830          1  \n",
       "97  0.463293   0.280296  0.280296  0.280296          1  \n",
       "98  0.699278   0.344232  0.306789  0.446165          1  \n",
       "99  0.411046   0.240402  0.241444  0.316999          1  \n",
       "\n",
       "[100 rows x 19 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Run time is 1286.5268962 seconds ---\n",
      "\n",
      "Accuracy of each fold: \n",
      " [0.6543778801843319, 0.6762672811059908, 0.6693548387096774, 0.684331797235023, 0.6620530565167243]\n",
      "Avg accuracy: 0.6692769707503495\n",
      "Std of accuracy : \n",
      "0.01048509551048603\n",
      "\n",
      "[[1376  712]\n",
      " [ 723 1528]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66      2088\n",
      "           1       0.68      0.68      0.68      2251\n",
      "\n",
      "    accuracy                           0.67      4339\n",
      "   macro avg       0.67      0.67      0.67      4339\n",
      "weighted avg       0.67      0.67      0.67      4339\n",
      "\n",
      "Sensitivity: 0.6788094180364282\n",
      "Specificity: 0.6590038314176245\n",
      "precision: 0.6555502620295379\n",
      "f1_score: 0.6572725101504657\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.8922809630181187\n",
      "Specificity: 0.6300813008130082\n",
      "precision: 0.7407407407407407\n",
      "f1_score: 0.6809445359692476\n",
      "Accuracy: [0.8062364515591129]\n"
     ]
    }
   ],
   "source": [
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], df2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhoom\\AppData\\Local\\Temp/ipykernel_65524/2883311971.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bigtest_df[\"Veg_class\"] = finalPredicted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OID_</th>\n",
       "      <th>Id</th>\n",
       "      <th>gridcode</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>CH_mean</th>\n",
       "      <th>ARVI_mean</th>\n",
       "      <th>ARVI_med</th>\n",
       "      <th>ARVI_max</th>\n",
       "      <th>EVI_mean</th>\n",
       "      <th>EVI_med</th>\n",
       "      <th>EVI_max</th>\n",
       "      <th>NDVI_mean</th>\n",
       "      <th>NDVI_med</th>\n",
       "      <th>NDVI_max</th>\n",
       "      <th>SAVI_mean</th>\n",
       "      <th>SAVI_med</th>\n",
       "      <th>SAVI_max</th>\n",
       "      <th>Veg_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.8</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207123</td>\n",
       "      <td>0.167813</td>\n",
       "      <td>0.335815</td>\n",
       "      <td>0.212394</td>\n",
       "      <td>0.186196</td>\n",
       "      <td>0.291992</td>\n",
       "      <td>0.350170</td>\n",
       "      <td>0.315030</td>\n",
       "      <td>0.463047</td>\n",
       "      <td>0.208236</td>\n",
       "      <td>0.183683</td>\n",
       "      <td>0.282265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.319239</td>\n",
       "      <td>0.334353</td>\n",
       "      <td>0.347863</td>\n",
       "      <td>0.275198</td>\n",
       "      <td>0.279712</td>\n",
       "      <td>0.299364</td>\n",
       "      <td>0.440041</td>\n",
       "      <td>0.453643</td>\n",
       "      <td>0.467307</td>\n",
       "      <td>0.263501</td>\n",
       "      <td>0.269123</td>\n",
       "      <td>0.285889</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3486.2</td>\n",
       "      <td>1942.77</td>\n",
       "      <td>0.120091</td>\n",
       "      <td>0.014902</td>\n",
       "      <td>-0.002058</td>\n",
       "      <td>0.388165</td>\n",
       "      <td>0.155599</td>\n",
       "      <td>0.143094</td>\n",
       "      <td>0.441733</td>\n",
       "      <td>0.199209</td>\n",
       "      <td>0.184422</td>\n",
       "      <td>0.503913</td>\n",
       "      <td>0.155182</td>\n",
       "      <td>0.144957</td>\n",
       "      <td>0.397375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>31.6</td>\n",
       "      <td>9.23</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.419381</td>\n",
       "      <td>0.443782</td>\n",
       "      <td>0.507713</td>\n",
       "      <td>0.344481</td>\n",
       "      <td>0.337613</td>\n",
       "      <td>0.405390</td>\n",
       "      <td>0.527309</td>\n",
       "      <td>0.540501</td>\n",
       "      <td>0.596006</td>\n",
       "      <td>0.326518</td>\n",
       "      <td>0.321309</td>\n",
       "      <td>0.381685</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>31.8</td>\n",
       "      <td>11.82</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.465333</td>\n",
       "      <td>0.440134</td>\n",
       "      <td>0.565654</td>\n",
       "      <td>0.335128</td>\n",
       "      <td>0.329047</td>\n",
       "      <td>0.380148</td>\n",
       "      <td>0.562001</td>\n",
       "      <td>0.551508</td>\n",
       "      <td>0.639035</td>\n",
       "      <td>0.321567</td>\n",
       "      <td>0.314126</td>\n",
       "      <td>0.359091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>10.4</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.480752</td>\n",
       "      <td>0.480752</td>\n",
       "      <td>0.487603</td>\n",
       "      <td>0.471716</td>\n",
       "      <td>0.471716</td>\n",
       "      <td>0.497532</td>\n",
       "      <td>0.587427</td>\n",
       "      <td>0.587427</td>\n",
       "      <td>0.591646</td>\n",
       "      <td>0.430824</td>\n",
       "      <td>0.430824</td>\n",
       "      <td>0.449164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>25.4</td>\n",
       "      <td>5.98</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189495</td>\n",
       "      <td>0.156688</td>\n",
       "      <td>0.420349</td>\n",
       "      <td>0.240643</td>\n",
       "      <td>0.246809</td>\n",
       "      <td>0.319283</td>\n",
       "      <td>0.350102</td>\n",
       "      <td>0.332174</td>\n",
       "      <td>0.515654</td>\n",
       "      <td>0.237516</td>\n",
       "      <td>0.245360</td>\n",
       "      <td>0.305830</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>9.8</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341788</td>\n",
       "      <td>0.341788</td>\n",
       "      <td>0.341788</td>\n",
       "      <td>0.291975</td>\n",
       "      <td>0.291975</td>\n",
       "      <td>0.291975</td>\n",
       "      <td>0.463293</td>\n",
       "      <td>0.463293</td>\n",
       "      <td>0.463293</td>\n",
       "      <td>0.280296</td>\n",
       "      <td>0.280296</td>\n",
       "      <td>0.280296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>19.2</td>\n",
       "      <td>4.90</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.472085</td>\n",
       "      <td>0.420540</td>\n",
       "      <td>0.650954</td>\n",
       "      <td>0.368233</td>\n",
       "      <td>0.320483</td>\n",
       "      <td>0.493067</td>\n",
       "      <td>0.562986</td>\n",
       "      <td>0.515776</td>\n",
       "      <td>0.699278</td>\n",
       "      <td>0.344232</td>\n",
       "      <td>0.306789</td>\n",
       "      <td>0.446165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>95.8</td>\n",
       "      <td>37.74</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150098</td>\n",
       "      <td>0.141256</td>\n",
       "      <td>0.265815</td>\n",
       "      <td>0.253935</td>\n",
       "      <td>0.256579</td>\n",
       "      <td>0.340908</td>\n",
       "      <td>0.310233</td>\n",
       "      <td>0.307031</td>\n",
       "      <td>0.411046</td>\n",
       "      <td>0.240402</td>\n",
       "      <td>0.241444</td>\n",
       "      <td>0.316999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    OID_   Id  gridcode  Shape_Length  Shape_Area   CH_mean  ARVI_mean  \\\n",
       "0      1    1         1          15.8        3.36  0.000000   0.207123   \n",
       "1      2    2         2          24.0        6.93  1.000000   0.319239   \n",
       "2      3    3         3        3486.2     1942.77  0.120091   0.014902   \n",
       "3      4    4         4          31.6        9.23  1.000000   0.419381   \n",
       "4      5    5         5          31.8       11.82  1.000000   0.465333   \n",
       "..   ...  ...       ...           ...         ...       ...        ...   \n",
       "95    96   96        96          10.4        2.31  0.500000   0.480752   \n",
       "96    97   97        97          25.4        5.98  0.000000   0.189495   \n",
       "97    98   98        98           9.8        2.05  0.000000   0.341788   \n",
       "98    99   99        99          19.2        4.90  1.000000   0.472085   \n",
       "99   100  100       100          95.8       37.74  0.000000   0.150098   \n",
       "\n",
       "    ARVI_med  ARVI_max  EVI_mean   EVI_med   EVI_max  NDVI_mean  NDVI_med  \\\n",
       "0   0.167813  0.335815  0.212394  0.186196  0.291992   0.350170  0.315030   \n",
       "1   0.334353  0.347863  0.275198  0.279712  0.299364   0.440041  0.453643   \n",
       "2  -0.002058  0.388165  0.155599  0.143094  0.441733   0.199209  0.184422   \n",
       "3   0.443782  0.507713  0.344481  0.337613  0.405390   0.527309  0.540501   \n",
       "4   0.440134  0.565654  0.335128  0.329047  0.380148   0.562001  0.551508   \n",
       "..       ...       ...       ...       ...       ...        ...       ...   \n",
       "95  0.480752  0.487603  0.471716  0.471716  0.497532   0.587427  0.587427   \n",
       "96  0.156688  0.420349  0.240643  0.246809  0.319283   0.350102  0.332174   \n",
       "97  0.341788  0.341788  0.291975  0.291975  0.291975   0.463293  0.463293   \n",
       "98  0.420540  0.650954  0.368233  0.320483  0.493067   0.562986  0.515776   \n",
       "99  0.141256  0.265815  0.253935  0.256579  0.340908   0.310233  0.307031   \n",
       "\n",
       "    NDVI_max  SAVI_mean  SAVI_med  SAVI_max  Veg_class  \n",
       "0   0.463047   0.208236  0.183683  0.282265          0  \n",
       "1   0.467307   0.263501  0.269123  0.285889          1  \n",
       "2   0.503913   0.155182  0.144957  0.397375          0  \n",
       "3   0.596006   0.326518  0.321309  0.381685          1  \n",
       "4   0.639035   0.321567  0.314126  0.359091          1  \n",
       "..       ...        ...       ...       ...        ...  \n",
       "95  0.591646   0.430824  0.430824  0.449164          1  \n",
       "96  0.515654   0.237516  0.245360  0.305830          0  \n",
       "97  0.463293   0.280296  0.280296  0.280296          1  \n",
       "98  0.699278   0.344232  0.306789  0.446165          1  \n",
       "99  0.411046   0.240402  0.241444  0.316999          0  \n",
       "\n",
       "[100 rows x 19 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting 17 JORN -> 21 SRER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    #Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Truth.extend(Y_test.values)\n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ada boost\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17 Jornada -> 17 SRER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    #Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Truth.extend(Y_test.values)\n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2017.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2017.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.seed(1000)\n",
    "bigtest_df = dffull2017\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2017.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2017.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2017\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ada boost\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2017.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2017.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2017\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigtest_df.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2017.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2017.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2017\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21 JORN -> 21 SRER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model =  GradientBoostingClassifier(n_estimators = 500, \n",
    "                                           learning_rate = 0.1, \n",
    "                                           max_depth = 4, \n",
    "                                           random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2021.iloc[:, 5:18], dfJornada2021.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ada boost\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=500, learning_rate = 0.1, algorithm=\"SAMME.R\", random_state=2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in kf.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1_Score: {}\".format(f1_score))\n",
    "\n",
    "      \n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2021.iloc[:, 5:18], dfJornada2021.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 10, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2021.iloc[:, 5:18], dfJornada2021.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "\n",
    "#criterion='entropy'\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    #Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Truth.extend(Y_test.values)\n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "print(precision)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2021.iloc[:, 5:18], dfJornada2021.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17 J -> 17 S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2017.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2017.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2017\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17 J -> 21 S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Model \n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2017): \n",
    "    \n",
    "    X_train = dfJornada2017.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2017.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2017.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2017.iloc[:, 5:18], dfJornada2017.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21 J -> 21 S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(dfJornada2021): \n",
    "    \n",
    "    X_train = dfJornada2021.iloc[train_index, 5:18]\n",
    "    X_test = dfJornada2021.iloc[test_index, 5:18]\n",
    "    Y_train = dfJornada2021.iloc[train_index, -1]\n",
    "    Y_test = dfJornada2021.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(dfJornada2021.iloc[:, 5:18], dfJornada2021.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17 S -> 21 S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "model = RandomForestClassifier(max_features = 3, random_state = 2)\n",
    "#scores = cross_val_score(model, X, y, cv=crossvalidation)\n",
    "#print('Accuracy of each fold: \\n {}'.format(scores))\n",
    "#print()\n",
    "#print(\"Avg accuracy: {}\".format(scores.mean()))\n",
    "#print(model)\n",
    "\n",
    "\n",
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "\n",
    "for train_index, test_index in crossvalidation.split(df2017): \n",
    "    \n",
    "    X_train = df2017.iloc[train_index, 5:18]\n",
    "    X_test = df2017.iloc[test_index, 5:18]\n",
    "    Y_train = df2017.iloc[train_index, -1]\n",
    "    Y_test = df2017.iloc[test_index, -1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, pred_values)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    Truth.extend(Y_test.values.reshape(Y_test.shape[0])) \n",
    "    Output.extend(pred_values)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Run time is %s seconds ---\" % elapsed)\n",
    "print()\n",
    "print('Accuracy of each fold: \\n {}'.format(acc_score))\n",
    "print(\"Avg accuracy: {}\".format(np.mean(acc_score)))\n",
    "print('Std of accuracy : \\n{}'.format(np.std(acc_score)))\n",
    "print()\n",
    "print(confusion_matrix(Truth, Output))\n",
    "print()\n",
    "print(classification_report(Truth, Output))\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "\n",
    "#statallfeatures_dic['Run Time'].append(elapsed)\n",
    "#statallfeatures_dic['Accuracy'].append(np.mean(acc_score))\n",
    "#statallfeatures_dic['Standard Error'].append(np.std(acc_score))\n",
    "#statallfeatures_dic['Sensitivity'].append(sensitivity)\n",
    "#statallfeatures_dic['Specificity'].append(specificity)\n",
    "#statallfeatures_dic['Precision'].append(precision)\n",
    "#statallfeatures_dic['F1_Score'].append(f1_score)\n",
    "\n",
    "#indaccs_dic['LogReg']=acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [];\n",
    "Truth = [];\n",
    "Output = [];\n",
    "testing = df2021.iloc[:,-1]\n",
    "model.fit(df2017.iloc[:, 5:18], dfJornada2021.iloc[:,-1])\n",
    "pred_values = model.predict(df2021.iloc[:, 5:18])\n",
    "\n",
    "acc = accuracy_score(testing, pred_values)\n",
    "acc_score.append(acc)\n",
    "Truth.extend(testing.values.reshape(testing.shape[0])) \n",
    "Output.extend(pred_values)\n",
    "\n",
    "cm = confusion_matrix(Truth, Output)\n",
    "\n",
    "sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "specificity = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "precision = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "f1_score = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "\n",
    "print(\"Sensitivity: {}\".format(specificity))\n",
    "print(\"Specificity: {}\".format(sensitivity))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1_score: {}\".format(f1_score))\n",
    "print(\"Accuracy: {}\".format(acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1000)\n",
    "bigtest_df = dffull2021\n",
    "finalPredicted = model.predict(bigtest_df.iloc[:,5:18])\n",
    "bigtest_df[\"Veg_class\"] = finalPredicted\n",
    "bigtest_df.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
